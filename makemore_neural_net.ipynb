{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Language Modeling with One Neuron\n",
    "This approach focuses on training a very simple model for character-level language modeling.\n",
    "\n",
    "### Character-Level Language Modeling:\n",
    "Instead of working with words or phrases, the model operates at the character level.\n",
    "It learns patterns between individual characters in a sequence. For example, it might learn that 'q' is often followed by 'u' in English\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"neural_net_model_demo.png\" alt=\"model_architecture_demo\" width=\"400\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "\n",
    "The model uses just a single neuron (a very simple architecture).\n",
    "This minimal setup is designed to demonstrate how even a simple model can capture some sequential patterns in character data.\n",
    "While the capacity of the model is very limited, it can still learn basic relationships between characters.\n",
    "\n",
    "\n",
    "The goal is to train the model to predict sequences of characters accurately enough to generate meaningful outputs.<br>\n",
    "This approach tries to find the probability distribution of the original characters in the dataset, same as what we did in makemore_probabilistic method. The model tends to make itself as close as possible to the probabilistic model (best closed formulated model).\n",
    "The dataset would typically consist of sequences of characters from a relevant corpus, such as names or text data.\n",
    "\n",
    "\n",
    "Once trained, the model is used to generate new sequences of characters. In this case, the goal is to generate new human names.\n",
    "By starting with an initial character and using the model to predict one character at a time, a sequence can be generated.\n",
    "\n",
    "\n",
    "This method highlights how even extremely simple neural network architectures can capture sequential data patterns to produce coherent outputs within their limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "The input to the model is one character at a time.<br>\n",
    "The output is the model's prediction of the next character in the sequence.<br>\n",
    "For example, given the input 'h', the model might predict 'e' as the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_int(data: list) -> dict:\n",
    "    '''\n",
    "    Given a dataset of words(names), char_to_int converts the unique characters to an integer and assigns an id to them.\n",
    "    This is for train step.\n",
    "\n",
    "    Args:\n",
    "        data: a list of names\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of keys being characters and values the corrosponding integer id to each token\n",
    "    '''\n",
    "\n",
    "    char_ids = {}\n",
    "    chars = sorted(set(''.join(data))) # unique characters\n",
    "\n",
    "    for idx, c in enumerate(chars):\n",
    "        char_ids[c] = idx + 1\n",
    "    \n",
    "    char_ids['.'] = 0\n",
    "\n",
    "    return char_ids\n",
    "\n",
    "\n",
    "def int_to_char(data: dict) -> dict:\n",
    "    '''\n",
    "    Given a dataset of ids, int_to_char converts the ids to their original character. This is for inference step.\n",
    "\n",
    "    Args:\n",
    "        data: a dictionary of (chars, ids)\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of (ids, chars)\n",
    "    '''\n",
    "    int_char = {v:k for k,v in data.items()}\n",
    "    return int_char\n",
    "\n",
    "\n",
    "char_ids = char_to_int(words)\n",
    "id_char = int_to_char(char_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataest(data: list, ids: dict) -> torch.tensor:\n",
    "    '''\n",
    "    Making training dataset.\n",
    "    Args:\n",
    "        data: a list of names\n",
    "        ids: tokenized words. Each character's corrosponding id is a value of this dictionary.\n",
    "\n",
    "    Returns:\n",
    "        X, y: data and label sets. Each being a tensor of integers denoting a sequence of characters.\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for w in data:\n",
    "        s = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(s, s[1:]):\n",
    "            X.append(ids[ch1])\n",
    "            y.append(ids[ch2])\n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "data, label = make_dataest(words[:6], char_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "\n",
    "Provide the model with many examples of sequences of characters (e.g., \"John\", \"Jane\").<br>\n",
    "Train the model to minimize the error in predicting the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 3.8021974563598633\n",
      "Epoch: 1, loss: 3.4321272373199463\n",
      "Epoch: 2, loss: 3.111724615097046\n",
      "Epoch: 3, loss: 2.8578903675079346\n",
      "Epoch: 4, loss: 2.661515474319458\n",
      "Epoch: 5, loss: 2.498030185699463\n",
      "Epoch: 6, loss: 2.355353593826294\n",
      "Epoch: 7, loss: 2.229495048522949\n",
      "Epoch: 8, loss: 2.1183438301086426\n",
      "Epoch: 9, loss: 2.020134210586548\n"
     ]
    }
   ],
   "source": [
    "def train(X: torch.tensor, y: torch.tensor, epochs) -> torch.tensor:\n",
    "    '''\n",
    "    Train with a model of one neuron.\n",
    "\n",
    "    Args:\n",
    "        X: input data\n",
    "        y: labels\n",
    "        epochs: number of epochs\n",
    "\n",
    "    Returns:\n",
    "        W: model which is a tensor of weights that can be used to infer predictions.\n",
    "    '''\n",
    "    W = torch.randn((27,27), requires_grad=True)\n",
    "    for epoch in range(epochs):\n",
    "        xenc = F.one_hot(X, 27).float()   # one hot function only accepts integer values\n",
    "        logits = xenc @ W  # W acts as the same matrix (P) in probabilistic method and xenc acts as w[idx] which triggers the right row\n",
    "        e = torch.exp(logits)\n",
    "        probs = e / e.sum(dim=1, keepdim=True)\n",
    "        loss = -probs[range(len(X)), y].log().mean() + 0.1*(W**2).mean() # regularization in here is like adding 1 to N, (N+1)\n",
    "        # loss in here would have to backpropagate through W directly as well. So while probs tend to minimize the loss, the regularization\n",
    "        # tends to make the dataset more uniform by adding an equal number to them - instead of data we add it to loss (why?)\n",
    "        # label smoothing in neural net is to try making the Ws close to each other (more weight values would be ignorante to tiny little changes)which means giving the same weight to each data\n",
    "        # It makes the predictions of softmax less confident.\n",
    "        print(f\"Epoch: {epoch}, loss: {loss}\")\n",
    "\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -10 * W.grad\n",
    "\n",
    "    return W\n",
    "\n",
    "model = train(data, label, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Start with a random character or a seed character (e.g., 'J').<br>\n",
    "Use the model to predict the next character (e.g., 'a').<br>\n",
    "Continue this process until a complete name or sequence is generated (e.g., \"Jane\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snxdeooqhnmophyvjcxjxttehisa', 'xeeeloxd', 'kyma', 'e', 'sambuta', 'a', 'mia', 'ymqvnc', 'e', 'egjmfoxbel']\n"
     ]
    }
   ],
   "source": [
    "def inference(model: torch.tensor, num_words: int, id_char: dict) -> list:\n",
    "    '''\n",
    "    Generating new names given a trained model.\n",
    "    Args:\n",
    "        model: weights of the model\n",
    "        num_words: numebr of words to be produced\n",
    "        id_char: character representation of the tokens\n",
    "\n",
    "    Returns:\n",
    "        names: a list of generated names in character (not int) format.\n",
    "    '''\n",
    "    names = []\n",
    "    idx = 0\n",
    "    for i in range(num_words):\n",
    "        name = ''\n",
    "        while True:\n",
    "            x_enc = F.one_hot(torch.tensor([idx]), num_classes=27).float()\n",
    "            logits = x_enc @ model\n",
    "            p = logits.exp()\n",
    "            p = p / p.sum(dim=1, keepdims=True)\n",
    "            idx = torch.multinomial(p, num_samples=1, replacement=True).item() # it should be probability value\n",
    "            # Model in keras or Torch do not have Multinomial. They only calculate until logits or softmax and then we always took argmax\n",
    "            # However instead of argmax, for variery, we can take randomly based on their probabilities using multinomial function\n",
    "            if idx == 0:\n",
    "                break\n",
    "            name += name.join(id_char[idx])\n",
    "\n",
    "        names.append(name)\n",
    "\n",
    "    return names\n",
    "\n",
    "\n",
    "names = inference(model, 10, id_char)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diffusion models learn the distribution space of each pixel probability based on their neighborhood with other pixels (instead of sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch_AndrejKarpathy_venv",
   "language": "python",
   "name": "pytorch_andrejkarpathy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
