{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Language Modeling with MLP\n",
    "Through this notebook the MLP architecture by Bengio et. al. [Link to paper](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf) is implemented with some optimization methods like learning rate decay, train-test-validation split, batching, and embedding dimensionality, all from scratch without any neural net libraries, for character-level language modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_int(data):\n",
    "    '''\n",
    "    Given a dataset of words(names), char_to_int converts the unique characters to an integer and assigns an id to them.\n",
    "    This is for train step.\n",
    "\n",
    "    Args:\n",
    "        data: a list of names\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of keys being characters and values the corrosponding integer id to each token\n",
    "    '''\n",
    "\n",
    "    char_ids = {}\n",
    "    chars = sorted(set(''.join(data)))\n",
    "    char_ids['.'] = 0\n",
    "    for idx, c in enumerate(chars):\n",
    "        char_ids[c] = idx + 1\n",
    "    return char_ids\n",
    "\n",
    "\n",
    "def int_to_char(data: dict):\n",
    "    '''\n",
    "    Given a dataset of ids, int_to_char converts the ids to their original character. This is for inference step.\n",
    "\n",
    "    Args:\n",
    "        data: a dictionary of (chars, ids)\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of (ids, chars)\n",
    "    '''\n",
    "    chars = {}\n",
    "    for k, v in data.items():\n",
    "        chars[v] = k\n",
    "\n",
    "    return chars\n",
    "\n",
    "\n",
    "char_ids = char_to_int(words)\n",
    "id_chars = int_to_char(char_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data: list, sequence_length: int) -> torch.tensor:\n",
    "    '''\n",
    "    Making dataset from x sequential tokens\n",
    "\n",
    "    Args: \n",
    "        data: words\n",
    "        sequence_length: length of sequential characters to be a sample in the dataset\n",
    "    \n",
    "    Returns:\n",
    "        X, y: data and labels of the dataset\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    random.shuffle(X)\n",
    "    \n",
    "    for word in data:\n",
    "        context = [0] * sequence_length\n",
    "        for ch in word + '.':\n",
    "            X.append(context)\n",
    "            y.append(char_ids[ch])\n",
    "            c_id = char_ids[ch]\n",
    "            \n",
    "            context = context[1:] + [c_id]    \n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "\n",
    "X, y = make_dataset(words, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numebr of train samples: 182517\n",
      "Number of validation samples: 22814\n",
      "Number of test samples: 22815\n"
     ]
    }
   ],
   "source": [
    "def data_split(X: torch.tensor, y: torch.tensor, trian_size: int) -> torch.tensor:\n",
    "    '''\n",
    "    Splitting dataset in 3 sets of train, validation, and test\n",
    "\n",
    "    Args:\n",
    "        X: sequential characters dataset\n",
    "        y: next tokens or labels\n",
    "\n",
    "    Returns: \n",
    "        train_size: size of the training set\n",
    "    '''\n",
    "\n",
    "    trian_size = round(X.size()[0]*trian_size)\n",
    "    validation_size = round((X.size()[0] - trian_size)*0.5)\n",
    "    \n",
    "    X_train = X[:trian_size]\n",
    "    X_validation = X[trian_size:trian_size+validation_size]\n",
    "    X_test = X[trian_size+validation_size:]\n",
    "\n",
    "    y_train = y[:trian_size]\n",
    "    y_validation = y[trian_size:trian_size+validation_size]\n",
    "    y_test = y[trian_size+validation_size:]\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test = data_split(X, y, trian_size=0.8)\n",
    "print(f\"Numebr of train samples: {X_train.size()[0]}\")\n",
    "print(f\"Number of validation samples: {X_validation.size()[0]}\")\n",
    "print(f\"Number of test samples: {X_test.size()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some notes\n",
    "Be aware that the model does not have a concept of sequence order. That's why we are able to concatenate the embeddings to be multiplied by W.<br>\n",
    " In other words, it doesn't care if x comes before y. It just knows that these characters are together.<br><br>\n",
    "\n",
    "\n",
    "\n",
    "There can be 2 perspectives to look at Embedding layer, or matrix C: \n",
    "1) embeddings are achieved by direct access through indexing to C\n",
    "2) words are One-hot filtering for accessing embeddings in C<br>\n",
    "https://chatgpt.com/share/6775af97-1aac-800a-b76a-3a9c2a2043c0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape: tuple, layer2_neurons: int, embedding_dim: int, n_classes: int) -> dict:\n",
    "    '''\n",
    "    Creating the model of the paper with 3 layers.\n",
    "\n",
    "    Args:\n",
    "        input_shape: sequence length of the input data\n",
    "        layer2_neurons: number of hidden layer neurons\n",
    "        embedding_dim: embedding layer dimensions. The number of dimensions we want our data to be presented with\n",
    "        n_classes: number of classes to be predicted. Last layer neurons\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of all layers that we call parameters of the network.\n",
    "    \n",
    "    '''\n",
    "    C = torch.rand((n_classes, embedding_dim))\n",
    "    # embed = C[input_data] # 27*3*2\n",
    "    # layer1 = embed.view((-1, input_data.size()[-1]*embedding_dim))\n",
    "    W1 = torch.rand((input_shape*embedding_dim, layer2_neurons)) # merging dimensions is always from the most inner dimension\n",
    "    b1  = torch.rand(layer2_neurons)\n",
    "    W2 = torch.rand((layer2_neurons, n_classes))\n",
    "    b2 = torch.rand(n_classes)\n",
    "\n",
    "    return {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "parameters = create_model(X.size()[-1], 100, 2, 27)\n",
    "print(f\"Total number of parameters: {sum(v.nelement() for k, v in parameters.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rates(n_samples: int) -> list:\n",
    "    '''\n",
    "    This function aims to provide some data to evaluate the proper learning rate for our model.\n",
    "    It is not used for the final training just because of keeping the training simple, but, it is used before the last training\n",
    "    round to find the elbow of the learning rate values versus losses at epochs.\n",
    "\n",
    "    Args:\n",
    "        n_samples: number of learning rate samples to be produced which we want it to be the same as the number of EPOCHS.\n",
    "\n",
    "    Returns:\n",
    "        10**le: a nonlinear list of learning rate values from high to low values\n",
    "        le: a linear list of learning rate values from high to low values\n",
    "    \n",
    "    '''\n",
    "    le = torch.linspace(-3, 0, n_samples)\n",
    "    return 10**le, le\n",
    "\n",
    "\n",
    "lrs, le = learning_rates(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes on loss function\n",
    "\n",
    "Torch's Crossentropy is preferred over our defined loss of log likehoods for many reasons like:\n",
    " - for higher values, the exp function yields to inf value and nan loss, which the library handles that with decreasing the highest value form all the data, making the losses logical.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " (cross entropy comapres only with relative to the correct label and not the distance from all the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1, iteration=0, train_loss=9.002056121826172\n",
      "Epoch=1, iteration=1, train_loss=5.635300159454346\n",
      "Epoch=1, iteration=2, train_loss=6.889501571655273\n",
      "Epoch=1, iteration=3, train_loss=8.088787078857422\n",
      "Epoch=1, val_loss=7.350344181060791\n",
      "Epoch=2, iteration=0, train_loss=7.455202579498291\n",
      "Epoch=2, iteration=1, train_loss=7.016330718994141\n",
      "Epoch=2, iteration=2, train_loss=5.227480411529541\n",
      "Epoch=2, iteration=3, train_loss=3.7517528533935547\n",
      "Epoch=2, val_loss=3.3020336627960205\n",
      "Epoch=3, iteration=0, train_loss=3.1433329582214355\n",
      "Epoch=3, iteration=1, train_loss=3.1585724353790283\n",
      "Epoch=3, iteration=2, train_loss=2.992389678955078\n",
      "Epoch=3, iteration=3, train_loss=3.2225048542022705\n",
      "Epoch=3, val_loss=2.9739298820495605\n",
      "Epoch=4, iteration=0, train_loss=2.8746752738952637\n",
      "Epoch=4, iteration=1, train_loss=2.824944496154785\n",
      "Epoch=4, iteration=2, train_loss=2.8767309188842773\n",
      "Epoch=4, iteration=3, train_loss=3.061100482940674\n",
      "Epoch=4, val_loss=3.2254714965820312\n",
      "Epoch=5, iteration=0, train_loss=3.2019546031951904\n",
      "Epoch=5, iteration=1, train_loss=2.8289239406585693\n",
      "Epoch=5, iteration=2, train_loss=2.8583595752716064\n",
      "Epoch=5, iteration=3, train_loss=2.9895617961883545\n",
      "Epoch=5, val_loss=2.9662256240844727\n",
      "Epoch=6, iteration=0, train_loss=2.8540680408477783\n",
      "Epoch=6, iteration=1, train_loss=2.835179090499878\n",
      "Epoch=6, iteration=2, train_loss=2.9596786499023438\n",
      "Epoch=6, iteration=3, train_loss=3.262542963027954\n",
      "Epoch=6, val_loss=2.9710114002227783\n",
      "Epoch=7, iteration=0, train_loss=2.90877366065979\n",
      "Epoch=7, iteration=1, train_loss=2.8526036739349365\n",
      "Epoch=7, iteration=2, train_loss=3.022906541824341\n",
      "Epoch=7, iteration=3, train_loss=2.999798059463501\n",
      "Epoch=7, val_loss=3.064655065536499\n",
      "Epoch=8, iteration=0, train_loss=3.008145570755005\n",
      "Epoch=8, iteration=1, train_loss=2.8308494091033936\n",
      "Epoch=8, iteration=2, train_loss=2.905740737915039\n",
      "Epoch=8, iteration=3, train_loss=3.044420003890991\n",
      "Epoch=8, val_loss=3.1557180881500244\n",
      "Epoch=9, iteration=0, train_loss=3.1212546825408936\n",
      "Epoch=9, iteration=1, train_loss=2.8047618865966797\n",
      "Epoch=9, iteration=2, train_loss=2.830967426300049\n",
      "Epoch=9, iteration=3, train_loss=2.9534506797790527\n",
      "Epoch=9, val_loss=2.930217981338501\n",
      "Epoch=10, iteration=0, train_loss=2.821415662765503\n",
      "Epoch=10, iteration=1, train_loss=2.7852649688720703\n",
      "Epoch=10, iteration=2, train_loss=2.8226306438446045\n",
      "Epoch=10, iteration=3, train_loss=2.9557111263275146\n",
      "Epoch=10, val_loss=2.9423816204071045\n",
      "Epoch=11, iteration=0, train_loss=2.8488712310791016\n",
      "Epoch=11, iteration=1, train_loss=2.832972764968872\n",
      "Epoch=11, iteration=2, train_loss=2.9476125240325928\n",
      "Epoch=11, iteration=3, train_loss=3.0095481872558594\n",
      "Epoch=11, val_loss=3.07232666015625\n",
      "Epoch=12, iteration=0, train_loss=3.0188775062561035\n",
      "Epoch=12, iteration=1, train_loss=2.8011279106140137\n",
      "Epoch=12, iteration=2, train_loss=2.8346023559570312\n",
      "Epoch=12, iteration=3, train_loss=2.992645502090454\n",
      "Epoch=12, val_loss=3.027559757232666\n",
      "Epoch=13, iteration=0, train_loss=2.962913751602173\n",
      "Epoch=13, iteration=1, train_loss=2.8102874755859375\n",
      "Epoch=13, iteration=2, train_loss=2.868194580078125\n",
      "Epoch=13, iteration=3, train_loss=3.0103161334991455\n",
      "Epoch=13, val_loss=3.063983201980591\n",
      "Epoch=14, iteration=0, train_loss=3.008465051651001\n",
      "Epoch=14, iteration=1, train_loss=2.7913315296173096\n",
      "Epoch=14, iteration=2, train_loss=2.8209433555603027\n",
      "Epoch=14, iteration=3, train_loss=2.9685893058776855\n",
      "Epoch=14, val_loss=2.9735822677612305\n",
      "Epoch=15, iteration=0, train_loss=2.8931145668029785\n",
      "Epoch=15, iteration=1, train_loss=2.812809467315674\n",
      "Epoch=15, iteration=2, train_loss=2.8889594078063965\n",
      "Epoch=15, iteration=3, train_loss=2.9949498176574707\n",
      "Epoch=15, val_loss=3.028625249862671\n",
      "Epoch=16, iteration=0, train_loss=2.963855504989624\n",
      "Epoch=16, iteration=1, train_loss=2.7882399559020996\n",
      "Epoch=16, iteration=2, train_loss=2.824082374572754\n",
      "Epoch=16, iteration=3, train_loss=2.972740411758423\n",
      "Epoch=16, val_loss=2.9807212352752686\n",
      "Epoch=17, iteration=0, train_loss=2.902013063430786\n",
      "Epoch=17, iteration=1, train_loss=2.7967846393585205\n",
      "Epoch=17, iteration=2, train_loss=2.851092576980591\n",
      "Epoch=17, iteration=3, train_loss=2.984675884246826\n",
      "Epoch=17, val_loss=3.0026097297668457\n",
      "Epoch=18, iteration=0, train_loss=2.930257797241211\n",
      "Epoch=18, iteration=1, train_loss=2.7844364643096924\n",
      "Epoch=18, iteration=2, train_loss=2.823014736175537\n",
      "Epoch=18, iteration=3, train_loss=2.968254804611206\n",
      "Epoch=18, val_loss=2.97023868560791\n",
      "Epoch=19, iteration=0, train_loss=2.8878026008605957\n",
      "Epoch=19, iteration=1, train_loss=2.7874934673309326\n",
      "Epoch=19, iteration=2, train_loss=2.834761142730713\n",
      "Epoch=19, iteration=3, train_loss=2.97220516204834\n",
      "Epoch=19, val_loss=2.9763095378875732\n",
      "Epoch=20, iteration=0, train_loss=2.895642042160034\n",
      "Epoch=20, iteration=1, train_loss=2.7806618213653564\n",
      "Epoch=20, iteration=2, train_loss=2.8215489387512207\n",
      "Epoch=20, iteration=3, train_loss=2.962949514389038\n",
      "Epoch=20, val_loss=2.9590959548950195\n",
      "Epoch=21, iteration=0, train_loss=2.872504472732544\n",
      "Epoch=21, iteration=1, train_loss=2.779329538345337\n",
      "Epoch=21, iteration=2, train_loss=2.8221888542175293\n",
      "Epoch=21, iteration=3, train_loss=2.9609196186065674\n",
      "Epoch=21, val_loss=2.9548895359039307\n",
      "Epoch=22, iteration=0, train_loss=2.866610288619995\n",
      "Epoch=22, iteration=1, train_loss=2.775217056274414\n",
      "Epoch=22, iteration=2, train_loss=2.8161306381225586\n",
      "Epoch=22, iteration=3, train_loss=2.955386161804199\n",
      "Epoch=22, val_loss=2.94531512260437\n",
      "Epoch=23, iteration=0, train_loss=2.853296995162964\n",
      "Epoch=23, iteration=1, train_loss=2.771951913833618\n",
      "Epoch=23, iteration=2, train_loss=2.812422513961792\n",
      "Epoch=23, iteration=3, train_loss=2.951240301132202\n",
      "Epoch=23, val_loss=2.9384214878082275\n",
      "Epoch=24, iteration=0, train_loss=2.8435006141662598\n",
      "Epoch=24, iteration=1, train_loss=2.7681450843811035\n",
      "Epoch=24, iteration=2, train_loss=2.8076648712158203\n",
      "Epoch=24, iteration=3, train_loss=2.9465622901916504\n",
      "Epoch=24, val_loss=2.9311769008636475\n",
      "Epoch=25, iteration=0, train_loss=2.833059549331665\n",
      "Epoch=25, iteration=1, train_loss=2.764174461364746\n",
      "Epoch=25, iteration=2, train_loss=2.8029110431671143\n",
      "Epoch=25, iteration=3, train_loss=2.9419031143188477\n",
      "Epoch=25, val_loss=2.924424171447754\n",
      "Epoch=26, iteration=0, train_loss=2.823155641555786\n",
      "Epoch=26, iteration=1, train_loss=2.7599165439605713\n",
      "Epoch=26, iteration=2, train_loss=2.797909736633301\n",
      "Epoch=26, iteration=3, train_loss=2.93705677986145\n",
      "Epoch=26, val_loss=2.9178807735443115\n",
      "Epoch=27, iteration=0, train_loss=2.8133952617645264\n",
      "Epoch=27, iteration=1, train_loss=2.7553470134735107\n",
      "Epoch=27, iteration=2, train_loss=2.792699098587036\n",
      "Epoch=27, iteration=3, train_loss=2.931997776031494\n",
      "Epoch=27, val_loss=2.9115281105041504\n",
      "Epoch=28, iteration=0, train_loss=2.803760051727295\n",
      "Epoch=28, iteration=1, train_loss=2.750424861907959\n",
      "Epoch=28, iteration=2, train_loss=2.787255048751831\n",
      "Epoch=28, iteration=3, train_loss=2.9266674518585205\n",
      "Epoch=28, val_loss=2.905308723449707\n",
      "Epoch=29, iteration=0, train_loss=2.7941794395446777\n",
      "Epoch=29, iteration=1, train_loss=2.74511456489563\n",
      "Epoch=29, iteration=2, train_loss=2.7815561294555664\n",
      "Epoch=29, iteration=3, train_loss=2.9210212230682373\n",
      "Epoch=29, val_loss=2.8991806507110596\n",
      "Epoch=30, iteration=0, train_loss=2.7846102714538574\n",
      "Epoch=30, iteration=1, train_loss=2.7393951416015625\n",
      "Epoch=30, iteration=2, train_loss=2.7755796909332275\n",
      "Epoch=30, iteration=3, train_loss=2.915034770965576\n",
      "Epoch=30, val_loss=2.893120765686035\n",
      "Epoch=31, iteration=0, train_loss=2.7750439643859863\n",
      "Epoch=31, iteration=1, train_loss=2.7332823276519775\n",
      "Epoch=31, iteration=2, train_loss=2.7693209648132324\n",
      "Epoch=31, iteration=3, train_loss=2.908731698989868\n",
      "Epoch=31, val_loss=2.8871474266052246\n",
      "Epoch=32, iteration=0, train_loss=2.7655348777770996\n",
      "Epoch=32, iteration=1, train_loss=2.72686767578125\n",
      "Epoch=32, iteration=2, train_loss=2.7628300189971924\n",
      "Epoch=32, iteration=3, train_loss=2.9022295475006104\n",
      "Epoch=32, val_loss=2.8813681602478027\n",
      "Epoch=33, iteration=0, train_loss=2.756253480911255\n",
      "Epoch=33, iteration=1, train_loss=2.7203757762908936\n",
      "Epoch=33, iteration=2, train_loss=2.7562832832336426\n",
      "Epoch=33, iteration=3, train_loss=2.8958075046539307\n",
      "Epoch=33, val_loss=2.87603759765625\n",
      "Epoch=34, iteration=0, train_loss=2.7475497722625732\n",
      "Epoch=34, iteration=1, train_loss=2.714205265045166\n",
      "Epoch=34, iteration=2, train_loss=2.7500417232513428\n",
      "Epoch=34, iteration=3, train_loss=2.889928102493286\n",
      "Epoch=34, val_loss=2.871549367904663\n",
      "Epoch=35, iteration=0, train_loss=2.7399396896362305\n",
      "Epoch=35, iteration=1, train_loss=2.708871603012085\n",
      "Epoch=35, iteration=2, train_loss=2.7446024417877197\n",
      "Epoch=35, iteration=3, train_loss=2.885110378265381\n",
      "Epoch=35, val_loss=2.868255615234375\n",
      "Epoch=36, iteration=0, train_loss=2.733910322189331\n",
      "Epoch=36, iteration=1, train_loss=2.7047715187072754\n",
      "Epoch=36, iteration=2, train_loss=2.740354061126709\n",
      "Epoch=36, iteration=3, train_loss=2.881639242172241\n",
      "Epoch=36, val_loss=2.8662047386169434\n",
      "Epoch=37, iteration=0, train_loss=2.729602575302124\n",
      "Epoch=37, iteration=1, train_loss=2.701932430267334\n",
      "Epoch=37, iteration=2, train_loss=2.737334728240967\n",
      "Epoch=37, iteration=3, train_loss=2.8793907165527344\n",
      "Epoch=37, val_loss=2.865093231201172\n",
      "Epoch=38, iteration=0, train_loss=2.7267253398895264\n",
      "Epoch=38, iteration=1, train_loss=2.7000467777252197\n",
      "Epoch=38, iteration=2, train_loss=2.73526930809021\n",
      "Epoch=38, iteration=3, train_loss=2.8779826164245605\n",
      "Epoch=38, val_loss=2.8645012378692627\n",
      "Epoch=39, iteration=0, train_loss=2.72479248046875\n",
      "Epoch=39, iteration=1, train_loss=2.698725938796997\n",
      "Epoch=39, iteration=2, train_loss=2.7338058948516846\n",
      "Epoch=39, iteration=3, train_loss=2.8770394325256348\n",
      "Epoch=39, val_loss=2.8641157150268555\n",
      "Epoch=40, iteration=0, train_loss=2.7233901023864746\n",
      "Epoch=40, iteration=1, train_loss=2.697688341140747\n",
      "Epoch=40, iteration=2, train_loss=2.732677698135376\n",
      "Epoch=40, iteration=3, train_loss=2.8763160705566406\n",
      "Epoch=40, val_loss=2.8637726306915283\n",
      "Epoch=41, iteration=0, train_loss=2.7222602367401123\n",
      "Epoch=41, iteration=1, train_loss=2.6967806816101074\n",
      "Epoch=41, iteration=2, train_loss=2.7317259311676025\n",
      "Epoch=41, iteration=3, train_loss=2.875682830810547\n",
      "Epoch=41, val_loss=2.8634095191955566\n",
      "Epoch=42, iteration=0, train_loss=2.721264362335205\n",
      "Epoch=42, iteration=1, train_loss=2.695929527282715\n",
      "Epoch=42, iteration=2, train_loss=2.7308645248413086\n",
      "Epoch=42, iteration=3, train_loss=2.8750784397125244\n",
      "Epoch=42, val_loss=2.8630080223083496\n",
      "Epoch=43, iteration=0, train_loss=2.720332145690918\n",
      "Epoch=43, iteration=1, train_loss=2.6951022148132324\n",
      "Epoch=43, iteration=2, train_loss=2.730048894882202\n",
      "Epoch=43, iteration=3, train_loss=2.8744757175445557\n",
      "Epoch=43, val_loss=2.862567663192749\n",
      "Epoch=44, iteration=0, train_loss=2.719428777694702\n",
      "Epoch=44, iteration=1, train_loss=2.6942832469940186\n",
      "Epoch=44, iteration=2, train_loss=2.7292542457580566\n",
      "Epoch=44, iteration=3, train_loss=2.873865842819214\n",
      "Epoch=44, val_loss=2.8620924949645996\n",
      "Epoch=45, iteration=0, train_loss=2.7185351848602295\n",
      "Epoch=45, iteration=1, train_loss=2.693464517593384\n",
      "Epoch=45, iteration=2, train_loss=2.7284677028656006\n",
      "Epoch=45, iteration=3, train_loss=2.8732404708862305\n",
      "Epoch=45, val_loss=2.861586570739746\n",
      "Epoch=46, iteration=0, train_loss=2.717642307281494\n",
      "Epoch=46, iteration=1, train_loss=2.692641258239746\n",
      "Epoch=46, iteration=2, train_loss=2.7276804447174072\n",
      "Epoch=46, iteration=3, train_loss=2.872600793838501\n",
      "Epoch=46, val_loss=2.861053943634033\n",
      "Epoch=47, iteration=0, train_loss=2.7167439460754395\n",
      "Epoch=47, iteration=1, train_loss=2.6918094158172607\n",
      "Epoch=47, iteration=2, train_loss=2.72688889503479\n",
      "Epoch=47, iteration=3, train_loss=2.8719441890716553\n",
      "Epoch=47, val_loss=2.86049747467041\n",
      "Epoch=48, iteration=0, train_loss=2.7158362865448\n",
      "Epoch=48, iteration=1, train_loss=2.690967559814453\n",
      "Epoch=48, iteration=2, train_loss=2.7260892391204834\n",
      "Epoch=48, iteration=3, train_loss=2.8712704181671143\n",
      "Epoch=48, val_loss=2.8599178791046143\n",
      "Epoch=49, iteration=0, train_loss=2.714916706085205\n",
      "Epoch=49, iteration=1, train_loss=2.6901142597198486\n",
      "Epoch=49, iteration=2, train_loss=2.7252793312072754\n",
      "Epoch=49, iteration=3, train_loss=2.870579242706299\n",
      "Epoch=49, val_loss=2.8593175411224365\n",
      "Epoch=50, iteration=0, train_loss=2.7139832973480225\n",
      "Epoch=50, iteration=1, train_loss=2.6892473697662354\n",
      "Epoch=50, iteration=2, train_loss=2.7244582176208496\n",
      "Epoch=50, iteration=3, train_loss=2.869870662689209\n",
      "Epoch=50, val_loss=2.858696937561035\n",
      "Epoch=51, iteration=0, train_loss=2.7130353450775146\n",
      "Epoch=51, iteration=1, train_loss=2.688366651535034\n",
      "Epoch=51, iteration=2, train_loss=2.723623752593994\n",
      "Epoch=51, iteration=3, train_loss=2.8691444396972656\n",
      "Epoch=51, val_loss=2.8580565452575684\n",
      "Epoch=52, iteration=0, train_loss=2.712071418762207\n",
      "Epoch=52, iteration=1, train_loss=2.6874704360961914\n",
      "Epoch=52, iteration=2, train_loss=2.7227749824523926\n",
      "Epoch=52, iteration=3, train_loss=2.8684003353118896\n",
      "Epoch=52, val_loss=2.8573966026306152\n",
      "Epoch=53, iteration=0, train_loss=2.711090564727783\n",
      "Epoch=53, iteration=1, train_loss=2.686558246612549\n",
      "Epoch=53, iteration=2, train_loss=2.7219114303588867\n",
      "Epoch=53, iteration=3, train_loss=2.8676373958587646\n",
      "Epoch=53, val_loss=2.856717824935913\n",
      "Epoch=54, iteration=0, train_loss=2.7100911140441895\n",
      "Epoch=54, iteration=1, train_loss=2.685629367828369\n",
      "Epoch=54, iteration=2, train_loss=2.72103214263916\n",
      "Epoch=54, iteration=3, train_loss=2.866856098175049\n",
      "Epoch=54, val_loss=2.8560190200805664\n",
      "Epoch=55, iteration=0, train_loss=2.709073543548584\n",
      "Epoch=55, iteration=1, train_loss=2.6846823692321777\n",
      "Epoch=55, iteration=2, train_loss=2.7201364040374756\n",
      "Epoch=55, iteration=3, train_loss=2.866055965423584\n",
      "Epoch=55, val_loss=2.8553013801574707\n",
      "Epoch=56, iteration=0, train_loss=2.708035945892334\n",
      "Epoch=56, iteration=1, train_loss=2.6837165355682373\n",
      "Epoch=56, iteration=2, train_loss=2.7192230224609375\n",
      "Epoch=56, iteration=3, train_loss=2.865236520767212\n",
      "Epoch=56, val_loss=2.8545639514923096\n",
      "Epoch=57, iteration=0, train_loss=2.7069778442382812\n",
      "Epoch=57, iteration=1, train_loss=2.682732582092285\n",
      "Epoch=57, iteration=2, train_loss=2.718291759490967\n",
      "Epoch=57, iteration=3, train_loss=2.864396810531616\n",
      "Epoch=57, val_loss=2.853806495666504\n",
      "Epoch=58, iteration=0, train_loss=2.7058990001678467\n",
      "Epoch=58, iteration=1, train_loss=2.681727886199951\n",
      "Epoch=58, iteration=2, train_loss=2.7173421382904053\n",
      "Epoch=58, iteration=3, train_loss=2.8635363578796387\n",
      "Epoch=58, val_loss=2.8530287742614746\n",
      "Epoch=59, iteration=0, train_loss=2.7047979831695557\n",
      "Epoch=59, iteration=1, train_loss=2.6807029247283936\n",
      "Epoch=59, iteration=2, train_loss=2.7163732051849365\n",
      "Epoch=59, iteration=3, train_loss=2.8626556396484375\n",
      "Epoch=59, val_loss=2.8522305488586426\n",
      "Epoch=60, iteration=0, train_loss=2.703674554824829\n",
      "Epoch=60, iteration=1, train_loss=2.679656982421875\n",
      "Epoch=60, iteration=2, train_loss=2.7153844833374023\n",
      "Epoch=60, iteration=3, train_loss=2.86175274848938\n",
      "Epoch=60, val_loss=2.8514115810394287\n",
      "Epoch=61, iteration=0, train_loss=2.7025279998779297\n",
      "Epoch=61, iteration=1, train_loss=2.6785888671875\n",
      "Epoch=61, iteration=2, train_loss=2.7143754959106445\n",
      "Epoch=61, iteration=3, train_loss=2.860828399658203\n",
      "Epoch=61, val_loss=2.8505704402923584\n",
      "Epoch=62, iteration=0, train_loss=2.701357126235962\n",
      "Epoch=62, iteration=1, train_loss=2.6774978637695312\n",
      "Epoch=62, iteration=2, train_loss=2.7133450508117676\n",
      "Epoch=62, iteration=3, train_loss=2.8598814010620117\n",
      "Epoch=62, val_loss=2.8497073650360107\n",
      "Epoch=63, iteration=0, train_loss=2.700161933898926\n",
      "Epoch=63, iteration=1, train_loss=2.6763851642608643\n",
      "Epoch=63, iteration=2, train_loss=2.712293863296509\n",
      "Epoch=63, iteration=3, train_loss=2.8589112758636475\n",
      "Epoch=63, val_loss=2.8488216400146484\n",
      "Epoch=64, iteration=0, train_loss=2.698942184448242\n",
      "Epoch=64, iteration=1, train_loss=2.6752474308013916\n",
      "Epoch=64, iteration=2, train_loss=2.7112205028533936\n",
      "Epoch=64, iteration=3, train_loss=2.857916831970215\n",
      "Epoch=64, val_loss=2.847913980484009\n",
      "Epoch=65, iteration=0, train_loss=2.6976959705352783\n",
      "Epoch=65, iteration=1, train_loss=2.674086332321167\n",
      "Epoch=65, iteration=2, train_loss=2.7101247310638428\n",
      "Epoch=65, iteration=3, train_loss=2.8568990230560303\n",
      "Epoch=65, val_loss=2.8469817638397217\n",
      "Epoch=66, iteration=0, train_loss=2.6964237689971924\n",
      "Epoch=66, iteration=1, train_loss=2.672900915145874\n",
      "Epoch=66, iteration=2, train_loss=2.7090067863464355\n",
      "Epoch=66, iteration=3, train_loss=2.855855941772461\n",
      "Epoch=66, val_loss=2.8460259437561035\n",
      "Epoch=67, iteration=0, train_loss=2.695124864578247\n",
      "Epoch=67, iteration=1, train_loss=2.6716902256011963\n",
      "Epoch=67, iteration=2, train_loss=2.7078659534454346\n",
      "Epoch=67, iteration=3, train_loss=2.854788303375244\n",
      "Epoch=67, val_loss=2.845045566558838\n",
      "Epoch=68, iteration=0, train_loss=2.6937990188598633\n",
      "Epoch=68, iteration=1, train_loss=2.670454263687134\n",
      "Epoch=68, iteration=2, train_loss=2.7067008018493652\n",
      "Epoch=68, iteration=3, train_loss=2.8536949157714844\n",
      "Epoch=68, val_loss=2.844040870666504\n",
      "Epoch=69, iteration=0, train_loss=2.692445993423462\n",
      "Epoch=69, iteration=1, train_loss=2.6691927909851074\n",
      "Epoch=69, iteration=2, train_loss=2.7055130004882812\n",
      "Epoch=69, iteration=3, train_loss=2.8525753021240234\n",
      "Epoch=69, val_loss=2.843010663986206\n",
      "Epoch=70, iteration=0, train_loss=2.6910650730133057\n",
      "Epoch=70, iteration=1, train_loss=2.667905569076538\n",
      "Epoch=70, iteration=2, train_loss=2.704301595687866\n",
      "Epoch=70, iteration=3, train_loss=2.8514297008514404\n",
      "Epoch=70, val_loss=2.8419549465179443\n",
      "Epoch=71, iteration=0, train_loss=2.6896562576293945\n",
      "Epoch=71, iteration=1, train_loss=2.666592836380005\n",
      "Epoch=71, iteration=2, train_loss=2.703066349029541\n",
      "Epoch=71, iteration=3, train_loss=2.850257396697998\n",
      "Epoch=71, val_loss=2.8408737182617188\n",
      "Epoch=72, iteration=0, train_loss=2.6882197856903076\n",
      "Epoch=72, iteration=1, train_loss=2.6652538776397705\n",
      "Epoch=72, iteration=2, train_loss=2.701807737350464\n",
      "Epoch=72, iteration=3, train_loss=2.8490583896636963\n",
      "Epoch=72, val_loss=2.8397669792175293\n",
      "Epoch=73, iteration=0, train_loss=2.686755657196045\n",
      "Epoch=73, iteration=1, train_loss=2.663888931274414\n",
      "Epoch=73, iteration=2, train_loss=2.7005257606506348\n",
      "Epoch=73, iteration=3, train_loss=2.8478329181671143\n",
      "Epoch=73, val_loss=2.8386340141296387\n",
      "Epoch=74, iteration=0, train_loss=2.685263156890869\n",
      "Epoch=74, iteration=1, train_loss=2.662498950958252\n",
      "Epoch=74, iteration=2, train_loss=2.6992204189300537\n",
      "Epoch=74, iteration=3, train_loss=2.846581220626831\n",
      "Epoch=74, val_loss=2.837475061416626\n",
      "Epoch=75, iteration=0, train_loss=2.683743715286255\n",
      "Epoch=75, iteration=1, train_loss=2.661083221435547\n",
      "Epoch=75, iteration=2, train_loss=2.697892665863037\n",
      "Epoch=75, iteration=3, train_loss=2.8453023433685303\n",
      "Epoch=75, val_loss=2.8362910747528076\n",
      "Epoch=76, iteration=0, train_loss=2.682197093963623\n",
      "Epoch=76, iteration=1, train_loss=2.6596426963806152\n",
      "Epoch=76, iteration=2, train_loss=2.696542263031006\n",
      "Epoch=76, iteration=3, train_loss=2.8439981937408447\n",
      "Epoch=76, val_loss=2.835081100463867\n",
      "Epoch=77, iteration=0, train_loss=2.680624008178711\n",
      "Epoch=77, iteration=1, train_loss=2.6581783294677734\n",
      "Epoch=77, iteration=2, train_loss=2.6951704025268555\n",
      "Epoch=77, iteration=3, train_loss=2.842668294906616\n",
      "Epoch=77, val_loss=2.8338465690612793\n",
      "Epoch=78, iteration=0, train_loss=2.679025173187256\n",
      "Epoch=78, iteration=1, train_loss=2.6566901206970215\n",
      "Epoch=78, iteration=2, train_loss=2.6937777996063232\n",
      "Epoch=78, iteration=3, train_loss=2.8413138389587402\n",
      "Epoch=78, val_loss=2.832587480545044\n",
      "Epoch=79, iteration=0, train_loss=2.6774017810821533\n",
      "Epoch=79, iteration=1, train_loss=2.655179977416992\n",
      "Epoch=79, iteration=2, train_loss=2.6923649311065674\n",
      "Epoch=79, iteration=3, train_loss=2.8399341106414795\n",
      "Epoch=79, val_loss=2.8313047885894775\n",
      "Epoch=80, iteration=0, train_loss=2.6757538318634033\n",
      "Epoch=80, iteration=1, train_loss=2.6536474227905273\n",
      "Epoch=80, iteration=2, train_loss=2.6909332275390625\n",
      "Epoch=80, iteration=3, train_loss=2.838531732559204\n",
      "Epoch=80, val_loss=2.82999849319458\n",
      "Epoch=81, iteration=0, train_loss=2.6740832328796387\n",
      "Epoch=81, iteration=1, train_loss=2.652095317840576\n",
      "Epoch=81, iteration=2, train_loss=2.6894843578338623\n",
      "Epoch=81, iteration=3, train_loss=2.8371071815490723\n",
      "Epoch=81, val_loss=2.8286709785461426\n",
      "Epoch=82, iteration=0, train_loss=2.672391414642334\n",
      "Epoch=82, iteration=1, train_loss=2.6505239009857178\n",
      "Epoch=82, iteration=2, train_loss=2.688018798828125\n",
      "Epoch=82, iteration=3, train_loss=2.8356621265411377\n",
      "Epoch=82, val_loss=2.8273229598999023\n",
      "Epoch=83, iteration=0, train_loss=2.6706795692443848\n",
      "Epoch=83, iteration=1, train_loss=2.648935317993164\n",
      "Epoch=83, iteration=2, train_loss=2.686537742614746\n",
      "Epoch=83, iteration=3, train_loss=2.8341968059539795\n",
      "Epoch=83, val_loss=2.825955390930176\n",
      "Epoch=84, iteration=0, train_loss=2.668949842453003\n",
      "Epoch=84, iteration=1, train_loss=2.6473307609558105\n",
      "Epoch=84, iteration=2, train_loss=2.685044050216675\n",
      "Epoch=84, iteration=3, train_loss=2.832714557647705\n",
      "Epoch=84, val_loss=2.8245697021484375\n",
      "Epoch=85, iteration=0, train_loss=2.667203903198242\n",
      "Epoch=85, iteration=1, train_loss=2.6457126140594482\n",
      "Epoch=85, iteration=2, train_loss=2.6835384368896484\n",
      "Epoch=85, iteration=3, train_loss=2.8312160968780518\n",
      "Epoch=85, val_loss=2.823167562484741\n",
      "Epoch=86, iteration=0, train_loss=2.6654434204101562\n",
      "Epoch=86, iteration=1, train_loss=2.6440823078155518\n",
      "Epoch=86, iteration=2, train_loss=2.6820223331451416\n",
      "Epoch=86, iteration=3, train_loss=2.8297040462493896\n",
      "Epoch=86, val_loss=2.8217508792877197\n",
      "Epoch=87, iteration=0, train_loss=2.663670539855957\n",
      "Epoch=87, iteration=1, train_loss=2.642442464828491\n",
      "Epoch=87, iteration=2, train_loss=2.6804988384246826\n",
      "Epoch=87, iteration=3, train_loss=2.828179359436035\n",
      "Epoch=87, val_loss=2.820321798324585\n",
      "Epoch=88, iteration=0, train_loss=2.6618881225585938\n",
      "Epoch=88, iteration=1, train_loss=2.6407947540283203\n",
      "Epoch=88, iteration=2, train_loss=2.678968906402588\n",
      "Epoch=88, iteration=3, train_loss=2.826645612716675\n",
      "Epoch=88, val_loss=2.8188819885253906\n",
      "Epoch=89, iteration=0, train_loss=2.66009783744812\n",
      "Epoch=89, iteration=1, train_loss=2.639141082763672\n",
      "Epoch=89, iteration=2, train_loss=2.6774353981018066\n",
      "Epoch=89, iteration=3, train_loss=2.825103759765625\n",
      "Epoch=89, val_loss=2.8174335956573486\n",
      "Epoch=90, iteration=0, train_loss=2.658301591873169\n",
      "Epoch=90, iteration=1, train_loss=2.6374850273132324\n",
      "Epoch=90, iteration=2, train_loss=2.675899028778076\n",
      "Epoch=90, iteration=3, train_loss=2.823556661605835\n",
      "Epoch=90, val_loss=2.815978765487671\n",
      "Epoch=91, iteration=0, train_loss=2.6565029621124268\n",
      "Epoch=91, iteration=1, train_loss=2.6358275413513184\n",
      "Epoch=91, iteration=2, train_loss=2.6743626594543457\n",
      "Epoch=91, iteration=3, train_loss=2.822007179260254\n",
      "Epoch=91, val_loss=2.8145196437835693\n",
      "Epoch=92, iteration=0, train_loss=2.6547036170959473\n",
      "Epoch=92, iteration=1, train_loss=2.634171485900879\n",
      "Epoch=92, iteration=2, train_loss=2.672828197479248\n",
      "Epoch=92, iteration=3, train_loss=2.8204574584960938\n",
      "Epoch=92, val_loss=2.813058614730835\n",
      "Epoch=93, iteration=0, train_loss=2.6529059410095215\n",
      "Epoch=93, iteration=1, train_loss=2.632519245147705\n",
      "Epoch=93, iteration=2, train_loss=2.671297550201416\n",
      "Epoch=93, iteration=3, train_loss=2.8189096450805664\n",
      "Epoch=93, val_loss=2.8115971088409424\n",
      "Epoch=94, iteration=0, train_loss=2.6511118412017822\n",
      "Epoch=94, iteration=1, train_loss=2.6308724880218506\n",
      "Epoch=94, iteration=2, train_loss=2.669772148132324\n",
      "Epoch=94, iteration=3, train_loss=2.817366600036621\n",
      "Epoch=94, val_loss=2.810138463973999\n",
      "Epoch=95, iteration=0, train_loss=2.649324655532837\n",
      "Epoch=95, iteration=1, train_loss=2.6292331218719482\n",
      "Epoch=95, iteration=2, train_loss=2.6682541370391846\n",
      "Epoch=95, iteration=3, train_loss=2.8158299922943115\n",
      "Epoch=95, val_loss=2.8086838722229004\n",
      "Epoch=96, iteration=0, train_loss=2.647545337677002\n",
      "Epoch=96, iteration=1, train_loss=2.627603530883789\n",
      "Epoch=96, iteration=2, train_loss=2.6667444705963135\n",
      "Epoch=96, iteration=3, train_loss=2.814303159713745\n",
      "Epoch=96, val_loss=2.8072361946105957\n",
      "Epoch=97, iteration=0, train_loss=2.6457765102386475\n",
      "Epoch=97, iteration=1, train_loss=2.6259853839874268\n",
      "Epoch=97, iteration=2, train_loss=2.6652450561523438\n",
      "Epoch=97, iteration=3, train_loss=2.8127870559692383\n",
      "Epoch=97, val_loss=2.8057966232299805\n",
      "Epoch=98, iteration=0, train_loss=2.64401912689209\n",
      "Epoch=98, iteration=1, train_loss=2.624380111694336\n",
      "Epoch=98, iteration=2, train_loss=2.663756847381592\n",
      "Epoch=98, iteration=3, train_loss=2.811283826828003\n",
      "Epoch=98, val_loss=2.8043670654296875\n",
      "Epoch=99, iteration=0, train_loss=2.64227557182312\n",
      "Epoch=99, iteration=1, train_loss=2.6227896213531494\n",
      "Epoch=99, iteration=2, train_loss=2.662281036376953\n",
      "Epoch=99, iteration=3, train_loss=2.809795618057251\n",
      "Epoch=99, val_loss=2.8029494285583496\n",
      "Epoch=100, iteration=0, train_loss=2.640547037124634\n",
      "Epoch=100, iteration=1, train_loss=2.6212143898010254\n",
      "Epoch=100, iteration=2, train_loss=2.6608176231384277\n",
      "Epoch=100, iteration=3, train_loss=2.808323383331299\n",
      "Epoch=100, val_loss=2.8015449047088623\n",
      "Epoch=101, iteration=0, train_loss=2.638833999633789\n",
      "Epoch=101, iteration=1, train_loss=2.6196553707122803\n",
      "Epoch=101, iteration=2, train_loss=2.6593682765960693\n",
      "Epoch=101, iteration=3, train_loss=2.8068697452545166\n",
      "Epoch=101, val_loss=2.8001551628112793\n",
      "Epoch=102, iteration=0, train_loss=2.6371383666992188\n",
      "Epoch=102, iteration=1, train_loss=2.6181139945983887\n",
      "Epoch=102, iteration=2, train_loss=2.657932996749878\n",
      "Epoch=102, iteration=3, train_loss=2.8054351806640625\n",
      "Epoch=102, val_loss=2.798780679702759\n",
      "Epoch=103, iteration=0, train_loss=2.6354598999023438\n",
      "Epoch=103, iteration=1, train_loss=2.6165895462036133\n",
      "Epoch=103, iteration=2, train_loss=2.6565120220184326\n",
      "Epoch=103, iteration=3, train_loss=2.804020643234253\n",
      "Epoch=103, val_loss=2.7974226474761963\n",
      "Epoch=104, iteration=0, train_loss=2.6338000297546387\n",
      "Epoch=104, iteration=1, train_loss=2.615083694458008\n",
      "Epoch=104, iteration=2, train_loss=2.6551053524017334\n",
      "Epoch=104, iteration=3, train_loss=2.802626609802246\n",
      "Epoch=104, val_loss=2.7960822582244873\n",
      "Epoch=105, iteration=0, train_loss=2.632158041000366\n",
      "Epoch=105, iteration=1, train_loss=2.613595724105835\n",
      "Epoch=105, iteration=2, train_loss=2.653712511062622\n",
      "Epoch=105, iteration=3, train_loss=2.8012545108795166\n",
      "Epoch=105, val_loss=2.7947592735290527\n",
      "Epoch=106, iteration=0, train_loss=2.6305344104766846\n",
      "Epoch=106, iteration=1, train_loss=2.6121256351470947\n",
      "Epoch=106, iteration=2, train_loss=2.6523334980010986\n",
      "Epoch=106, iteration=3, train_loss=2.799903392791748\n",
      "Epoch=106, val_loss=2.793454885482788\n",
      "Epoch=107, iteration=0, train_loss=2.6289286613464355\n",
      "Epoch=107, iteration=1, train_loss=2.610673189163208\n",
      "Epoch=107, iteration=2, train_loss=2.650968074798584\n",
      "Epoch=107, iteration=3, train_loss=2.7985751628875732\n",
      "Epoch=107, val_loss=2.7921690940856934\n",
      "Epoch=108, iteration=0, train_loss=2.6273412704467773\n",
      "Epoch=108, iteration=1, train_loss=2.609238624572754\n",
      "Epoch=108, iteration=2, train_loss=2.6496150493621826\n",
      "Epoch=108, iteration=3, train_loss=2.7972686290740967\n",
      "Epoch=108, val_loss=2.7909016609191895\n",
      "Epoch=109, iteration=0, train_loss=2.6257708072662354\n",
      "Epoch=109, iteration=1, train_loss=2.6078202724456787\n",
      "Epoch=109, iteration=2, train_loss=2.6482746601104736\n",
      "Epoch=109, iteration=3, train_loss=2.7959840297698975\n",
      "Epoch=109, val_loss=2.7896525859832764\n",
      "Epoch=110, iteration=0, train_loss=2.6242175102233887\n",
      "Epoch=110, iteration=1, train_loss=2.6064181327819824\n",
      "Epoch=110, iteration=2, train_loss=2.6469452381134033\n",
      "Epoch=110, iteration=3, train_loss=2.7947211265563965\n",
      "Epoch=110, val_loss=2.788421630859375\n",
      "Epoch=111, iteration=0, train_loss=2.6226806640625\n",
      "Epoch=111, iteration=1, train_loss=2.6050314903259277\n",
      "Epoch=111, iteration=2, train_loss=2.645627021789551\n",
      "Epoch=111, iteration=3, train_loss=2.7934792041778564\n",
      "Epoch=111, val_loss=2.7872092723846436\n",
      "Epoch=112, iteration=0, train_loss=2.621159315109253\n",
      "Epoch=112, iteration=1, train_loss=2.6036593914031982\n",
      "Epoch=112, iteration=2, train_loss=2.6443185806274414\n",
      "Epoch=112, iteration=3, train_loss=2.7922589778900146\n",
      "Epoch=112, val_loss=2.7860143184661865\n",
      "Epoch=113, iteration=0, train_loss=2.619652509689331\n",
      "Epoch=113, iteration=1, train_loss=2.6023013591766357\n",
      "Epoch=113, iteration=2, train_loss=2.643018960952759\n",
      "Epoch=113, iteration=3, train_loss=2.791057825088501\n",
      "Epoch=113, val_loss=2.784836769104004\n",
      "Epoch=114, iteration=0, train_loss=2.6181602478027344\n",
      "Epoch=114, iteration=1, train_loss=2.600956678390503\n",
      "Epoch=114, iteration=2, train_loss=2.641728162765503\n",
      "Epoch=114, iteration=3, train_loss=2.789877414703369\n",
      "Epoch=114, val_loss=2.7836761474609375\n",
      "Epoch=115, iteration=0, train_loss=2.6166810989379883\n",
      "Epoch=115, iteration=1, train_loss=2.599623918533325\n",
      "Epoch=115, iteration=2, train_loss=2.64044451713562\n",
      "Epoch=115, iteration=3, train_loss=2.7887158393859863\n",
      "Epoch=115, val_loss=2.78253173828125\n",
      "Epoch=116, iteration=0, train_loss=2.6152143478393555\n",
      "Epoch=116, iteration=1, train_loss=2.5983030796051025\n",
      "Epoch=116, iteration=2, train_loss=2.639167547225952\n",
      "Epoch=116, iteration=3, train_loss=2.7875728607177734\n",
      "Epoch=116, val_loss=2.7814033031463623\n",
      "Epoch=117, iteration=0, train_loss=2.6137590408325195\n",
      "Epoch=117, iteration=1, train_loss=2.596992254257202\n",
      "Epoch=117, iteration=2, train_loss=2.637897253036499\n",
      "Epoch=117, iteration=3, train_loss=2.786447525024414\n",
      "Epoch=117, val_loss=2.780289888381958\n",
      "Epoch=118, iteration=0, train_loss=2.6123156547546387\n",
      "Epoch=118, iteration=1, train_loss=2.5956926345825195\n",
      "Epoch=118, iteration=2, train_loss=2.6366326808929443\n",
      "Epoch=118, iteration=3, train_loss=2.785338878631592\n",
      "Epoch=118, val_loss=2.779191493988037\n",
      "Epoch=119, iteration=0, train_loss=2.610882043838501\n",
      "Epoch=119, iteration=1, train_loss=2.5944015979766846\n",
      "Epoch=119, iteration=2, train_loss=2.6353728771209717\n",
      "Epoch=119, iteration=3, train_loss=2.7842469215393066\n",
      "Epoch=119, val_loss=2.778107166290283\n",
      "Epoch=120, iteration=0, train_loss=2.6094584465026855\n",
      "Epoch=120, iteration=1, train_loss=2.5931200981140137\n",
      "Epoch=120, iteration=2, train_loss=2.6341183185577393\n",
      "Epoch=120, iteration=3, train_loss=2.783170461654663\n",
      "Epoch=120, val_loss=2.777036666870117\n",
      "Epoch=121, iteration=0, train_loss=2.6080446243286133\n",
      "Epoch=121, iteration=1, train_loss=2.5918467044830322\n",
      "Epoch=121, iteration=2, train_loss=2.6328675746917725\n",
      "Epoch=121, iteration=3, train_loss=2.782109498977661\n",
      "Epoch=121, val_loss=2.7759792804718018\n",
      "Epoch=122, iteration=0, train_loss=2.6066393852233887\n",
      "Epoch=122, iteration=1, train_loss=2.5905818939208984\n",
      "Epoch=122, iteration=2, train_loss=2.6316211223602295\n",
      "Epoch=122, iteration=3, train_loss=2.781061887741089\n",
      "Epoch=122, val_loss=2.774934768676758\n",
      "Epoch=123, iteration=0, train_loss=2.605242967605591\n",
      "Epoch=123, iteration=1, train_loss=2.589324951171875\n",
      "Epoch=123, iteration=2, train_loss=2.630378007888794\n",
      "Epoch=123, iteration=3, train_loss=2.780028820037842\n",
      "Epoch=123, val_loss=2.77390193939209\n",
      "Epoch=124, iteration=0, train_loss=2.6038546562194824\n",
      "Epoch=124, iteration=1, train_loss=2.5880746841430664\n",
      "Epoch=124, iteration=2, train_loss=2.629138946533203\n",
      "Epoch=124, iteration=3, train_loss=2.7790088653564453\n",
      "Epoch=124, val_loss=2.772881031036377\n",
      "Epoch=125, iteration=0, train_loss=2.6024739742279053\n",
      "Epoch=125, iteration=1, train_loss=2.5868325233459473\n",
      "Epoch=125, iteration=2, train_loss=2.6279032230377197\n",
      "Epoch=125, iteration=3, train_loss=2.778001070022583\n",
      "Epoch=125, val_loss=2.771872043609619\n",
      "Epoch=126, iteration=0, train_loss=2.6011016368865967\n",
      "Epoch=126, iteration=1, train_loss=2.585597038269043\n",
      "Epoch=126, iteration=2, train_loss=2.6266708374023438\n",
      "Epoch=126, iteration=3, train_loss=2.777005910873413\n",
      "Epoch=126, val_loss=2.770873546600342\n",
      "Epoch=127, iteration=0, train_loss=2.5997369289398193\n",
      "Epoch=127, iteration=1, train_loss=2.584368944168091\n",
      "Epoch=127, iteration=2, train_loss=2.6254422664642334\n",
      "Epoch=127, iteration=3, train_loss=2.776022434234619\n",
      "Epoch=127, val_loss=2.769885778427124\n",
      "Epoch=128, iteration=0, train_loss=2.5983798503875732\n",
      "Epoch=128, iteration=1, train_loss=2.5831477642059326\n",
      "Epoch=128, iteration=2, train_loss=2.6242177486419678\n",
      "Epoch=128, iteration=3, train_loss=2.775050163269043\n",
      "Epoch=128, val_loss=2.7689082622528076\n",
      "Epoch=129, iteration=0, train_loss=2.5970304012298584\n",
      "Epoch=129, iteration=1, train_loss=2.5819339752197266\n",
      "Epoch=129, iteration=2, train_loss=2.6229968070983887\n",
      "Epoch=129, iteration=3, train_loss=2.7740888595581055\n",
      "Epoch=129, val_loss=2.7679405212402344\n",
      "Epoch=130, iteration=0, train_loss=2.595688819885254\n",
      "Epoch=130, iteration=1, train_loss=2.5807275772094727\n",
      "Epoch=130, iteration=2, train_loss=2.6217801570892334\n",
      "Epoch=130, iteration=3, train_loss=2.7731375694274902\n",
      "Epoch=130, val_loss=2.7669830322265625\n",
      "Epoch=131, iteration=0, train_loss=2.594355344772339\n",
      "Epoch=131, iteration=1, train_loss=2.579528570175171\n",
      "Epoch=131, iteration=2, train_loss=2.620567560195923\n",
      "Epoch=131, iteration=3, train_loss=2.7721967697143555\n",
      "Epoch=131, val_loss=2.7660346031188965\n",
      "Epoch=132, iteration=0, train_loss=2.5930299758911133\n",
      "Epoch=132, iteration=1, train_loss=2.5783371925354004\n",
      "Epoch=132, iteration=2, train_loss=2.6193597316741943\n",
      "Epoch=132, iteration=3, train_loss=2.7712655067443848\n",
      "Epoch=132, val_loss=2.7650954723358154\n",
      "Epoch=133, iteration=0, train_loss=2.5917134284973145\n",
      "Epoch=133, iteration=1, train_loss=2.5771536827087402\n",
      "Epoch=133, iteration=2, train_loss=2.618157148361206\n",
      "Epoch=133, iteration=3, train_loss=2.7703440189361572\n",
      "Epoch=133, val_loss=2.7641654014587402\n",
      "Epoch=134, iteration=0, train_loss=2.590404987335205\n",
      "Epoch=134, iteration=1, train_loss=2.5759787559509277\n",
      "Epoch=134, iteration=2, train_loss=2.616959571838379\n",
      "Epoch=134, iteration=3, train_loss=2.7694313526153564\n",
      "Epoch=134, val_loss=2.763244152069092\n",
      "Epoch=135, iteration=0, train_loss=2.5891056060791016\n",
      "Epoch=135, iteration=1, train_loss=2.5748116970062256\n",
      "Epoch=135, iteration=2, train_loss=2.6157681941986084\n",
      "Epoch=135, iteration=3, train_loss=2.7685277462005615\n",
      "Epoch=135, val_loss=2.762331485748291\n",
      "Epoch=136, iteration=0, train_loss=2.587815523147583\n",
      "Epoch=136, iteration=1, train_loss=2.5736536979675293\n",
      "Epoch=136, iteration=2, train_loss=2.6145827770233154\n",
      "Epoch=136, iteration=3, train_loss=2.7676329612731934\n",
      "Epoch=136, val_loss=2.7614269256591797\n",
      "Epoch=137, iteration=0, train_loss=2.5865347385406494\n",
      "Epoch=137, iteration=1, train_loss=2.572504997253418\n",
      "Epoch=137, iteration=2, train_loss=2.613403797149658\n",
      "Epoch=137, iteration=3, train_loss=2.7667462825775146\n",
      "Epoch=137, val_loss=2.760531187057495\n",
      "Epoch=138, iteration=0, train_loss=2.585264205932617\n",
      "Epoch=138, iteration=1, train_loss=2.5713648796081543\n",
      "Epoch=138, iteration=2, train_loss=2.612231492996216\n",
      "Epoch=138, iteration=3, train_loss=2.7658677101135254\n",
      "Epoch=138, val_loss=2.759643316268921\n",
      "Epoch=139, iteration=0, train_loss=2.584003210067749\n",
      "Epoch=139, iteration=1, train_loss=2.570235013961792\n",
      "Epoch=139, iteration=2, train_loss=2.6110665798187256\n",
      "Epoch=139, iteration=3, train_loss=2.7649972438812256\n",
      "Epoch=139, val_loss=2.758763074874878\n",
      "Epoch=140, iteration=0, train_loss=2.5827529430389404\n",
      "Epoch=140, iteration=1, train_loss=2.569115161895752\n",
      "Epoch=140, iteration=2, train_loss=2.609909772872925\n",
      "Epoch=140, iteration=3, train_loss=2.764134645462036\n",
      "Epoch=140, val_loss=2.7578909397125244\n",
      "Epoch=141, iteration=0, train_loss=2.5815134048461914\n",
      "Epoch=141, iteration=1, train_loss=2.5680055618286133\n",
      "Epoch=141, iteration=2, train_loss=2.6087610721588135\n",
      "Epoch=141, iteration=3, train_loss=2.7632791996002197\n",
      "Epoch=141, val_loss=2.757026195526123\n",
      "Epoch=142, iteration=0, train_loss=2.5802855491638184\n",
      "Epoch=142, iteration=1, train_loss=2.566906452178955\n",
      "Epoch=142, iteration=2, train_loss=2.6076204776763916\n",
      "Epoch=142, iteration=3, train_loss=2.7624313831329346\n",
      "Epoch=142, val_loss=2.756169080734253\n",
      "Epoch=143, iteration=0, train_loss=2.579068660736084\n",
      "Epoch=143, iteration=1, train_loss=2.5658183097839355\n",
      "Epoch=143, iteration=2, train_loss=2.6064891815185547\n",
      "Epoch=143, iteration=3, train_loss=2.7615904808044434\n",
      "Epoch=143, val_loss=2.755319118499756\n",
      "Epoch=144, iteration=0, train_loss=2.577863931655884\n",
      "Epoch=144, iteration=1, train_loss=2.5647411346435547\n",
      "Epoch=144, iteration=2, train_loss=2.6053659915924072\n",
      "Epoch=144, iteration=3, train_loss=2.760756731033325\n",
      "Epoch=144, val_loss=2.754476308822632\n",
      "Epoch=145, iteration=0, train_loss=2.5766711235046387\n",
      "Epoch=145, iteration=1, train_loss=2.56367564201355\n",
      "Epoch=145, iteration=2, train_loss=2.6042532920837402\n",
      "Epoch=145, iteration=3, train_loss=2.7599291801452637\n",
      "Epoch=145, val_loss=2.7536404132843018\n",
      "Epoch=146, iteration=0, train_loss=2.575490713119507\n",
      "Epoch=146, iteration=1, train_loss=2.562621831893921\n",
      "Epoch=146, iteration=2, train_loss=2.6031506061553955\n",
      "Epoch=146, iteration=3, train_loss=2.759108781814575\n",
      "Epoch=146, val_loss=2.752811908721924\n",
      "Epoch=147, iteration=0, train_loss=2.5743231773376465\n",
      "Epoch=147, iteration=1, train_loss=2.5615792274475098\n",
      "Epoch=147, iteration=2, train_loss=2.602057456970215\n",
      "Epoch=147, iteration=3, train_loss=2.758293867111206\n",
      "Epoch=147, val_loss=2.7519898414611816\n",
      "Epoch=148, iteration=0, train_loss=2.5731680393218994\n",
      "Epoch=148, iteration=1, train_loss=2.560549020767212\n",
      "Epoch=148, iteration=2, train_loss=2.600975275039673\n",
      "Epoch=148, iteration=3, train_loss=2.7574853897094727\n",
      "Epoch=148, val_loss=2.751173973083496\n",
      "Epoch=149, iteration=0, train_loss=2.572025775909424\n",
      "Epoch=149, iteration=1, train_loss=2.55953049659729\n",
      "Epoch=149, iteration=2, train_loss=2.5999033451080322\n",
      "Epoch=149, iteration=3, train_loss=2.756682872772217\n",
      "Epoch=149, val_loss=2.7503647804260254\n",
      "Epoch=150, iteration=0, train_loss=2.570897340774536\n",
      "Epoch=150, iteration=1, train_loss=2.5585246086120605\n",
      "Epoch=150, iteration=2, train_loss=2.598841905593872\n",
      "Epoch=150, iteration=3, train_loss=2.7558863162994385\n",
      "Epoch=150, val_loss=2.7495615482330322\n",
      "Epoch=151, iteration=0, train_loss=2.56978178024292\n",
      "Epoch=151, iteration=1, train_loss=2.5575311183929443\n",
      "Epoch=151, iteration=2, train_loss=2.597792387008667\n",
      "Epoch=151, iteration=3, train_loss=2.755094528198242\n",
      "Epoch=151, val_loss=2.7487637996673584\n",
      "Epoch=152, iteration=0, train_loss=2.5686800479888916\n",
      "Epoch=152, iteration=1, train_loss=2.5565497875213623\n",
      "Epoch=152, iteration=2, train_loss=2.596752882003784\n",
      "Epoch=152, iteration=3, train_loss=2.7543084621429443\n",
      "Epoch=152, val_loss=2.7479724884033203\n",
      "Epoch=153, iteration=0, train_loss=2.567591428756714\n",
      "Epoch=153, iteration=1, train_loss=2.5555808544158936\n",
      "Epoch=153, iteration=2, train_loss=2.5957257747650146\n",
      "Epoch=153, iteration=3, train_loss=2.7535269260406494\n",
      "Epoch=153, val_loss=2.7471864223480225\n",
      "Epoch=154, iteration=0, train_loss=2.566516876220703\n",
      "Epoch=154, iteration=1, train_loss=2.554624557495117\n",
      "Epoch=154, iteration=2, train_loss=2.5947093963623047\n",
      "Epoch=154, iteration=3, train_loss=2.752750873565674\n",
      "Epoch=154, val_loss=2.746405839920044\n",
      "Epoch=155, iteration=0, train_loss=2.565455675125122\n",
      "Epoch=155, iteration=1, train_loss=2.553680419921875\n",
      "Epoch=155, iteration=2, train_loss=2.5937044620513916\n",
      "Epoch=155, iteration=3, train_loss=2.751978635787964\n",
      "Epoch=155, val_loss=2.745630979537964\n",
      "Epoch=156, iteration=0, train_loss=2.56440806388855\n",
      "Epoch=156, iteration=1, train_loss=2.552748918533325\n",
      "Epoch=156, iteration=2, train_loss=2.5927112102508545\n",
      "Epoch=156, iteration=3, train_loss=2.751211404800415\n",
      "Epoch=156, val_loss=2.744861125946045\n",
      "Epoch=157, iteration=0, train_loss=2.5633745193481445\n",
      "Epoch=157, iteration=1, train_loss=2.5518293380737305\n",
      "Epoch=157, iteration=2, train_loss=2.5917294025421143\n",
      "Epoch=157, iteration=3, train_loss=2.75044846534729\n",
      "Epoch=157, val_loss=2.7440953254699707\n",
      "Epoch=158, iteration=0, train_loss=2.5623538494110107\n",
      "Epoch=158, iteration=1, train_loss=2.550922393798828\n",
      "Epoch=158, iteration=2, train_loss=2.59075927734375\n",
      "Epoch=158, iteration=3, train_loss=2.7496893405914307\n",
      "Epoch=158, val_loss=2.7433347702026367\n",
      "Epoch=159, iteration=0, train_loss=2.561347246170044\n",
      "Epoch=159, iteration=1, train_loss=2.550027370452881\n",
      "Epoch=159, iteration=2, train_loss=2.5898001194000244\n",
      "Epoch=159, iteration=3, train_loss=2.748933792114258\n",
      "Epoch=159, val_loss=2.7425789833068848\n",
      "Epoch=160, iteration=0, train_loss=2.560354232788086\n",
      "Epoch=160, iteration=1, train_loss=2.549144744873047\n",
      "Epoch=160, iteration=2, train_loss=2.588852882385254\n",
      "Epoch=160, iteration=3, train_loss=2.748182535171509\n",
      "Epoch=160, val_loss=2.7418274879455566\n",
      "Epoch=161, iteration=0, train_loss=2.5593740940093994\n",
      "Epoch=161, iteration=1, train_loss=2.5482735633850098\n",
      "Epoch=161, iteration=2, train_loss=2.587916612625122\n",
      "Epoch=161, iteration=3, train_loss=2.747434377670288\n",
      "Epoch=161, val_loss=2.741079807281494\n",
      "Epoch=162, iteration=0, train_loss=2.558407783508301\n",
      "Epoch=162, iteration=1, train_loss=2.5474138259887695\n",
      "Epoch=162, iteration=2, train_loss=2.586991786956787\n",
      "Epoch=162, iteration=3, train_loss=2.746689558029175\n",
      "Epoch=162, val_loss=2.7403361797332764\n",
      "Epoch=163, iteration=0, train_loss=2.5574541091918945\n",
      "Epoch=163, iteration=1, train_loss=2.5465660095214844\n",
      "Epoch=163, iteration=2, train_loss=2.586077928543091\n",
      "Epoch=163, iteration=3, train_loss=2.745948076248169\n",
      "Epoch=163, val_loss=2.7395966053009033\n",
      "Epoch=164, iteration=0, train_loss=2.5565133094787598\n",
      "Epoch=164, iteration=1, train_loss=2.545729160308838\n",
      "Epoch=164, iteration=2, train_loss=2.5851755142211914\n",
      "Epoch=164, iteration=3, train_loss=2.7452099323272705\n",
      "Epoch=164, val_loss=2.738860607147217\n",
      "Epoch=165, iteration=0, train_loss=2.5555849075317383\n",
      "Epoch=165, iteration=1, train_loss=2.544903516769409\n",
      "Epoch=165, iteration=2, train_loss=2.5842840671539307\n",
      "Epoch=165, iteration=3, train_loss=2.744474411010742\n",
      "Epoch=165, val_loss=2.738128185272217\n",
      "Epoch=166, iteration=0, train_loss=2.5546696186065674\n",
      "Epoch=166, iteration=1, train_loss=2.54408860206604\n",
      "Epoch=166, iteration=2, train_loss=2.5834028720855713\n",
      "Epoch=166, iteration=3, train_loss=2.743741989135742\n",
      "Epoch=166, val_loss=2.737399101257324\n",
      "Epoch=167, iteration=0, train_loss=2.5537660121917725\n",
      "Epoch=167, iteration=1, train_loss=2.5432844161987305\n",
      "Epoch=167, iteration=2, train_loss=2.5825324058532715\n",
      "Epoch=167, iteration=3, train_loss=2.743011713027954\n",
      "Epoch=167, val_loss=2.736673593521118\n",
      "Epoch=168, iteration=0, train_loss=2.55287504196167\n",
      "Epoch=168, iteration=1, train_loss=2.5424904823303223\n",
      "Epoch=168, iteration=2, train_loss=2.5816729068756104\n",
      "Epoch=168, iteration=3, train_loss=2.7422845363616943\n",
      "Epoch=168, val_loss=2.7359509468078613\n",
      "Epoch=169, iteration=0, train_loss=2.5519957542419434\n",
      "Epoch=169, iteration=1, train_loss=2.5417068004608154\n",
      "Epoch=169, iteration=2, train_loss=2.5808231830596924\n",
      "Epoch=169, iteration=3, train_loss=2.7415595054626465\n",
      "Epoch=169, val_loss=2.735231399536133\n",
      "Epoch=170, iteration=0, train_loss=2.5511279106140137\n",
      "Epoch=170, iteration=1, train_loss=2.540933132171631\n",
      "Epoch=170, iteration=2, train_loss=2.579983711242676\n",
      "Epoch=170, iteration=3, train_loss=2.7408370971679688\n",
      "Epoch=170, val_loss=2.7345147132873535\n",
      "Epoch=171, iteration=0, train_loss=2.550271511077881\n",
      "Epoch=171, iteration=1, train_loss=2.5401690006256104\n",
      "Epoch=171, iteration=2, train_loss=2.5791542530059814\n",
      "Epoch=171, iteration=3, train_loss=2.740117073059082\n",
      "Epoch=171, val_loss=2.7338011264801025\n",
      "Epoch=172, iteration=0, train_loss=2.549426317214966\n",
      "Epoch=172, iteration=1, train_loss=2.539414405822754\n",
      "Epoch=172, iteration=2, train_loss=2.578334331512451\n",
      "Epoch=172, iteration=3, train_loss=2.739398717880249\n",
      "Epoch=172, val_loss=2.7330899238586426\n",
      "Epoch=173, iteration=0, train_loss=2.548591375350952\n",
      "Epoch=173, iteration=1, train_loss=2.538668632507324\n",
      "Epoch=173, iteration=2, train_loss=2.577523946762085\n",
      "Epoch=173, iteration=3, train_loss=2.7386832237243652\n",
      "Epoch=173, val_loss=2.732381820678711\n",
      "Epoch=174, iteration=0, train_loss=2.5477676391601562\n",
      "Epoch=174, iteration=1, train_loss=2.5379316806793213\n",
      "Epoch=174, iteration=2, train_loss=2.576723098754883\n",
      "Epoch=174, iteration=3, train_loss=2.737969160079956\n",
      "Epoch=174, val_loss=2.7316761016845703\n",
      "Epoch=175, iteration=0, train_loss=2.5469539165496826\n",
      "Epoch=175, iteration=1, train_loss=2.5372040271759033\n",
      "Epoch=175, iteration=2, train_loss=2.5759308338165283\n",
      "Epoch=175, iteration=3, train_loss=2.737257480621338\n",
      "Epoch=175, val_loss=2.7309730052948\n",
      "Epoch=176, iteration=0, train_loss=2.5461504459381104\n",
      "Epoch=176, iteration=1, train_loss=2.536484479904175\n",
      "Epoch=176, iteration=2, train_loss=2.575148105621338\n",
      "Epoch=176, iteration=3, train_loss=2.7365481853485107\n",
      "Epoch=176, val_loss=2.7302722930908203\n",
      "Epoch=177, iteration=0, train_loss=2.545356512069702\n",
      "Epoch=177, iteration=1, train_loss=2.5357725620269775\n",
      "Epoch=177, iteration=2, train_loss=2.574373722076416\n",
      "Epoch=177, iteration=3, train_loss=2.7358407974243164\n",
      "Epoch=177, val_loss=2.729573965072632\n",
      "Epoch=178, iteration=0, train_loss=2.544572353363037\n",
      "Epoch=178, iteration=1, train_loss=2.5350687503814697\n",
      "Epoch=178, iteration=2, train_loss=2.573608160018921\n",
      "Epoch=178, iteration=3, train_loss=2.735135555267334\n",
      "Epoch=178, val_loss=2.7288784980773926\n",
      "Epoch=179, iteration=0, train_loss=2.543797016143799\n",
      "Epoch=179, iteration=1, train_loss=2.5343730449676514\n",
      "Epoch=179, iteration=2, train_loss=2.5728507041931152\n",
      "Epoch=179, iteration=3, train_loss=2.7344319820404053\n",
      "Epoch=179, val_loss=2.728184938430786\n",
      "Epoch=180, iteration=0, train_loss=2.5430309772491455\n",
      "Epoch=180, iteration=1, train_loss=2.533684253692627\n",
      "Epoch=180, iteration=2, train_loss=2.5721020698547363\n",
      "Epoch=180, iteration=3, train_loss=2.7337307929992676\n",
      "Epoch=180, val_loss=2.7274937629699707\n",
      "Epoch=181, iteration=0, train_loss=2.542273759841919\n",
      "Epoch=181, iteration=1, train_loss=2.533003330230713\n",
      "Epoch=181, iteration=2, train_loss=2.5713605880737305\n",
      "Epoch=181, iteration=3, train_loss=2.7330315113067627\n",
      "Epoch=181, val_loss=2.7268052101135254\n",
      "Epoch=182, iteration=0, train_loss=2.541524887084961\n",
      "Epoch=182, iteration=1, train_loss=2.5323288440704346\n",
      "Epoch=182, iteration=2, train_loss=2.570626974105835\n",
      "Epoch=182, iteration=3, train_loss=2.7323341369628906\n",
      "Epoch=182, val_loss=2.726118803024292\n",
      "Epoch=183, iteration=0, train_loss=2.5407845973968506\n",
      "Epoch=183, iteration=1, train_loss=2.531661033630371\n",
      "Epoch=183, iteration=2, train_loss=2.5699009895324707\n",
      "Epoch=183, iteration=3, train_loss=2.7316391468048096\n",
      "Epoch=183, val_loss=2.7254347801208496\n",
      "Epoch=184, iteration=0, train_loss=2.5400519371032715\n",
      "Epoch=184, iteration=1, train_loss=2.5309998989105225\n",
      "Epoch=184, iteration=2, train_loss=2.569182872772217\n",
      "Epoch=184, iteration=3, train_loss=2.7309460639953613\n",
      "Epoch=184, val_loss=2.72475266456604\n",
      "Epoch=185, iteration=0, train_loss=2.5393271446228027\n",
      "Epoch=185, iteration=1, train_loss=2.5303452014923096\n",
      "Epoch=185, iteration=2, train_loss=2.568471670150757\n",
      "Epoch=185, iteration=3, train_loss=2.730255365371704\n",
      "Epoch=185, val_loss=2.7240731716156006\n",
      "Epoch=186, iteration=0, train_loss=2.538609743118286\n",
      "Epoch=186, iteration=1, train_loss=2.529695749282837\n",
      "Epoch=186, iteration=2, train_loss=2.5677671432495117\n",
      "Epoch=186, iteration=3, train_loss=2.7295665740966797\n",
      "Epoch=186, val_loss=2.723395824432373\n",
      "Epoch=187, iteration=0, train_loss=2.5378997325897217\n",
      "Epoch=187, iteration=1, train_loss=2.529052972793579\n",
      "Epoch=187, iteration=2, train_loss=2.5670695304870605\n",
      "Epoch=187, iteration=3, train_loss=2.7288801670074463\n",
      "Epoch=187, val_loss=2.7227213382720947\n",
      "Epoch=188, iteration=0, train_loss=2.5371968746185303\n",
      "Epoch=188, iteration=1, train_loss=2.5284154415130615\n",
      "Epoch=188, iteration=2, train_loss=2.5663788318634033\n",
      "Epoch=188, iteration=3, train_loss=2.7281956672668457\n",
      "Epoch=188, val_loss=2.72204852104187\n",
      "Epoch=189, iteration=0, train_loss=2.5365006923675537\n",
      "Epoch=189, iteration=1, train_loss=2.5277841091156006\n",
      "Epoch=189, iteration=2, train_loss=2.565694570541382\n",
      "Epoch=189, iteration=3, train_loss=2.7275137901306152\n",
      "Epoch=189, val_loss=2.7213783264160156\n",
      "Epoch=190, iteration=0, train_loss=2.535811185836792\n",
      "Epoch=190, iteration=1, train_loss=2.5271573066711426\n",
      "Epoch=190, iteration=2, train_loss=2.565016508102417\n",
      "Epoch=190, iteration=3, train_loss=2.726834297180176\n",
      "Epoch=190, val_loss=2.7207107543945312\n",
      "Epoch=191, iteration=0, train_loss=2.535127639770508\n",
      "Epoch=191, iteration=1, train_loss=2.526536226272583\n",
      "Epoch=191, iteration=2, train_loss=2.564344644546509\n",
      "Epoch=191, iteration=3, train_loss=2.7261569499969482\n",
      "Epoch=191, val_loss=2.7200450897216797\n",
      "Epoch=192, iteration=0, train_loss=2.5344507694244385\n",
      "Epoch=192, iteration=1, train_loss=2.5259196758270264\n",
      "Epoch=192, iteration=2, train_loss=2.5636792182922363\n",
      "Epoch=192, iteration=3, train_loss=2.725482225418091\n",
      "Epoch=192, val_loss=2.7193820476531982\n",
      "Epoch=193, iteration=0, train_loss=2.5337796211242676\n",
      "Epoch=193, iteration=1, train_loss=2.5253076553344727\n",
      "Epoch=193, iteration=2, train_loss=2.5630195140838623\n",
      "Epoch=193, iteration=3, train_loss=2.7248098850250244\n",
      "Epoch=193, val_loss=2.718721866607666\n",
      "Epoch=194, iteration=0, train_loss=2.533113956451416\n",
      "Epoch=194, iteration=1, train_loss=2.52470064163208\n",
      "Epoch=194, iteration=2, train_loss=2.5623650550842285\n",
      "Epoch=194, iteration=3, train_loss=2.724140167236328\n",
      "Epoch=194, val_loss=2.7180638313293457\n",
      "Epoch=195, iteration=0, train_loss=2.532454490661621\n",
      "Epoch=195, iteration=1, train_loss=2.5240979194641113\n",
      "Epoch=195, iteration=2, train_loss=2.5617167949676514\n",
      "Epoch=195, iteration=3, train_loss=2.723473310470581\n",
      "Epoch=195, val_loss=2.7174079418182373\n",
      "Epoch=196, iteration=0, train_loss=2.5318000316619873\n",
      "Epoch=196, iteration=1, train_loss=2.5234992504119873\n",
      "Epoch=196, iteration=2, train_loss=2.5610733032226562\n",
      "Epoch=196, iteration=3, train_loss=2.722809076309204\n",
      "Epoch=196, val_loss=2.716754913330078\n",
      "Epoch=197, iteration=0, train_loss=2.5311505794525146\n",
      "Epoch=197, iteration=1, train_loss=2.5229053497314453\n",
      "Epoch=197, iteration=2, train_loss=2.5604357719421387\n",
      "Epoch=197, iteration=3, train_loss=2.722146987915039\n",
      "Epoch=197, val_loss=2.716104745864868\n",
      "Epoch=198, iteration=0, train_loss=2.5305063724517822\n",
      "Epoch=198, iteration=1, train_loss=2.5223145484924316\n",
      "Epoch=198, iteration=2, train_loss=2.559803009033203\n",
      "Epoch=198, iteration=3, train_loss=2.7214879989624023\n",
      "Epoch=198, val_loss=2.71545672416687\n",
      "Epoch=199, iteration=0, train_loss=2.529866933822632\n",
      "Epoch=199, iteration=1, train_loss=2.521728038787842\n",
      "Epoch=199, iteration=2, train_loss=2.5591752529144287\n",
      "Epoch=199, iteration=3, train_loss=2.7208316326141357\n",
      "Epoch=199, val_loss=2.7148115634918213\n",
      "Epoch=200, iteration=0, train_loss=2.5292320251464844\n",
      "Epoch=200, iteration=1, train_loss=2.5211451053619385\n",
      "Epoch=200, iteration=2, train_loss=2.5585527420043945\n",
      "Epoch=200, iteration=3, train_loss=2.7201786041259766\n",
      "Epoch=200, val_loss=2.7141690254211426\n",
      "Epoch=201, iteration=0, train_loss=2.528601884841919\n",
      "Epoch=201, iteration=1, train_loss=2.520565986633301\n",
      "Epoch=201, iteration=2, train_loss=2.557934284210205\n",
      "Epoch=201, iteration=3, train_loss=2.7195281982421875\n",
      "Epoch=201, val_loss=2.713528871536255\n",
      "Epoch=202, iteration=0, train_loss=2.5279760360717773\n",
      "Epoch=202, iteration=1, train_loss=2.5199902057647705\n",
      "Epoch=202, iteration=2, train_loss=2.557321071624756\n",
      "Epoch=202, iteration=3, train_loss=2.7188806533813477\n",
      "Epoch=202, val_loss=2.7128915786743164\n",
      "Epoch=203, iteration=0, train_loss=2.5273542404174805\n",
      "Epoch=203, iteration=1, train_loss=2.5194172859191895\n",
      "Epoch=203, iteration=2, train_loss=2.5567123889923096\n",
      "Epoch=203, iteration=3, train_loss=2.718235969543457\n",
      "Epoch=203, val_loss=2.7122573852539062\n",
      "Epoch=204, iteration=0, train_loss=2.526736259460449\n",
      "Epoch=204, iteration=1, train_loss=2.518847942352295\n",
      "Epoch=204, iteration=2, train_loss=2.556107997894287\n",
      "Epoch=204, iteration=3, train_loss=2.717594861984253\n",
      "Epoch=204, val_loss=2.711625337600708\n",
      "Epoch=205, iteration=0, train_loss=2.526122570037842\n",
      "Epoch=205, iteration=1, train_loss=2.5182812213897705\n",
      "Epoch=205, iteration=2, train_loss=2.5555078983306885\n",
      "Epoch=205, iteration=3, train_loss=2.716956377029419\n",
      "Epoch=205, val_loss=2.710996150970459\n",
      "Epoch=206, iteration=0, train_loss=2.5255119800567627\n",
      "Epoch=206, iteration=1, train_loss=2.5177183151245117\n",
      "Epoch=206, iteration=2, train_loss=2.5549118518829346\n",
      "Epoch=206, iteration=3, train_loss=2.7163209915161133\n",
      "Epoch=206, val_loss=2.7103703022003174\n",
      "Epoch=207, iteration=0, train_loss=2.5249054431915283\n",
      "Epoch=207, iteration=1, train_loss=2.5171573162078857\n",
      "Epoch=207, iteration=2, train_loss=2.5543200969696045\n",
      "Epoch=207, iteration=3, train_loss=2.715688705444336\n",
      "Epoch=207, val_loss=2.7097465991973877\n",
      "Epoch=208, iteration=0, train_loss=2.524301528930664\n",
      "Epoch=208, iteration=1, train_loss=2.516599178314209\n",
      "Epoch=208, iteration=2, train_loss=2.55373215675354\n",
      "Epoch=208, iteration=3, train_loss=2.715060234069824\n",
      "Epoch=208, val_loss=2.7091262340545654\n",
      "Epoch=209, iteration=0, train_loss=2.5237016677856445\n",
      "Epoch=209, iteration=1, train_loss=2.5160439014434814\n",
      "Epoch=209, iteration=2, train_loss=2.5531485080718994\n",
      "Epoch=209, iteration=3, train_loss=2.7144341468811035\n",
      "Epoch=209, val_loss=2.7085084915161133\n",
      "Epoch=210, iteration=0, train_loss=2.5231049060821533\n",
      "Epoch=210, iteration=1, train_loss=2.515491008758545\n",
      "Epoch=210, iteration=2, train_loss=2.5525684356689453\n",
      "Epoch=210, iteration=3, train_loss=2.7138116359710693\n",
      "Epoch=210, val_loss=2.7078936100006104\n",
      "Epoch=211, iteration=0, train_loss=2.5225110054016113\n",
      "Epoch=211, iteration=1, train_loss=2.5149405002593994\n",
      "Epoch=211, iteration=2, train_loss=2.5519919395446777\n",
      "Epoch=211, iteration=3, train_loss=2.7131927013397217\n",
      "Epoch=211, val_loss=2.7072815895080566\n",
      "Epoch=212, iteration=0, train_loss=2.5219199657440186\n",
      "Epoch=212, iteration=1, train_loss=2.514392375946045\n",
      "Epoch=212, iteration=2, train_loss=2.5514190196990967\n",
      "Epoch=212, iteration=3, train_loss=2.7125766277313232\n",
      "Epoch=212, val_loss=2.706672430038452\n",
      "Epoch=213, iteration=0, train_loss=2.521332025527954\n",
      "Epoch=213, iteration=1, train_loss=2.5138461589813232\n",
      "Epoch=213, iteration=2, train_loss=2.5508499145507812\n",
      "Epoch=213, iteration=3, train_loss=2.7119641304016113\n",
      "Epoch=213, val_loss=2.706066131591797\n",
      "Epoch=214, iteration=0, train_loss=2.5207462310791016\n",
      "Epoch=214, iteration=1, train_loss=2.5133025646209717\n",
      "Epoch=214, iteration=2, train_loss=2.5502841472625732\n",
      "Epoch=214, iteration=3, train_loss=2.711354970932007\n",
      "Epoch=214, val_loss=2.70546293258667\n",
      "Epoch=215, iteration=0, train_loss=2.5201635360717773\n",
      "Epoch=215, iteration=1, train_loss=2.512760639190674\n",
      "Epoch=215, iteration=2, train_loss=2.5497214794158936\n",
      "Epoch=215, iteration=3, train_loss=2.7107491493225098\n",
      "Epoch=215, val_loss=2.7048628330230713\n",
      "Epoch=216, iteration=0, train_loss=2.5195834636688232\n",
      "Epoch=216, iteration=1, train_loss=2.512221097946167\n",
      "Epoch=216, iteration=2, train_loss=2.5491623878479004\n",
      "Epoch=216, iteration=3, train_loss=2.710146903991699\n",
      "Epoch=216, val_loss=2.7042651176452637\n",
      "Epoch=217, iteration=0, train_loss=2.519005298614502\n",
      "Epoch=217, iteration=1, train_loss=2.5116829872131348\n",
      "Epoch=217, iteration=2, train_loss=2.5486059188842773\n",
      "Epoch=217, iteration=3, train_loss=2.709547996520996\n",
      "Epoch=217, val_loss=2.7036707401275635\n",
      "Epoch=218, iteration=0, train_loss=2.518429756164551\n",
      "Epoch=218, iteration=1, train_loss=2.5111474990844727\n",
      "Epoch=218, iteration=2, train_loss=2.548053503036499\n",
      "Epoch=218, iteration=3, train_loss=2.7089529037475586\n",
      "Epoch=218, val_loss=2.7030792236328125\n",
      "Epoch=219, iteration=0, train_loss=2.5178565979003906\n",
      "Epoch=219, iteration=1, train_loss=2.510613203048706\n",
      "Epoch=219, iteration=2, train_loss=2.54750394821167\n",
      "Epoch=219, iteration=3, train_loss=2.7083609104156494\n",
      "Epoch=219, val_loss=2.702491044998169\n",
      "Epoch=220, iteration=0, train_loss=2.517284870147705\n",
      "Epoch=220, iteration=1, train_loss=2.5100808143615723\n",
      "Epoch=220, iteration=2, train_loss=2.54695725440979\n",
      "Epoch=220, iteration=3, train_loss=2.7077722549438477\n",
      "Epoch=220, val_loss=2.7019054889678955\n",
      "Epoch=221, iteration=0, train_loss=2.5167155265808105\n",
      "Epoch=221, iteration=1, train_loss=2.509550094604492\n",
      "Epoch=221, iteration=2, train_loss=2.5464134216308594\n",
      "Epoch=221, iteration=3, train_loss=2.7071874141693115\n",
      "Epoch=221, val_loss=2.7013227939605713\n",
      "Epoch=222, iteration=0, train_loss=2.516148328781128\n",
      "Epoch=222, iteration=1, train_loss=2.509021043777466\n",
      "Epoch=222, iteration=2, train_loss=2.545872449874878\n",
      "Epoch=222, iteration=3, train_loss=2.706606149673462\n",
      "Epoch=222, val_loss=2.7007436752319336\n",
      "Epoch=223, iteration=0, train_loss=2.515583038330078\n",
      "Epoch=223, iteration=1, train_loss=2.508493661880493\n",
      "Epoch=223, iteration=2, train_loss=2.5453343391418457\n",
      "Epoch=223, iteration=3, train_loss=2.7060279846191406\n",
      "Epoch=223, val_loss=2.700167179107666\n",
      "Epoch=224, iteration=0, train_loss=2.515019655227661\n",
      "Epoch=224, iteration=1, train_loss=2.507967710494995\n",
      "Epoch=224, iteration=2, train_loss=2.544799327850342\n",
      "Epoch=224, iteration=3, train_loss=2.705453634262085\n",
      "Epoch=224, val_loss=2.6995937824249268\n",
      "Epoch=225, iteration=0, train_loss=2.514458179473877\n",
      "Epoch=225, iteration=1, train_loss=2.507443428039551\n",
      "Epoch=225, iteration=2, train_loss=2.544266939163208\n",
      "Epoch=225, iteration=3, train_loss=2.704883098602295\n",
      "Epoch=225, val_loss=2.6990232467651367\n",
      "Epoch=226, iteration=0, train_loss=2.5138983726501465\n",
      "Epoch=226, iteration=1, train_loss=2.50692081451416\n",
      "Epoch=226, iteration=2, train_loss=2.5437378883361816\n",
      "Epoch=226, iteration=3, train_loss=2.7043161392211914\n",
      "Epoch=226, val_loss=2.698456048965454\n",
      "Epoch=227, iteration=0, train_loss=2.5133402347564697\n",
      "Epoch=227, iteration=1, train_loss=2.506399631500244\n",
      "Epoch=227, iteration=2, train_loss=2.543210983276367\n",
      "Epoch=227, iteration=3, train_loss=2.703752279281616\n",
      "Epoch=227, val_loss=2.6978919506073\n",
      "Epoch=228, iteration=0, train_loss=2.512784004211426\n",
      "Epoch=228, iteration=1, train_loss=2.5058798789978027\n",
      "Epoch=228, iteration=2, train_loss=2.542686939239502\n",
      "Epoch=228, iteration=3, train_loss=2.7031924724578857\n",
      "Epoch=228, val_loss=2.6973307132720947\n",
      "Epoch=229, iteration=0, train_loss=2.5122294425964355\n",
      "Epoch=229, iteration=1, train_loss=2.505362033843994\n",
      "Epoch=229, iteration=2, train_loss=2.5421650409698486\n",
      "Epoch=229, iteration=3, train_loss=2.702636241912842\n",
      "Epoch=229, val_loss=2.696772575378418\n",
      "Epoch=230, iteration=0, train_loss=2.511676549911499\n",
      "Epoch=230, iteration=1, train_loss=2.504845380783081\n",
      "Epoch=230, iteration=2, train_loss=2.5416464805603027\n",
      "Epoch=230, iteration=3, train_loss=2.702083110809326\n",
      "Epoch=230, val_loss=2.6962177753448486\n",
      "Epoch=231, iteration=0, train_loss=2.5111255645751953\n",
      "Epoch=231, iteration=1, train_loss=2.5043299198150635\n",
      "Epoch=231, iteration=2, train_loss=2.541130542755127\n",
      "Epoch=231, iteration=3, train_loss=2.7015342712402344\n",
      "Epoch=231, val_loss=2.695665121078491\n",
      "Epoch=232, iteration=0, train_loss=2.510576009750366\n",
      "Epoch=232, iteration=1, train_loss=2.503816604614258\n",
      "Epoch=232, iteration=2, train_loss=2.5406172275543213\n",
      "Epoch=232, iteration=3, train_loss=2.70098876953125\n",
      "Epoch=232, val_loss=2.6951167583465576\n",
      "Epoch=233, iteration=0, train_loss=2.5100278854370117\n",
      "Epoch=233, iteration=1, train_loss=2.5033044815063477\n",
      "Epoch=233, iteration=2, train_loss=2.5401062965393066\n",
      "Epoch=233, iteration=3, train_loss=2.7004470825195312\n",
      "Epoch=233, val_loss=2.694571018218994\n",
      "Epoch=234, iteration=0, train_loss=2.50948166847229\n",
      "Epoch=234, iteration=1, train_loss=2.502794027328491\n",
      "Epoch=234, iteration=2, train_loss=2.539598226547241\n",
      "Epoch=234, iteration=3, train_loss=2.69990873336792\n",
      "Epoch=234, val_loss=2.694028615951538\n",
      "Epoch=235, iteration=0, train_loss=2.508936882019043\n",
      "Epoch=235, iteration=1, train_loss=2.5022850036621094\n",
      "Epoch=235, iteration=2, train_loss=2.539092540740967\n",
      "Epoch=235, iteration=3, train_loss=2.699374198913574\n",
      "Epoch=235, val_loss=2.693488836288452\n",
      "Epoch=236, iteration=0, train_loss=2.5083940029144287\n",
      "Epoch=236, iteration=1, train_loss=2.5017778873443604\n",
      "Epoch=236, iteration=2, train_loss=2.5385894775390625\n",
      "Epoch=236, iteration=3, train_loss=2.698843240737915\n",
      "Epoch=236, val_loss=2.6929526329040527\n",
      "Epoch=237, iteration=0, train_loss=2.5078532695770264\n",
      "Epoch=237, iteration=1, train_loss=2.501272201538086\n",
      "Epoch=237, iteration=2, train_loss=2.5380895137786865\n",
      "Epoch=237, iteration=3, train_loss=2.6983160972595215\n",
      "Epoch=237, val_loss=2.6924192905426025\n",
      "Epoch=238, iteration=0, train_loss=2.5073134899139404\n",
      "Epoch=238, iteration=1, train_loss=2.5007681846618652\n",
      "Epoch=238, iteration=2, train_loss=2.5375914573669434\n",
      "Epoch=238, iteration=3, train_loss=2.6977925300598145\n",
      "Epoch=238, val_loss=2.6918892860412598\n",
      "Epoch=239, iteration=0, train_loss=2.5067758560180664\n",
      "Epoch=239, iteration=1, train_loss=2.5002658367156982\n",
      "Epoch=239, iteration=2, train_loss=2.5370965003967285\n",
      "Epoch=239, iteration=3, train_loss=2.697272539138794\n",
      "Epoch=239, val_loss=2.6913628578186035\n",
      "Epoch=240, iteration=0, train_loss=2.506239891052246\n",
      "Epoch=240, iteration=1, train_loss=2.499765157699585\n",
      "Epoch=240, iteration=2, train_loss=2.536604404449463\n",
      "Epoch=240, iteration=3, train_loss=2.6967568397521973\n",
      "Epoch=240, val_loss=2.6908390522003174\n",
      "Epoch=241, iteration=0, train_loss=2.5057058334350586\n",
      "Epoch=241, iteration=1, train_loss=2.4992668628692627\n",
      "Epoch=241, iteration=2, train_loss=2.5361149311065674\n",
      "Epoch=241, iteration=3, train_loss=2.6962435245513916\n",
      "Epoch=241, val_loss=2.6903188228607178\n",
      "Epoch=242, iteration=0, train_loss=2.505173444747925\n",
      "Epoch=242, iteration=1, train_loss=2.498770236968994\n",
      "Epoch=242, iteration=2, train_loss=2.535627603530884\n",
      "Epoch=242, iteration=3, train_loss=2.695734977722168\n",
      "Epoch=242, val_loss=2.6898012161254883\n",
      "Epoch=243, iteration=0, train_loss=2.504643201828003\n",
      "Epoch=243, iteration=1, train_loss=2.4982757568359375\n",
      "Epoch=243, iteration=2, train_loss=2.5351433753967285\n",
      "Epoch=243, iteration=3, train_loss=2.6952295303344727\n",
      "Epoch=243, val_loss=2.6892874240875244\n",
      "Epoch=244, iteration=0, train_loss=2.5041146278381348\n",
      "Epoch=244, iteration=1, train_loss=2.4977829456329346\n",
      "Epoch=244, iteration=2, train_loss=2.5346617698669434\n",
      "Epoch=244, iteration=3, train_loss=2.694728136062622\n",
      "Epoch=244, val_loss=2.688776731491089\n",
      "Epoch=245, iteration=0, train_loss=2.5035884380340576\n",
      "Epoch=245, iteration=1, train_loss=2.4972918033599854\n",
      "Epoch=245, iteration=2, train_loss=2.5341830253601074\n",
      "Epoch=245, iteration=3, train_loss=2.694230318069458\n",
      "Epoch=245, val_loss=2.6882691383361816\n",
      "Epoch=246, iteration=0, train_loss=2.503063440322876\n",
      "Epoch=246, iteration=1, train_loss=2.4968032836914062\n",
      "Epoch=246, iteration=2, train_loss=2.5337071418762207\n",
      "Epoch=246, iteration=3, train_loss=2.6937355995178223\n",
      "Epoch=246, val_loss=2.6877646446228027\n",
      "Epoch=247, iteration=0, train_loss=2.5025413036346436\n",
      "Epoch=247, iteration=1, train_loss=2.496317148208618\n",
      "Epoch=247, iteration=2, train_loss=2.5332343578338623\n",
      "Epoch=247, iteration=3, train_loss=2.6932449340820312\n",
      "Epoch=247, val_loss=2.6872634887695312\n",
      "Epoch=248, iteration=0, train_loss=2.502021312713623\n",
      "Epoch=248, iteration=1, train_loss=2.495833158493042\n",
      "Epoch=248, iteration=2, train_loss=2.532763719558716\n",
      "Epoch=248, iteration=3, train_loss=2.6927578449249268\n",
      "Epoch=248, val_loss=2.686765670776367\n",
      "Epoch=249, iteration=0, train_loss=2.5015032291412354\n",
      "Epoch=249, iteration=1, train_loss=2.4953510761260986\n",
      "Epoch=249, iteration=2, train_loss=2.5322961807250977\n",
      "Epoch=249, iteration=3, train_loss=2.6922740936279297\n",
      "Epoch=249, val_loss=2.6862711906433105\n",
      "Epoch=250, iteration=0, train_loss=2.5009875297546387\n",
      "Epoch=250, iteration=1, train_loss=2.4948716163635254\n",
      "Epoch=250, iteration=2, train_loss=2.531831741333008\n",
      "Epoch=250, iteration=3, train_loss=2.6917943954467773\n",
      "Epoch=250, val_loss=2.6857798099517822\n",
      "Epoch=251, iteration=0, train_loss=2.500474214553833\n",
      "Epoch=251, iteration=1, train_loss=2.494394540786743\n",
      "Epoch=251, iteration=2, train_loss=2.531369924545288\n",
      "Epoch=251, iteration=3, train_loss=2.6913182735443115\n",
      "Epoch=251, val_loss=2.685291290283203\n",
      "Epoch=252, iteration=0, train_loss=2.4999632835388184\n",
      "Epoch=252, iteration=1, train_loss=2.493919849395752\n",
      "Epoch=252, iteration=2, train_loss=2.530911445617676\n",
      "Epoch=252, iteration=3, train_loss=2.690845012664795\n",
      "Epoch=252, val_loss=2.6848065853118896\n",
      "Epoch=253, iteration=0, train_loss=2.4994547367095947\n",
      "Epoch=253, iteration=1, train_loss=2.493448257446289\n",
      "Epoch=253, iteration=2, train_loss=2.5304555892944336\n",
      "Epoch=253, iteration=3, train_loss=2.690375566482544\n",
      "Epoch=253, val_loss=2.6843247413635254\n",
      "Epoch=254, iteration=0, train_loss=2.498948335647583\n",
      "Epoch=254, iteration=1, train_loss=2.492978572845459\n",
      "Epoch=254, iteration=2, train_loss=2.5300025939941406\n",
      "Epoch=254, iteration=3, train_loss=2.6899094581604004\n",
      "Epoch=254, val_loss=2.6838462352752686\n",
      "Epoch=255, iteration=0, train_loss=2.4984450340270996\n",
      "Epoch=255, iteration=1, train_loss=2.4925122261047363\n",
      "Epoch=255, iteration=2, train_loss=2.529552698135376\n",
      "Epoch=255, iteration=3, train_loss=2.6894471645355225\n",
      "Epoch=255, val_loss=2.68337082862854\n",
      "Epoch=256, iteration=0, train_loss=2.4979441165924072\n",
      "Epoch=256, iteration=1, train_loss=2.4920482635498047\n",
      "Epoch=256, iteration=2, train_loss=2.5291059017181396\n",
      "Epoch=256, iteration=3, train_loss=2.688987970352173\n",
      "Epoch=256, val_loss=2.682898759841919\n",
      "Epoch=257, iteration=0, train_loss=2.497445583343506\n",
      "Epoch=257, iteration=1, train_loss=2.491586923599243\n",
      "Epoch=257, iteration=2, train_loss=2.5286619663238525\n",
      "Epoch=257, iteration=3, train_loss=2.6885323524475098\n",
      "Epoch=257, val_loss=2.6824300289154053\n",
      "Epoch=258, iteration=0, train_loss=2.496950149536133\n",
      "Epoch=258, iteration=1, train_loss=2.491128921508789\n",
      "Epoch=258, iteration=2, train_loss=2.5282211303710938\n",
      "Epoch=258, iteration=3, train_loss=2.688080072402954\n",
      "Epoch=258, val_loss=2.681964159011841\n",
      "Epoch=259, iteration=0, train_loss=2.496457099914551\n",
      "Epoch=259, iteration=1, train_loss=2.490673065185547\n",
      "Epoch=259, iteration=2, train_loss=2.5277836322784424\n",
      "Epoch=259, iteration=3, train_loss=2.6876308917999268\n",
      "Epoch=259, val_loss=2.6815011501312256\n",
      "Epoch=260, iteration=0, train_loss=2.495966672897339\n",
      "Epoch=260, iteration=1, train_loss=2.490220785140991\n",
      "Epoch=260, iteration=2, train_loss=2.5273489952087402\n",
      "Epoch=260, iteration=3, train_loss=2.687185525894165\n",
      "Epoch=260, val_loss=2.681041717529297\n",
      "Epoch=261, iteration=0, train_loss=2.495479106903076\n",
      "Epoch=261, iteration=1, train_loss=2.4897706508636475\n",
      "Epoch=261, iteration=2, train_loss=2.5269172191619873\n",
      "Epoch=261, iteration=3, train_loss=2.6867430210113525\n",
      "Epoch=261, val_loss=2.6805849075317383\n",
      "Epoch=262, iteration=0, train_loss=2.494994878768921\n",
      "Epoch=262, iteration=1, train_loss=2.4893245697021484\n",
      "Epoch=262, iteration=2, train_loss=2.526489019393921\n",
      "Epoch=262, iteration=3, train_loss=2.6863036155700684\n",
      "Epoch=262, val_loss=2.680131196975708\n",
      "Epoch=263, iteration=0, train_loss=2.4945125579833984\n",
      "Epoch=263, iteration=1, train_loss=2.4888811111450195\n",
      "Epoch=263, iteration=2, train_loss=2.5260632038116455\n",
      "Epoch=263, iteration=3, train_loss=2.6858677864074707\n",
      "Epoch=263, val_loss=2.6796810626983643\n",
      "Epoch=264, iteration=0, train_loss=2.4940342903137207\n",
      "Epoch=264, iteration=1, train_loss=2.488440990447998\n",
      "Epoch=264, iteration=2, train_loss=2.5256407260894775\n",
      "Epoch=264, iteration=3, train_loss=2.6854352951049805\n",
      "Epoch=264, val_loss=2.6792335510253906\n",
      "Epoch=265, iteration=0, train_loss=2.493558883666992\n",
      "Epoch=265, iteration=1, train_loss=2.4880032539367676\n",
      "Epoch=265, iteration=2, train_loss=2.525221347808838\n",
      "Epoch=265, iteration=3, train_loss=2.6850051879882812\n",
      "Epoch=265, val_loss=2.6787893772125244\n",
      "Epoch=266, iteration=0, train_loss=2.4930856227874756\n",
      "Epoch=266, iteration=1, train_loss=2.4875693321228027\n",
      "Epoch=266, iteration=2, train_loss=2.5248050689697266\n",
      "Epoch=266, iteration=3, train_loss=2.6845779418945312\n",
      "Epoch=266, val_loss=2.678347587585449\n",
      "Epoch=267, iteration=0, train_loss=2.4926161766052246\n",
      "Epoch=267, iteration=1, train_loss=2.487138032913208\n",
      "Epoch=267, iteration=2, train_loss=2.5243918895721436\n",
      "Epoch=267, iteration=3, train_loss=2.6841542720794678\n",
      "Epoch=267, val_loss=2.6779091358184814\n",
      "Epoch=268, iteration=0, train_loss=2.4921491146087646\n",
      "Epoch=268, iteration=1, train_loss=2.4867100715637207\n",
      "Epoch=268, iteration=2, train_loss=2.5239815711975098\n",
      "Epoch=268, iteration=3, train_loss=2.6837334632873535\n",
      "Epoch=268, val_loss=2.677473545074463\n",
      "Epoch=269, iteration=0, train_loss=2.491685390472412\n",
      "Epoch=269, iteration=1, train_loss=2.4862852096557617\n",
      "Epoch=269, iteration=2, train_loss=2.5235745906829834\n",
      "Epoch=269, iteration=3, train_loss=2.6833152770996094\n",
      "Epoch=269, val_loss=2.6770405769348145\n",
      "Epoch=270, iteration=0, train_loss=2.491224527359009\n",
      "Epoch=270, iteration=1, train_loss=2.48586368560791\n",
      "Epoch=270, iteration=2, train_loss=2.523170232772827\n",
      "Epoch=270, iteration=3, train_loss=2.6829001903533936\n",
      "Epoch=270, val_loss=2.6766107082366943\n",
      "Epoch=271, iteration=0, train_loss=2.490767002105713\n",
      "Epoch=271, iteration=1, train_loss=2.485445499420166\n",
      "Epoch=271, iteration=2, train_loss=2.522768974304199\n",
      "Epoch=271, iteration=3, train_loss=2.682487726211548\n",
      "Epoch=271, val_loss=2.6761832237243652\n",
      "Epoch=272, iteration=0, train_loss=2.490312337875366\n",
      "Epoch=272, iteration=1, train_loss=2.485030174255371\n",
      "Epoch=272, iteration=2, train_loss=2.5223705768585205\n",
      "Epoch=272, iteration=3, train_loss=2.6820781230926514\n",
      "Epoch=272, val_loss=2.6757586002349854\n",
      "Epoch=273, iteration=0, train_loss=2.4898605346679688\n",
      "Epoch=273, iteration=1, train_loss=2.4846181869506836\n",
      "Epoch=273, iteration=2, train_loss=2.5219757556915283\n",
      "Epoch=273, iteration=3, train_loss=2.681671142578125\n",
      "Epoch=273, val_loss=2.6753368377685547\n",
      "Epoch=274, iteration=0, train_loss=2.489412307739258\n",
      "Epoch=274, iteration=1, train_loss=2.4842095375061035\n",
      "Epoch=274, iteration=2, train_loss=2.5215837955474854\n",
      "Epoch=274, iteration=3, train_loss=2.6812667846679688\n",
      "Epoch=274, val_loss=2.6749179363250732\n",
      "Epoch=275, iteration=0, train_loss=2.488966941833496\n",
      "Epoch=275, iteration=1, train_loss=2.4838037490844727\n",
      "Epoch=275, iteration=2, train_loss=2.5211946964263916\n",
      "Epoch=275, iteration=3, train_loss=2.6808650493621826\n",
      "Epoch=275, val_loss=2.674501657485962\n",
      "Epoch=276, iteration=0, train_loss=2.4885244369506836\n",
      "Epoch=276, iteration=1, train_loss=2.4834015369415283\n",
      "Epoch=276, iteration=2, train_loss=2.520808219909668\n",
      "Epoch=276, iteration=3, train_loss=2.6804659366607666\n",
      "Epoch=276, val_loss=2.6740877628326416\n",
      "Epoch=277, iteration=0, train_loss=2.4880852699279785\n",
      "Epoch=277, iteration=1, train_loss=2.483001947402954\n",
      "Epoch=277, iteration=2, train_loss=2.5204248428344727\n",
      "Epoch=277, iteration=3, train_loss=2.6800694465637207\n",
      "Epoch=277, val_loss=2.6736764907836914\n",
      "Epoch=278, iteration=0, train_loss=2.4876489639282227\n",
      "Epoch=278, iteration=1, train_loss=2.4826059341430664\n",
      "Epoch=278, iteration=2, train_loss=2.5200440883636475\n",
      "Epoch=278, iteration=3, train_loss=2.679675340652466\n",
      "Epoch=278, val_loss=2.6732676029205322\n",
      "Epoch=279, iteration=0, train_loss=2.487215757369995\n",
      "Epoch=279, iteration=1, train_loss=2.482213020324707\n",
      "Epoch=279, iteration=2, train_loss=2.5196661949157715\n",
      "Epoch=279, iteration=3, train_loss=2.679283618927002\n",
      "Epoch=279, val_loss=2.672861337661743\n",
      "Epoch=280, iteration=0, train_loss=2.486785411834717\n",
      "Epoch=280, iteration=1, train_loss=2.481823444366455\n",
      "Epoch=280, iteration=2, train_loss=2.519291400909424\n",
      "Epoch=280, iteration=3, train_loss=2.678894281387329\n",
      "Epoch=280, val_loss=2.672457695007324\n",
      "Epoch=281, iteration=0, train_loss=2.4863579273223877\n",
      "Epoch=281, iteration=1, train_loss=2.4814364910125732\n",
      "Epoch=281, iteration=2, train_loss=2.518918991088867\n",
      "Epoch=281, iteration=3, train_loss=2.6785073280334473\n",
      "Epoch=281, val_loss=2.6720564365386963\n",
      "Epoch=282, iteration=0, train_loss=2.485933780670166\n",
      "Epoch=282, iteration=1, train_loss=2.481052875518799\n",
      "Epoch=282, iteration=2, train_loss=2.518549680709839\n",
      "Epoch=282, iteration=3, train_loss=2.6781225204467773\n",
      "Epoch=282, val_loss=2.6716575622558594\n",
      "Epoch=283, iteration=0, train_loss=2.4855124950408936\n",
      "Epoch=283, iteration=1, train_loss=2.480672597885132\n",
      "Epoch=283, iteration=2, train_loss=2.5181832313537598\n",
      "Epoch=283, iteration=3, train_loss=2.6777403354644775\n",
      "Epoch=283, val_loss=2.6712610721588135\n",
      "Epoch=284, iteration=0, train_loss=2.4850945472717285\n",
      "Epoch=284, iteration=1, train_loss=2.480294942855835\n",
      "Epoch=284, iteration=2, train_loss=2.5178191661834717\n",
      "Epoch=284, iteration=3, train_loss=2.6773598194122314\n",
      "Epoch=284, val_loss=2.6708667278289795\n",
      "Epoch=285, iteration=0, train_loss=2.4846787452697754\n",
      "Epoch=285, iteration=1, train_loss=2.4799201488494873\n",
      "Epoch=285, iteration=2, train_loss=2.517457962036133\n",
      "Epoch=285, iteration=3, train_loss=2.6769816875457764\n",
      "Epoch=285, val_loss=2.6704747676849365\n",
      "Epoch=286, iteration=0, train_loss=2.4842660427093506\n",
      "Epoch=286, iteration=1, train_loss=2.479548454284668\n",
      "Epoch=286, iteration=2, train_loss=2.517099380493164\n",
      "Epoch=286, iteration=3, train_loss=2.6766059398651123\n",
      "Epoch=286, val_loss=2.6700854301452637\n",
      "Epoch=287, iteration=0, train_loss=2.483856439590454\n",
      "Epoch=287, iteration=1, train_loss=2.479179620742798\n",
      "Epoch=287, iteration=2, train_loss=2.5167434215545654\n",
      "Epoch=287, iteration=3, train_loss=2.676232099533081\n",
      "Epoch=287, val_loss=2.6696979999542236\n",
      "Epoch=288, iteration=0, train_loss=2.483449697494507\n",
      "Epoch=288, iteration=1, train_loss=2.478813886642456\n",
      "Epoch=288, iteration=2, train_loss=2.516389846801758\n",
      "Epoch=288, iteration=3, train_loss=2.6758601665496826\n",
      "Epoch=288, val_loss=2.6693129539489746\n",
      "Epoch=289, iteration=0, train_loss=2.4830455780029297\n",
      "Epoch=289, iteration=1, train_loss=2.4784510135650635\n",
      "Epoch=289, iteration=2, train_loss=2.5160388946533203\n",
      "Epoch=289, iteration=3, train_loss=2.675490617752075\n",
      "Epoch=289, val_loss=2.6689298152923584\n",
      "Epoch=290, iteration=0, train_loss=2.4826440811157227\n",
      "Epoch=290, iteration=1, train_loss=2.478090763092041\n",
      "Epoch=290, iteration=2, train_loss=2.515690565109253\n",
      "Epoch=290, iteration=3, train_loss=2.6751229763031006\n",
      "Epoch=290, val_loss=2.6685492992401123\n",
      "Epoch=291, iteration=0, train_loss=2.482245445251465\n",
      "Epoch=291, iteration=1, train_loss=2.477733850479126\n",
      "Epoch=291, iteration=2, train_loss=2.5153446197509766\n",
      "Epoch=291, iteration=3, train_loss=2.6747570037841797\n",
      "Epoch=291, val_loss=2.668170690536499\n",
      "Epoch=292, iteration=0, train_loss=2.481849431991577\n",
      "Epoch=292, iteration=1, train_loss=2.477379322052002\n",
      "Epoch=292, iteration=2, train_loss=2.5150015354156494\n",
      "Epoch=292, iteration=3, train_loss=2.6743931770324707\n",
      "Epoch=292, val_loss=2.6677939891815186\n",
      "Epoch=293, iteration=0, train_loss=2.4814565181732178\n",
      "Epoch=293, iteration=1, train_loss=2.477027416229248\n",
      "Epoch=293, iteration=2, train_loss=2.514660120010376\n",
      "Epoch=293, iteration=3, train_loss=2.6740312576293945\n",
      "Epoch=293, val_loss=2.667419672012329\n",
      "Epoch=294, iteration=0, train_loss=2.4810657501220703\n",
      "Epoch=294, iteration=1, train_loss=2.4766783714294434\n",
      "Epoch=294, iteration=2, train_loss=2.5143215656280518\n",
      "Epoch=294, iteration=3, train_loss=2.673671245574951\n",
      "Epoch=294, val_loss=2.6670472621917725\n",
      "Epoch=295, iteration=0, train_loss=2.480678081512451\n",
      "Epoch=295, iteration=1, train_loss=2.476331949234009\n",
      "Epoch=295, iteration=2, train_loss=2.5139851570129395\n",
      "Epoch=295, iteration=3, train_loss=2.6733129024505615\n",
      "Epoch=295, val_loss=2.6666769981384277\n",
      "Epoch=296, iteration=0, train_loss=2.480292320251465\n",
      "Epoch=296, iteration=1, train_loss=2.4759881496429443\n",
      "Epoch=296, iteration=2, train_loss=2.513650894165039\n",
      "Epoch=296, iteration=3, train_loss=2.6729564666748047\n",
      "Epoch=296, val_loss=2.666308641433716\n",
      "Epoch=297, iteration=0, train_loss=2.479909896850586\n",
      "Epoch=297, iteration=1, train_loss=2.47564697265625\n",
      "Epoch=297, iteration=2, train_loss=2.513319253921509\n",
      "Epoch=297, iteration=3, train_loss=2.672602415084839\n",
      "Epoch=297, val_loss=2.665942430496216\n",
      "Epoch=298, iteration=0, train_loss=2.47952938079834\n",
      "Epoch=298, iteration=1, train_loss=2.4753081798553467\n",
      "Epoch=298, iteration=2, train_loss=2.5129897594451904\n",
      "Epoch=298, iteration=3, train_loss=2.6722493171691895\n",
      "Epoch=298, val_loss=2.6655781269073486\n",
      "Epoch=299, iteration=0, train_loss=2.4791512489318848\n",
      "Epoch=299, iteration=1, train_loss=2.4749717712402344\n",
      "Epoch=299, iteration=2, train_loss=2.512661933898926\n",
      "Epoch=299, iteration=3, train_loss=2.671898126602173\n",
      "Epoch=299, val_loss=2.6652159690856934\n",
      "Epoch=300, iteration=0, train_loss=2.4787757396698\n",
      "Epoch=300, iteration=1, train_loss=2.4746384620666504\n",
      "Epoch=300, iteration=2, train_loss=2.5123369693756104\n",
      "Epoch=300, iteration=3, train_loss=2.671548843383789\n",
      "Epoch=300, val_loss=2.664855480194092\n",
      "Epoch=301, iteration=0, train_loss=2.478402853012085\n",
      "Epoch=301, iteration=1, train_loss=2.47430682182312\n",
      "Epoch=301, iteration=2, train_loss=2.5120139122009277\n",
      "Epoch=301, iteration=3, train_loss=2.671201467514038\n",
      "Epoch=301, val_loss=2.664497137069702\n",
      "Epoch=302, iteration=0, train_loss=2.478031873703003\n",
      "Epoch=302, iteration=1, train_loss=2.473978042602539\n",
      "Epoch=302, iteration=2, train_loss=2.511693000793457\n",
      "Epoch=302, iteration=3, train_loss=2.6708555221557617\n",
      "Epoch=302, val_loss=2.664140462875366\n",
      "Epoch=303, iteration=0, train_loss=2.477663278579712\n",
      "Epoch=303, iteration=1, train_loss=2.473651647567749\n",
      "Epoch=303, iteration=2, train_loss=2.511373996734619\n",
      "Epoch=303, iteration=3, train_loss=2.670511484146118\n",
      "Epoch=303, val_loss=2.6637864112854004\n",
      "Epoch=304, iteration=0, train_loss=2.477297067642212\n",
      "Epoch=304, iteration=1, train_loss=2.473327159881592\n",
      "Epoch=304, iteration=2, train_loss=2.511057138442993\n",
      "Epoch=304, iteration=3, train_loss=2.670168399810791\n",
      "Epoch=304, val_loss=2.663433313369751\n",
      "Epoch=305, iteration=0, train_loss=2.476933240890503\n",
      "Epoch=305, iteration=1, train_loss=2.473005533218384\n",
      "Epoch=305, iteration=2, train_loss=2.510742425918579\n",
      "Epoch=305, iteration=3, train_loss=2.669827461242676\n",
      "Epoch=305, val_loss=2.6630825996398926\n",
      "Epoch=306, iteration=0, train_loss=2.4765713214874268\n",
      "Epoch=306, iteration=1, train_loss=2.4726853370666504\n",
      "Epoch=306, iteration=2, train_loss=2.5104291439056396\n",
      "Epoch=306, iteration=3, train_loss=2.6694881916046143\n",
      "Epoch=306, val_loss=2.662733554840088\n",
      "Epoch=307, iteration=0, train_loss=2.4762117862701416\n",
      "Epoch=307, iteration=1, train_loss=2.472368001937866\n",
      "Epoch=307, iteration=2, train_loss=2.5101184844970703\n",
      "Epoch=307, iteration=3, train_loss=2.6691503524780273\n",
      "Epoch=307, val_loss=2.662386178970337\n",
      "Epoch=308, iteration=0, train_loss=2.4758541584014893\n",
      "Epoch=308, iteration=1, train_loss=2.4720523357391357\n",
      "Epoch=308, iteration=2, train_loss=2.5098094940185547\n",
      "Epoch=308, iteration=3, train_loss=2.6688144207000732\n",
      "Epoch=308, val_loss=2.6620407104492188\n",
      "Epoch=309, iteration=0, train_loss=2.4754984378814697\n",
      "Epoch=309, iteration=1, train_loss=2.4717395305633545\n",
      "Epoch=309, iteration=2, train_loss=2.509502410888672\n",
      "Epoch=309, iteration=3, train_loss=2.6684796810150146\n",
      "Epoch=309, val_loss=2.6616976261138916\n",
      "Epoch=310, iteration=0, train_loss=2.4751453399658203\n",
      "Epoch=310, iteration=1, train_loss=2.471428155899048\n",
      "Epoch=310, iteration=2, train_loss=2.509197235107422\n",
      "Epoch=310, iteration=3, train_loss=2.668146848678589\n",
      "Epoch=310, val_loss=2.66135573387146\n",
      "Epoch=311, iteration=0, train_loss=2.4747939109802246\n",
      "Epoch=311, iteration=1, train_loss=2.4711194038391113\n",
      "Epoch=311, iteration=2, train_loss=2.5088937282562256\n",
      "Epoch=311, iteration=3, train_loss=2.6678152084350586\n",
      "Epoch=311, val_loss=2.661015510559082\n",
      "Epoch=312, iteration=0, train_loss=2.4744441509246826\n",
      "Epoch=312, iteration=1, train_loss=2.4708120822906494\n",
      "Epoch=312, iteration=2, train_loss=2.508592128753662\n",
      "Epoch=312, iteration=3, train_loss=2.667485237121582\n",
      "Epoch=312, val_loss=2.660677194595337\n",
      "Epoch=313, iteration=0, train_loss=2.4740967750549316\n",
      "Epoch=313, iteration=1, train_loss=2.4705071449279785\n",
      "Epoch=313, iteration=2, train_loss=2.5082921981811523\n",
      "Epoch=313, iteration=3, train_loss=2.6671571731567383\n",
      "Epoch=313, val_loss=2.6603410243988037\n",
      "Epoch=314, iteration=0, train_loss=2.4737515449523926\n",
      "Epoch=314, iteration=1, train_loss=2.4702038764953613\n",
      "Epoch=314, iteration=2, train_loss=2.5079944133758545\n",
      "Epoch=314, iteration=3, train_loss=2.666830062866211\n",
      "Epoch=314, val_loss=2.660006284713745\n",
      "Epoch=315, iteration=0, train_loss=2.473407745361328\n",
      "Epoch=315, iteration=1, train_loss=2.469902753829956\n",
      "Epoch=315, iteration=2, train_loss=2.507697582244873\n",
      "Epoch=315, iteration=3, train_loss=2.6665050983428955\n",
      "Epoch=315, val_loss=2.6596732139587402\n",
      "Epoch=316, iteration=0, train_loss=2.4730656147003174\n",
      "Epoch=316, iteration=1, train_loss=2.4696035385131836\n",
      "Epoch=316, iteration=2, train_loss=2.5074028968811035\n",
      "Epoch=316, iteration=3, train_loss=2.6661808490753174\n",
      "Epoch=316, val_loss=2.659342050552368\n",
      "Epoch=317, iteration=0, train_loss=2.4727253913879395\n",
      "Epoch=317, iteration=1, train_loss=2.469306230545044\n",
      "Epoch=317, iteration=2, train_loss=2.5071098804473877\n",
      "Epoch=317, iteration=3, train_loss=2.665858268737793\n",
      "Epoch=317, val_loss=2.6590123176574707\n",
      "Epoch=318, iteration=0, train_loss=2.4723870754241943\n",
      "Epoch=318, iteration=1, train_loss=2.4690101146698\n",
      "Epoch=318, iteration=2, train_loss=2.5068185329437256\n",
      "Epoch=318, iteration=3, train_loss=2.6655375957489014\n",
      "Epoch=318, val_loss=2.658684492111206\n",
      "Epoch=319, iteration=0, train_loss=2.472050666809082\n",
      "Epoch=319, iteration=1, train_loss=2.4687161445617676\n",
      "Epoch=319, iteration=2, train_loss=2.506528615951538\n",
      "Epoch=319, iteration=3, train_loss=2.6652181148529053\n",
      "Epoch=319, val_loss=2.658358335494995\n",
      "Epoch=320, iteration=0, train_loss=2.4717159271240234\n",
      "Epoch=320, iteration=1, train_loss=2.4684245586395264\n",
      "Epoch=320, iteration=2, train_loss=2.5062403678894043\n",
      "Epoch=320, iteration=3, train_loss=2.6648998260498047\n",
      "Epoch=320, val_loss=2.658033847808838\n",
      "Epoch=321, iteration=0, train_loss=2.4713823795318604\n",
      "Epoch=321, iteration=1, train_loss=2.4681339263916016\n",
      "Epoch=321, iteration=2, train_loss=2.505953550338745\n",
      "Epoch=321, iteration=3, train_loss=2.664583444595337\n",
      "Epoch=321, val_loss=2.6577112674713135\n",
      "Epoch=322, iteration=0, train_loss=2.47105073928833\n",
      "Epoch=322, iteration=1, train_loss=2.4678449630737305\n",
      "Epoch=322, iteration=2, train_loss=2.5056684017181396\n",
      "Epoch=322, iteration=3, train_loss=2.6642680168151855\n",
      "Epoch=322, val_loss=2.6573898792266846\n",
      "Epoch=323, iteration=0, train_loss=2.4707212448120117\n",
      "Epoch=323, iteration=1, train_loss=2.4675581455230713\n",
      "Epoch=323, iteration=2, train_loss=2.505384683609009\n",
      "Epoch=323, iteration=3, train_loss=2.663954019546509\n",
      "Epoch=323, val_loss=2.6570701599121094\n",
      "Epoch=324, iteration=0, train_loss=2.4703927040100098\n",
      "Epoch=324, iteration=1, train_loss=2.4672725200653076\n",
      "Epoch=324, iteration=2, train_loss=2.5051023960113525\n",
      "Epoch=324, iteration=3, train_loss=2.6636416912078857\n",
      "Epoch=324, val_loss=2.656752824783325\n",
      "Epoch=325, iteration=0, train_loss=2.4700655937194824\n",
      "Epoch=325, iteration=1, train_loss=2.4669888019561768\n",
      "Epoch=325, iteration=2, train_loss=2.504822015762329\n",
      "Epoch=325, iteration=3, train_loss=2.6633307933807373\n",
      "Epoch=325, val_loss=2.6564364433288574\n",
      "Epoch=326, iteration=0, train_loss=2.469740390777588\n",
      "Epoch=326, iteration=1, train_loss=2.4667065143585205\n",
      "Epoch=326, iteration=2, train_loss=2.504542589187622\n",
      "Epoch=326, iteration=3, train_loss=2.6630208492279053\n",
      "Epoch=326, val_loss=2.6561217308044434\n",
      "Epoch=327, iteration=0, train_loss=2.469416856765747\n",
      "Epoch=327, iteration=1, train_loss=2.466425657272339\n",
      "Epoch=327, iteration=2, train_loss=2.5042648315429688\n",
      "Epoch=327, iteration=3, train_loss=2.662712574005127\n",
      "Epoch=327, val_loss=2.655808448791504\n",
      "Epoch=328, iteration=0, train_loss=2.4690945148468018\n",
      "Epoch=328, iteration=1, train_loss=2.46614670753479\n",
      "Epoch=328, iteration=2, train_loss=2.503988265991211\n",
      "Epoch=328, iteration=3, train_loss=2.6624057292938232\n",
      "Epoch=328, val_loss=2.6554975509643555\n",
      "Epoch=329, iteration=0, train_loss=2.468773603439331\n",
      "Epoch=329, iteration=1, train_loss=2.465869188308716\n",
      "Epoch=329, iteration=2, train_loss=2.503713369369507\n",
      "Epoch=329, iteration=3, train_loss=2.662100076675415\n",
      "Epoch=329, val_loss=2.6551873683929443\n",
      "Epoch=330, iteration=0, train_loss=2.468453884124756\n",
      "Epoch=330, iteration=1, train_loss=2.465592861175537\n",
      "Epoch=330, iteration=2, train_loss=2.503439426422119\n",
      "Epoch=330, iteration=3, train_loss=2.6617958545684814\n",
      "Epoch=330, val_loss=2.654879093170166\n",
      "Epoch=331, iteration=0, train_loss=2.4681358337402344\n",
      "Epoch=331, iteration=1, train_loss=2.465318202972412\n",
      "Epoch=331, iteration=2, train_loss=2.5031673908233643\n",
      "Epoch=331, iteration=3, train_loss=2.6614928245544434\n",
      "Epoch=331, val_loss=2.6545724868774414\n",
      "Epoch=332, iteration=0, train_loss=2.4678192138671875\n",
      "Epoch=332, iteration=1, train_loss=2.4650447368621826\n",
      "Epoch=332, iteration=2, train_loss=2.5028960704803467\n",
      "Epoch=332, iteration=3, train_loss=2.661191463470459\n",
      "Epoch=332, val_loss=2.6542673110961914\n",
      "Epoch=333, iteration=0, train_loss=2.467503547668457\n",
      "Epoch=333, iteration=1, train_loss=2.4647727012634277\n",
      "Epoch=333, iteration=2, train_loss=2.5026261806488037\n",
      "Epoch=333, iteration=3, train_loss=2.660891056060791\n",
      "Epoch=333, val_loss=2.653963804244995\n",
      "Epoch=334, iteration=0, train_loss=2.4671895503997803\n",
      "Epoch=334, iteration=1, train_loss=2.4645020961761475\n",
      "Epoch=334, iteration=2, train_loss=2.5023577213287354\n",
      "Epoch=334, iteration=3, train_loss=2.6605918407440186\n",
      "Epoch=334, val_loss=2.6536619663238525\n",
      "Epoch=335, iteration=0, train_loss=2.46687650680542\n",
      "Epoch=335, iteration=1, train_loss=2.464233160018921\n",
      "Epoch=335, iteration=2, train_loss=2.5020904541015625\n",
      "Epoch=335, iteration=3, train_loss=2.6602942943573\n",
      "Epoch=335, val_loss=2.6533610820770264\n",
      "Epoch=336, iteration=0, train_loss=2.4665651321411133\n",
      "Epoch=336, iteration=1, train_loss=2.4639651775360107\n",
      "Epoch=336, iteration=2, train_loss=2.501824378967285\n",
      "Epoch=336, iteration=3, train_loss=2.6599984169006348\n",
      "Epoch=336, val_loss=2.653062343597412\n",
      "Epoch=337, iteration=0, train_loss=2.466254949569702\n",
      "Epoch=337, iteration=1, train_loss=2.463698625564575\n",
      "Epoch=337, iteration=2, train_loss=2.5015594959259033\n",
      "Epoch=337, iteration=3, train_loss=2.659703016281128\n",
      "Epoch=337, val_loss=2.6527650356292725\n",
      "Epoch=338, iteration=0, train_loss=2.4659457206726074\n",
      "Epoch=338, iteration=1, train_loss=2.4634335041046143\n",
      "Epoch=338, iteration=2, train_loss=2.501296043395996\n",
      "Epoch=338, iteration=3, train_loss=2.659409523010254\n",
      "Epoch=338, val_loss=2.652468681335449\n",
      "Epoch=339, iteration=0, train_loss=2.465637445449829\n",
      "Epoch=339, iteration=1, train_loss=2.463169574737549\n",
      "Epoch=339, iteration=2, train_loss=2.5010335445404053\n",
      "Epoch=339, iteration=3, train_loss=2.6591169834136963\n",
      "Epoch=339, val_loss=2.652174949645996\n",
      "Epoch=340, iteration=0, train_loss=2.4653310775756836\n",
      "Epoch=340, iteration=1, train_loss=2.4629065990448\n",
      "Epoch=340, iteration=2, train_loss=2.50077223777771\n",
      "Epoch=340, iteration=3, train_loss=2.658825397491455\n",
      "Epoch=340, val_loss=2.651881694793701\n",
      "Epoch=341, iteration=0, train_loss=2.4650254249572754\n",
      "Epoch=341, iteration=1, train_loss=2.4626448154449463\n",
      "Epoch=341, iteration=2, train_loss=2.50051212310791\n",
      "Epoch=341, iteration=3, train_loss=2.6585352420806885\n",
      "Epoch=341, val_loss=2.651590347290039\n",
      "Epoch=342, iteration=0, train_loss=2.4647207260131836\n",
      "Epoch=342, iteration=1, train_loss=2.4623844623565674\n",
      "Epoch=342, iteration=2, train_loss=2.5002529621124268\n",
      "Epoch=342, iteration=3, train_loss=2.6582465171813965\n",
      "Epoch=342, val_loss=2.6513004302978516\n",
      "Epoch=343, iteration=0, train_loss=2.4644174575805664\n",
      "Epoch=343, iteration=1, train_loss=2.462125301361084\n",
      "Epoch=343, iteration=2, train_loss=2.499995231628418\n",
      "Epoch=343, iteration=3, train_loss=2.657958984375\n",
      "Epoch=343, val_loss=2.6510121822357178\n",
      "Epoch=344, iteration=0, train_loss=2.4641153812408447\n",
      "Epoch=344, iteration=1, train_loss=2.461867094039917\n",
      "Epoch=344, iteration=2, train_loss=2.4997384548187256\n",
      "Epoch=344, iteration=3, train_loss=2.657672882080078\n",
      "Epoch=344, val_loss=2.6507251262664795\n",
      "Epoch=345, iteration=0, train_loss=2.4638137817382812\n",
      "Epoch=345, iteration=1, train_loss=2.4616103172302246\n",
      "Epoch=345, iteration=2, train_loss=2.4994826316833496\n",
      "Epoch=345, iteration=3, train_loss=2.6573877334594727\n",
      "Epoch=345, val_loss=2.650439500808716\n",
      "Epoch=346, iteration=0, train_loss=2.4635140895843506\n",
      "Epoch=346, iteration=1, train_loss=2.4613544940948486\n",
      "Epoch=346, iteration=2, train_loss=2.4992282390594482\n",
      "Epoch=346, iteration=3, train_loss=2.6571037769317627\n",
      "Epoch=346, val_loss=2.650155544281006\n",
      "Epoch=347, iteration=0, train_loss=2.46321439743042\n",
      "Epoch=347, iteration=1, train_loss=2.4611001014709473\n",
      "Epoch=347, iteration=2, train_loss=2.498974323272705\n",
      "Epoch=347, iteration=3, train_loss=2.6568210124969482\n",
      "Epoch=347, val_loss=2.6498725414276123\n",
      "Epoch=348, iteration=0, train_loss=2.462916612625122\n",
      "Epoch=348, iteration=1, train_loss=2.460846424102783\n",
      "Epoch=348, iteration=2, train_loss=2.4987220764160156\n",
      "Epoch=348, iteration=3, train_loss=2.6565396785736084\n",
      "Epoch=348, val_loss=2.6495912075042725\n",
      "Epoch=349, iteration=0, train_loss=2.4626193046569824\n",
      "Epoch=349, iteration=1, train_loss=2.4605941772460938\n",
      "Epoch=349, iteration=2, train_loss=2.4984705448150635\n",
      "Epoch=349, iteration=3, train_loss=2.656259298324585\n",
      "Epoch=349, val_loss=2.6493113040924072\n",
      "Epoch=350, iteration=0, train_loss=2.4623234272003174\n",
      "Epoch=350, iteration=1, train_loss=2.4603426456451416\n",
      "Epoch=350, iteration=2, train_loss=2.4982199668884277\n",
      "Epoch=350, iteration=3, train_loss=2.655980110168457\n",
      "Epoch=350, val_loss=2.6490328311920166\n",
      "Epoch=351, iteration=0, train_loss=2.4620280265808105\n",
      "Epoch=351, iteration=1, train_loss=2.460092306137085\n",
      "Epoch=351, iteration=2, train_loss=2.4979701042175293\n",
      "Epoch=351, iteration=3, train_loss=2.6557023525238037\n",
      "Epoch=351, val_loss=2.6487560272216797\n",
      "Epoch=352, iteration=0, train_loss=2.46173357963562\n",
      "Epoch=352, iteration=1, train_loss=2.4598429203033447\n",
      "Epoch=352, iteration=2, train_loss=2.4977216720581055\n",
      "Epoch=352, iteration=3, train_loss=2.6554253101348877\n",
      "Epoch=352, val_loss=2.6484804153442383\n",
      "Epoch=353, iteration=0, train_loss=2.4614405632019043\n",
      "Epoch=353, iteration=1, train_loss=2.4595940113067627\n",
      "Epoch=353, iteration=2, train_loss=2.497474431991577\n",
      "Epoch=353, iteration=3, train_loss=2.6551496982574463\n",
      "Epoch=353, val_loss=2.6482057571411133\n",
      "Epoch=354, iteration=0, train_loss=2.461148500442505\n",
      "Epoch=354, iteration=1, train_loss=2.4593467712402344\n",
      "Epoch=354, iteration=2, train_loss=2.497227668762207\n",
      "Epoch=354, iteration=3, train_loss=2.6548750400543213\n",
      "Epoch=354, val_loss=2.647933006286621\n",
      "Epoch=355, iteration=0, train_loss=2.4608564376831055\n",
      "Epoch=355, iteration=1, train_loss=2.4591004848480225\n",
      "Epoch=355, iteration=2, train_loss=2.4969818592071533\n",
      "Epoch=355, iteration=3, train_loss=2.65460205078125\n",
      "Epoch=355, val_loss=2.6476614475250244\n",
      "Epoch=356, iteration=0, train_loss=2.460566282272339\n",
      "Epoch=356, iteration=1, train_loss=2.458854913711548\n",
      "Epoch=356, iteration=2, train_loss=2.496737003326416\n",
      "Epoch=356, iteration=3, train_loss=2.654330015182495\n",
      "Epoch=356, val_loss=2.6473913192749023\n",
      "Epoch=357, iteration=0, train_loss=2.4602766036987305\n",
      "Epoch=357, iteration=1, train_loss=2.4586105346679688\n",
      "Epoch=357, iteration=2, train_loss=2.496493339538574\n",
      "Epoch=357, iteration=3, train_loss=2.6540586948394775\n",
      "Epoch=357, val_loss=2.647122383117676\n",
      "Epoch=358, iteration=0, train_loss=2.4599876403808594\n",
      "Epoch=358, iteration=1, train_loss=2.458366870880127\n",
      "Epoch=358, iteration=2, train_loss=2.496251106262207\n",
      "Epoch=358, iteration=3, train_loss=2.6537890434265137\n",
      "Epoch=358, val_loss=2.646855354309082\n",
      "Epoch=359, iteration=0, train_loss=2.459699869155884\n",
      "Epoch=359, iteration=1, train_loss=2.4581243991851807\n",
      "Epoch=359, iteration=2, train_loss=2.49600887298584\n",
      "Epoch=359, iteration=3, train_loss=2.653519630432129\n",
      "Epoch=359, val_loss=2.6465890407562256\n",
      "Epoch=360, iteration=0, train_loss=2.4594130516052246\n",
      "Epoch=360, iteration=1, train_loss=2.4578826427459717\n",
      "Epoch=360, iteration=2, train_loss=2.495767593383789\n",
      "Epoch=360, iteration=3, train_loss=2.653252363204956\n",
      "Epoch=360, val_loss=2.6463239192962646\n",
      "Epoch=361, iteration=0, train_loss=2.4591264724731445\n",
      "Epoch=361, iteration=1, train_loss=2.457641839981079\n",
      "Epoch=361, iteration=2, train_loss=2.4955272674560547\n",
      "Epoch=361, iteration=3, train_loss=2.6529855728149414\n",
      "Epoch=361, val_loss=2.646059989929199\n",
      "Epoch=362, iteration=0, train_loss=2.458840847015381\n",
      "Epoch=362, iteration=1, train_loss=2.457401990890503\n",
      "Epoch=362, iteration=2, train_loss=2.4952878952026367\n",
      "Epoch=362, iteration=3, train_loss=2.652719736099243\n",
      "Epoch=362, val_loss=2.6457982063293457\n",
      "Epoch=363, iteration=0, train_loss=2.458556652069092\n",
      "Epoch=363, iteration=1, train_loss=2.457162857055664\n",
      "Epoch=363, iteration=2, train_loss=2.495049476623535\n",
      "Epoch=363, iteration=3, train_loss=2.6524555683135986\n",
      "Epoch=363, val_loss=2.6455368995666504\n",
      "Epoch=364, iteration=0, train_loss=2.458272933959961\n",
      "Epoch=364, iteration=1, train_loss=2.4569249153137207\n",
      "Epoch=364, iteration=2, train_loss=2.494811773300171\n",
      "Epoch=364, iteration=3, train_loss=2.6521918773651123\n",
      "Epoch=364, val_loss=2.6452770233154297\n",
      "Epoch=365, iteration=0, train_loss=2.4579896926879883\n",
      "Epoch=365, iteration=1, train_loss=2.4566874504089355\n",
      "Epoch=365, iteration=2, train_loss=2.494575023651123\n",
      "Epoch=365, iteration=3, train_loss=2.6519293785095215\n",
      "Epoch=365, val_loss=2.6450188159942627\n",
      "Epoch=366, iteration=0, train_loss=2.457707643508911\n",
      "Epoch=366, iteration=1, train_loss=2.456450939178467\n",
      "Epoch=366, iteration=2, train_loss=2.4943392276763916\n",
      "Epoch=366, iteration=3, train_loss=2.6516683101654053\n",
      "Epoch=366, val_loss=2.644761800765991\n",
      "Epoch=367, iteration=0, train_loss=2.4574265480041504\n",
      "Epoch=367, iteration=1, train_loss=2.4562149047851562\n",
      "Epoch=367, iteration=2, train_loss=2.4941041469573975\n",
      "Epoch=367, iteration=3, train_loss=2.6514077186584473\n",
      "Epoch=367, val_loss=2.644505739212036\n",
      "Epoch=368, iteration=0, train_loss=2.4571454524993896\n",
      "Epoch=368, iteration=1, train_loss=2.455980062484741\n",
      "Epoch=368, iteration=2, train_loss=2.4938697814941406\n",
      "Epoch=368, iteration=3, train_loss=2.651148796081543\n",
      "Epoch=368, val_loss=2.6442511081695557\n",
      "Epoch=369, iteration=0, train_loss=2.4568655490875244\n",
      "Epoch=369, iteration=1, train_loss=2.4557461738586426\n",
      "Epoch=369, iteration=2, train_loss=2.4936366081237793\n",
      "Epoch=369, iteration=3, train_loss=2.650890350341797\n",
      "Epoch=369, val_loss=2.6439976692199707\n",
      "Epoch=370, iteration=0, train_loss=2.4565865993499756\n",
      "Epoch=370, iteration=1, train_loss=2.455512762069702\n",
      "Epoch=370, iteration=2, train_loss=2.493403911590576\n",
      "Epoch=370, iteration=3, train_loss=2.6506330966949463\n",
      "Epoch=370, val_loss=2.6437456607818604\n",
      "Epoch=371, iteration=0, train_loss=2.456307888031006\n",
      "Epoch=371, iteration=1, train_loss=2.4552807807922363\n",
      "Epoch=371, iteration=2, train_loss=2.4931719303131104\n",
      "Epoch=371, iteration=3, train_loss=2.6503775119781494\n",
      "Epoch=371, val_loss=2.6434946060180664\n",
      "Epoch=372, iteration=0, train_loss=2.4560301303863525\n",
      "Epoch=372, iteration=1, train_loss=2.4550492763519287\n",
      "Epoch=372, iteration=2, train_loss=2.492940664291382\n",
      "Epoch=372, iteration=3, train_loss=2.6501221656799316\n",
      "Epoch=372, val_loss=2.643244743347168\n",
      "Epoch=373, iteration=0, train_loss=2.4557533264160156\n",
      "Epoch=373, iteration=1, train_loss=2.4548182487487793\n",
      "Epoch=373, iteration=2, train_loss=2.4927103519439697\n",
      "Epoch=373, iteration=3, train_loss=2.6498677730560303\n",
      "Epoch=373, val_loss=2.642996311187744\n",
      "Epoch=374, iteration=0, train_loss=2.455476999282837\n",
      "Epoch=374, iteration=1, train_loss=2.4545881748199463\n",
      "Epoch=374, iteration=2, train_loss=2.492480754852295\n",
      "Epoch=374, iteration=3, train_loss=2.6496150493621826\n",
      "Epoch=374, val_loss=2.6427488327026367\n",
      "Epoch=375, iteration=0, train_loss=2.4552016258239746\n",
      "Epoch=375, iteration=1, train_loss=2.4543590545654297\n",
      "Epoch=375, iteration=2, train_loss=2.4922518730163574\n",
      "Epoch=375, iteration=3, train_loss=2.649362802505493\n",
      "Epoch=375, val_loss=2.642503023147583\n",
      "Epoch=376, iteration=0, train_loss=2.4549264907836914\n",
      "Epoch=376, iteration=1, train_loss=2.4541306495666504\n",
      "Epoch=376, iteration=2, train_loss=2.4920241832733154\n",
      "Epoch=376, iteration=3, train_loss=2.64911150932312\n",
      "Epoch=376, val_loss=2.6422579288482666\n",
      "Epoch=377, iteration=0, train_loss=2.4546525478363037\n",
      "Epoch=377, iteration=1, train_loss=2.45390248298645\n",
      "Epoch=377, iteration=2, train_loss=2.4917964935302734\n",
      "Epoch=377, iteration=3, train_loss=2.6488616466522217\n",
      "Epoch=377, val_loss=2.6420140266418457\n",
      "Epoch=378, iteration=0, train_loss=2.4543793201446533\n",
      "Epoch=378, iteration=1, train_loss=2.4536755084991455\n",
      "Epoch=378, iteration=2, train_loss=2.491570234298706\n",
      "Epoch=378, iteration=3, train_loss=2.6486122608184814\n",
      "Epoch=378, val_loss=2.6417717933654785\n",
      "Epoch=379, iteration=0, train_loss=2.454106330871582\n",
      "Epoch=379, iteration=1, train_loss=2.453449010848999\n",
      "Epoch=379, iteration=2, train_loss=2.4913442134857178\n",
      "Epoch=379, iteration=3, train_loss=2.648364305496216\n",
      "Epoch=379, val_loss=2.6415305137634277\n",
      "Epoch=380, iteration=0, train_loss=2.4538345336914062\n",
      "Epoch=380, iteration=1, train_loss=2.453223466873169\n",
      "Epoch=380, iteration=2, train_loss=2.491119384765625\n",
      "Epoch=380, iteration=3, train_loss=2.6481165885925293\n",
      "Epoch=380, val_loss=2.6412899494171143\n",
      "Epoch=381, iteration=0, train_loss=2.4535632133483887\n",
      "Epoch=381, iteration=1, train_loss=2.452997922897339\n",
      "Epoch=381, iteration=2, train_loss=2.4908947944641113\n",
      "Epoch=381, iteration=3, train_loss=2.6478703022003174\n",
      "Epoch=381, val_loss=2.6410510540008545\n",
      "Epoch=382, iteration=0, train_loss=2.4532926082611084\n",
      "Epoch=382, iteration=1, train_loss=2.4527745246887207\n",
      "Epoch=382, iteration=2, train_loss=2.490671157836914\n",
      "Epoch=382, iteration=3, train_loss=2.6476247310638428\n",
      "Epoch=382, val_loss=2.640812635421753\n",
      "Epoch=383, iteration=0, train_loss=2.4530222415924072\n",
      "Epoch=383, iteration=1, train_loss=2.4525506496429443\n",
      "Epoch=383, iteration=2, train_loss=2.490447998046875\n",
      "Epoch=383, iteration=3, train_loss=2.647380828857422\n",
      "Epoch=383, val_loss=2.640576124191284\n",
      "Epoch=384, iteration=0, train_loss=2.4527530670166016\n",
      "Epoch=384, iteration=1, train_loss=2.4523277282714844\n",
      "Epoch=384, iteration=2, train_loss=2.4902260303497314\n",
      "Epoch=384, iteration=3, train_loss=2.64713716506958\n",
      "Epoch=384, val_loss=2.6403403282165527\n",
      "Epoch=385, iteration=0, train_loss=2.452483892440796\n",
      "Epoch=385, iteration=1, train_loss=2.4521055221557617\n",
      "Epoch=385, iteration=2, train_loss=2.490004301071167\n",
      "Epoch=385, iteration=3, train_loss=2.6468944549560547\n",
      "Epoch=385, val_loss=2.6401052474975586\n",
      "Epoch=386, iteration=0, train_loss=2.452216386795044\n",
      "Epoch=386, iteration=1, train_loss=2.4518842697143555\n",
      "Epoch=386, iteration=2, train_loss=2.489783525466919\n",
      "Epoch=386, iteration=3, train_loss=2.6466526985168457\n",
      "Epoch=386, val_loss=2.6398720741271973\n",
      "Epoch=387, iteration=0, train_loss=2.451948881149292\n",
      "Epoch=387, iteration=1, train_loss=2.4516632556915283\n",
      "Epoch=387, iteration=2, train_loss=2.4895637035369873\n",
      "Epoch=387, iteration=3, train_loss=2.646411895751953\n",
      "Epoch=387, val_loss=2.6396398544311523\n",
      "Epoch=388, iteration=0, train_loss=2.4516820907592773\n",
      "Epoch=388, iteration=1, train_loss=2.4514429569244385\n",
      "Epoch=388, iteration=2, train_loss=2.4893438816070557\n",
      "Epoch=388, iteration=3, train_loss=2.646172046661377\n",
      "Epoch=388, val_loss=2.6394081115722656\n",
      "Epoch=389, iteration=0, train_loss=2.451416015625\n",
      "Epoch=389, iteration=1, train_loss=2.451223850250244\n",
      "Epoch=389, iteration=2, train_loss=2.4891252517700195\n",
      "Epoch=389, iteration=3, train_loss=2.645933151245117\n",
      "Epoch=389, val_loss=2.6391777992248535\n",
      "Epoch=390, iteration=0, train_loss=2.45115065574646\n",
      "Epoch=390, iteration=1, train_loss=2.451005220413208\n",
      "Epoch=390, iteration=2, train_loss=2.4889070987701416\n",
      "Epoch=390, iteration=3, train_loss=2.645695209503174\n",
      "Epoch=390, val_loss=2.638948678970337\n",
      "Epoch=391, iteration=0, train_loss=2.450885772705078\n",
      "Epoch=391, iteration=1, train_loss=2.45078706741333\n",
      "Epoch=391, iteration=2, train_loss=2.48868989944458\n",
      "Epoch=391, iteration=3, train_loss=2.645458221435547\n",
      "Epoch=391, val_loss=2.638720750808716\n",
      "Epoch=392, iteration=0, train_loss=2.4506211280822754\n",
      "Epoch=392, iteration=1, train_loss=2.4505693912506104\n",
      "Epoch=392, iteration=2, train_loss=2.4884731769561768\n",
      "Epoch=392, iteration=3, train_loss=2.645221710205078\n",
      "Epoch=392, val_loss=2.638493776321411\n",
      "Epoch=393, iteration=0, train_loss=2.450357675552368\n",
      "Epoch=393, iteration=1, train_loss=2.450352430343628\n",
      "Epoch=393, iteration=2, train_loss=2.4882569313049316\n",
      "Epoch=393, iteration=3, train_loss=2.644986391067505\n",
      "Epoch=393, val_loss=2.6382675170898438\n",
      "Epoch=394, iteration=0, train_loss=2.4500949382781982\n",
      "Epoch=394, iteration=1, train_loss=2.450136423110962\n",
      "Epoch=394, iteration=2, train_loss=2.4880409240722656\n",
      "Epoch=394, iteration=3, train_loss=2.64475154876709\n",
      "Epoch=394, val_loss=2.638042688369751\n",
      "Epoch=395, iteration=0, train_loss=2.4498326778411865\n",
      "Epoch=395, iteration=1, train_loss=2.449920654296875\n",
      "Epoch=395, iteration=2, train_loss=2.487826347351074\n",
      "Epoch=395, iteration=3, train_loss=2.6445178985595703\n",
      "Epoch=395, val_loss=2.6378188133239746\n",
      "Epoch=396, iteration=0, train_loss=2.449571132659912\n",
      "Epoch=396, iteration=1, train_loss=2.4497053623199463\n",
      "Epoch=396, iteration=2, train_loss=2.48761248588562\n",
      "Epoch=396, iteration=3, train_loss=2.644285202026367\n",
      "Epoch=396, val_loss=2.6375958919525146\n",
      "Epoch=397, iteration=0, train_loss=2.449309825897217\n",
      "Epoch=397, iteration=1, train_loss=2.449491262435913\n",
      "Epoch=397, iteration=2, train_loss=2.487398862838745\n",
      "Epoch=397, iteration=3, train_loss=2.6440532207489014\n",
      "Epoch=397, val_loss=2.637373924255371\n",
      "Epoch=398, iteration=0, train_loss=2.449049711227417\n",
      "Epoch=398, iteration=1, train_loss=2.449277639389038\n",
      "Epoch=398, iteration=2, train_loss=2.4871859550476074\n",
      "Epoch=398, iteration=3, train_loss=2.643821954727173\n",
      "Epoch=398, val_loss=2.637153387069702\n",
      "Epoch=399, iteration=0, train_loss=2.4487898349761963\n",
      "Epoch=399, iteration=1, train_loss=2.4490644931793213\n",
      "Epoch=399, iteration=2, train_loss=2.486974000930786\n",
      "Epoch=399, iteration=3, train_loss=2.64359188079834\n",
      "Epoch=399, val_loss=2.6369330883026123\n",
      "Epoch=400, iteration=0, train_loss=2.448530673980713\n",
      "Epoch=400, iteration=1, train_loss=2.4488525390625\n",
      "Epoch=400, iteration=2, train_loss=2.486762523651123\n",
      "Epoch=400, iteration=3, train_loss=2.643362522125244\n",
      "Epoch=400, val_loss=2.636714458465576\n",
      "Epoch=401, iteration=0, train_loss=2.4482717514038086\n",
      "Epoch=401, iteration=1, train_loss=2.4486398696899414\n",
      "Epoch=401, iteration=2, train_loss=2.486551284790039\n",
      "Epoch=401, iteration=3, train_loss=2.6431338787078857\n",
      "Epoch=401, val_loss=2.6364967823028564\n",
      "Epoch=402, iteration=0, train_loss=2.4480140209198\n",
      "Epoch=402, iteration=1, train_loss=2.4484288692474365\n",
      "Epoch=402, iteration=2, train_loss=2.4863409996032715\n",
      "Epoch=402, iteration=3, train_loss=2.6429057121276855\n",
      "Epoch=402, val_loss=2.636279821395874\n",
      "Epoch=403, iteration=0, train_loss=2.447756767272949\n",
      "Epoch=403, iteration=1, train_loss=2.44821834564209\n",
      "Epoch=403, iteration=2, train_loss=2.486131429672241\n",
      "Epoch=403, iteration=3, train_loss=2.642679214477539\n",
      "Epoch=403, val_loss=2.636064052581787\n",
      "Epoch=404, iteration=0, train_loss=2.447500228881836\n",
      "Epoch=404, iteration=1, train_loss=2.4480082988739014\n",
      "Epoch=404, iteration=2, train_loss=2.4859225749969482\n",
      "Epoch=404, iteration=3, train_loss=2.642453193664551\n",
      "Epoch=404, val_loss=2.6358489990234375\n",
      "Epoch=405, iteration=0, train_loss=2.4472439289093018\n",
      "Epoch=405, iteration=1, train_loss=2.447798490524292\n",
      "Epoch=405, iteration=2, train_loss=2.4857141971588135\n",
      "Epoch=405, iteration=3, train_loss=2.6422278881073\n",
      "Epoch=405, val_loss=2.6356353759765625\n",
      "Epoch=406, iteration=0, train_loss=2.446988344192505\n",
      "Epoch=406, iteration=1, train_loss=2.447589874267578\n",
      "Epoch=406, iteration=2, train_loss=2.485506057739258\n",
      "Epoch=406, iteration=3, train_loss=2.642003059387207\n",
      "Epoch=406, val_loss=2.6354222297668457\n",
      "Epoch=407, iteration=0, train_loss=2.4467334747314453\n",
      "Epoch=407, iteration=1, train_loss=2.4473817348480225\n",
      "Epoch=407, iteration=2, train_loss=2.4852991104125977\n",
      "Epoch=407, iteration=3, train_loss=2.6417791843414307\n",
      "Epoch=407, val_loss=2.6352102756500244\n",
      "Epoch=408, iteration=0, train_loss=2.446479558944702\n",
      "Epoch=408, iteration=1, train_loss=2.447173595428467\n",
      "Epoch=408, iteration=2, train_loss=2.4850924015045166\n",
      "Epoch=408, iteration=3, train_loss=2.64155650138855\n",
      "Epoch=408, val_loss=2.6349992752075195\n",
      "Epoch=409, iteration=0, train_loss=2.446225643157959\n",
      "Epoch=409, iteration=1, train_loss=2.4469666481018066\n",
      "Epoch=409, iteration=2, train_loss=2.484886407852173\n",
      "Epoch=409, iteration=3, train_loss=2.641334295272827\n",
      "Epoch=409, val_loss=2.634789228439331\n",
      "Epoch=410, iteration=0, train_loss=2.445972442626953\n",
      "Epoch=410, iteration=1, train_loss=2.4467599391937256\n",
      "Epoch=410, iteration=2, train_loss=2.4846808910369873\n",
      "Epoch=410, iteration=3, train_loss=2.64111328125\n",
      "Epoch=410, val_loss=2.634580135345459\n",
      "Epoch=411, iteration=0, train_loss=2.4457199573516846\n",
      "Epoch=411, iteration=1, train_loss=2.446553945541382\n",
      "Epoch=411, iteration=2, train_loss=2.4844765663146973\n",
      "Epoch=411, iteration=3, train_loss=2.640892505645752\n",
      "Epoch=411, val_loss=2.634371757507324\n",
      "Epoch=412, iteration=0, train_loss=2.445467948913574\n",
      "Epoch=412, iteration=1, train_loss=2.4463484287261963\n",
      "Epoch=412, iteration=2, train_loss=2.484272003173828\n",
      "Epoch=412, iteration=3, train_loss=2.6406726837158203\n",
      "Epoch=412, val_loss=2.634164333343506\n",
      "Epoch=413, iteration=0, train_loss=2.4452168941497803\n",
      "Epoch=413, iteration=1, train_loss=2.446143627166748\n",
      "Epoch=413, iteration=2, train_loss=2.4840686321258545\n",
      "Epoch=413, iteration=3, train_loss=2.640453577041626\n",
      "Epoch=413, val_loss=2.633958101272583\n",
      "Epoch=414, iteration=0, train_loss=2.4449665546417236\n",
      "Epoch=414, iteration=1, train_loss=2.445939302444458\n",
      "Epoch=414, iteration=2, train_loss=2.483865737915039\n",
      "Epoch=414, iteration=3, train_loss=2.640235424041748\n",
      "Epoch=414, val_loss=2.6337528228759766\n",
      "Epoch=415, iteration=0, train_loss=2.444716215133667\n",
      "Epoch=415, iteration=1, train_loss=2.4457356929779053\n",
      "Epoch=415, iteration=2, train_loss=2.483663558959961\n",
      "Epoch=415, iteration=3, train_loss=2.6400177478790283\n",
      "Epoch=415, val_loss=2.6335480213165283\n",
      "Epoch=416, iteration=0, train_loss=2.4444665908813477\n",
      "Epoch=416, iteration=1, train_loss=2.4455325603485107\n",
      "Epoch=416, iteration=2, train_loss=2.483461856842041\n",
      "Epoch=416, iteration=3, train_loss=2.639801502227783\n",
      "Epoch=416, val_loss=2.6333444118499756\n",
      "Epoch=417, iteration=0, train_loss=2.4442179203033447\n",
      "Epoch=417, iteration=1, train_loss=2.4453299045562744\n",
      "Epoch=417, iteration=2, train_loss=2.483260154724121\n",
      "Epoch=417, iteration=3, train_loss=2.6395857334136963\n",
      "Epoch=417, val_loss=2.63314151763916\n",
      "Epoch=418, iteration=0, train_loss=2.4439697265625\n",
      "Epoch=418, iteration=1, train_loss=2.445127487182617\n",
      "Epoch=418, iteration=2, train_loss=2.4830596446990967\n",
      "Epoch=418, iteration=3, train_loss=2.6393704414367676\n",
      "Epoch=418, val_loss=2.632939577102661\n",
      "Epoch=419, iteration=0, train_loss=2.4437224864959717\n",
      "Epoch=419, iteration=1, train_loss=2.4449265003204346\n",
      "Epoch=419, iteration=2, train_loss=2.4828600883483887\n",
      "Epoch=419, iteration=3, train_loss=2.639155864715576\n",
      "Epoch=419, val_loss=2.6327385902404785\n",
      "Epoch=420, iteration=0, train_loss=2.4434752464294434\n",
      "Epoch=420, iteration=1, train_loss=2.444725275039673\n",
      "Epoch=420, iteration=2, train_loss=2.4826605319976807\n",
      "Epoch=420, iteration=3, train_loss=2.638942003250122\n",
      "Epoch=420, val_loss=2.6325387954711914\n",
      "Epoch=421, iteration=0, train_loss=2.4432287216186523\n",
      "Epoch=421, iteration=1, train_loss=2.4445250034332275\n",
      "Epoch=421, iteration=2, train_loss=2.482461929321289\n",
      "Epoch=421, iteration=3, train_loss=2.6387290954589844\n",
      "Epoch=421, val_loss=2.6323394775390625\n",
      "Epoch=422, iteration=0, train_loss=2.4429831504821777\n",
      "Epoch=422, iteration=1, train_loss=2.4443249702453613\n",
      "Epoch=422, iteration=2, train_loss=2.4822635650634766\n",
      "Epoch=422, iteration=3, train_loss=2.638516902923584\n",
      "Epoch=422, val_loss=2.632140874862671\n",
      "Epoch=423, iteration=0, train_loss=2.4427378177642822\n",
      "Epoch=423, iteration=1, train_loss=2.4441258907318115\n",
      "Epoch=423, iteration=2, train_loss=2.4820661544799805\n",
      "Epoch=423, iteration=3, train_loss=2.6383056640625\n",
      "Epoch=423, val_loss=2.631943464279175\n",
      "Epoch=424, iteration=0, train_loss=2.442492961883545\n",
      "Epoch=424, iteration=1, train_loss=2.443927049636841\n",
      "Epoch=424, iteration=2, train_loss=2.4818689823150635\n",
      "Epoch=424, iteration=3, train_loss=2.638094663619995\n",
      "Epoch=424, val_loss=2.631746768951416\n",
      "Epoch=425, iteration=0, train_loss=2.442249298095703\n",
      "Epoch=425, iteration=1, train_loss=2.4437289237976074\n",
      "Epoch=425, iteration=2, train_loss=2.481672525405884\n",
      "Epoch=425, iteration=3, train_loss=2.6378848552703857\n",
      "Epoch=425, val_loss=2.6315505504608154\n",
      "Epoch=426, iteration=0, train_loss=2.4420063495635986\n",
      "Epoch=426, iteration=1, train_loss=2.443531036376953\n",
      "Epoch=426, iteration=2, train_loss=2.4814765453338623\n",
      "Epoch=426, iteration=3, train_loss=2.6376755237579346\n",
      "Epoch=426, val_loss=2.6313557624816895\n",
      "Epoch=427, iteration=0, train_loss=2.441763401031494\n",
      "Epoch=427, iteration=1, train_loss=2.443333625793457\n",
      "Epoch=427, iteration=2, train_loss=2.481281280517578\n",
      "Epoch=427, iteration=3, train_loss=2.6374671459198\n",
      "Epoch=427, val_loss=2.631161689758301\n",
      "Epoch=428, iteration=0, train_loss=2.441521167755127\n",
      "Epoch=428, iteration=1, train_loss=2.4431369304656982\n",
      "Epoch=428, iteration=2, train_loss=2.481086492538452\n",
      "Epoch=428, iteration=3, train_loss=2.637259006500244\n",
      "Epoch=428, val_loss=2.6309683322906494\n",
      "Epoch=429, iteration=0, train_loss=2.441279172897339\n",
      "Epoch=429, iteration=1, train_loss=2.4429409503936768\n",
      "Epoch=429, iteration=2, train_loss=2.4808919429779053\n",
      "Epoch=429, iteration=3, train_loss=2.637052059173584\n",
      "Epoch=429, val_loss=2.6307756900787354\n",
      "Epoch=430, iteration=0, train_loss=2.4410388469696045\n",
      "Epoch=430, iteration=1, train_loss=2.4427454471588135\n",
      "Epoch=430, iteration=2, train_loss=2.480698347091675\n",
      "Epoch=430, iteration=3, train_loss=2.636845827102661\n",
      "Epoch=430, val_loss=2.630584239959717\n",
      "Epoch=431, iteration=0, train_loss=2.440798282623291\n",
      "Epoch=431, iteration=1, train_loss=2.4425504207611084\n",
      "Epoch=431, iteration=2, train_loss=2.4805052280426025\n",
      "Epoch=431, iteration=3, train_loss=2.6366398334503174\n",
      "Epoch=431, val_loss=2.6303932666778564\n",
      "Epoch=432, iteration=0, train_loss=2.440558433532715\n",
      "Epoch=432, iteration=1, train_loss=2.4423556327819824\n",
      "Epoch=432, iteration=2, train_loss=2.4803130626678467\n",
      "Epoch=432, iteration=3, train_loss=2.636435031890869\n",
      "Epoch=432, val_loss=2.6302030086517334\n",
      "Epoch=433, iteration=0, train_loss=2.440319538116455\n",
      "Epoch=433, iteration=1, train_loss=2.442162036895752\n",
      "Epoch=433, iteration=2, train_loss=2.480120897293091\n",
      "Epoch=433, iteration=3, train_loss=2.63623046875\n",
      "Epoch=433, val_loss=2.630014181137085\n",
      "Epoch=434, iteration=0, train_loss=2.440080404281616\n",
      "Epoch=434, iteration=1, train_loss=2.4419682025909424\n",
      "Epoch=434, iteration=2, train_loss=2.4799294471740723\n",
      "Epoch=434, iteration=3, train_loss=2.6360268592834473\n",
      "Epoch=434, val_loss=2.6298253536224365\n",
      "Epoch=435, iteration=0, train_loss=2.439842700958252\n",
      "Epoch=435, iteration=1, train_loss=2.44177508354187\n",
      "Epoch=435, iteration=2, train_loss=2.47973895072937\n",
      "Epoch=435, iteration=3, train_loss=2.635823965072632\n",
      "Epoch=435, val_loss=2.6296374797821045\n",
      "Epoch=436, iteration=0, train_loss=2.439605712890625\n",
      "Epoch=436, iteration=1, train_loss=2.441582441329956\n",
      "Epoch=436, iteration=2, train_loss=2.479548454284668\n",
      "Epoch=436, iteration=3, train_loss=2.6356217861175537\n",
      "Epoch=436, val_loss=2.629450798034668\n",
      "Epoch=437, iteration=0, train_loss=2.439368963241577\n",
      "Epoch=437, iteration=1, train_loss=2.4413907527923584\n",
      "Epoch=437, iteration=2, train_loss=2.4793589115142822\n",
      "Epoch=437, iteration=3, train_loss=2.635420083999634\n",
      "Epoch=437, val_loss=2.6292643547058105\n",
      "Epoch=438, iteration=0, train_loss=2.4391329288482666\n",
      "Epoch=438, iteration=1, train_loss=2.4411990642547607\n",
      "Epoch=438, iteration=2, train_loss=2.4791696071624756\n",
      "Epoch=438, iteration=3, train_loss=2.635219097137451\n",
      "Epoch=438, val_loss=2.6290791034698486\n",
      "Epoch=439, iteration=0, train_loss=2.4388973712921143\n",
      "Epoch=439, iteration=1, train_loss=2.4410083293914795\n",
      "Epoch=439, iteration=2, train_loss=2.4789810180664062\n",
      "Epoch=439, iteration=3, train_loss=2.635018825531006\n",
      "Epoch=439, val_loss=2.628894805908203\n",
      "Epoch=440, iteration=0, train_loss=2.43866229057312\n",
      "Epoch=440, iteration=1, train_loss=2.4408175945281982\n",
      "Epoch=440, iteration=2, train_loss=2.478792905807495\n",
      "Epoch=440, iteration=3, train_loss=2.634819507598877\n",
      "Epoch=440, val_loss=2.628710985183716\n",
      "Epoch=441, iteration=0, train_loss=2.4384284019470215\n",
      "Epoch=441, iteration=1, train_loss=2.4406280517578125\n",
      "Epoch=441, iteration=2, train_loss=2.478605270385742\n",
      "Epoch=441, iteration=3, train_loss=2.6346206665039062\n",
      "Epoch=441, val_loss=2.628528118133545\n",
      "Epoch=442, iteration=0, train_loss=2.438194513320923\n",
      "Epoch=442, iteration=1, train_loss=2.440438747406006\n",
      "Epoch=442, iteration=2, train_loss=2.4784185886383057\n",
      "Epoch=442, iteration=3, train_loss=2.6344220638275146\n",
      "Epoch=442, val_loss=2.628345251083374\n",
      "Epoch=443, iteration=0, train_loss=2.4379615783691406\n",
      "Epoch=443, iteration=1, train_loss=2.4402496814727783\n",
      "Epoch=443, iteration=2, train_loss=2.478231906890869\n",
      "Epoch=443, iteration=3, train_loss=2.6342248916625977\n",
      "Epoch=443, val_loss=2.628164052963257\n",
      "Epoch=444, iteration=0, train_loss=2.4377291202545166\n",
      "Epoch=444, iteration=1, train_loss=2.44006085395813\n",
      "Epoch=444, iteration=2, train_loss=2.478046178817749\n",
      "Epoch=444, iteration=3, train_loss=2.6340277194976807\n",
      "Epoch=444, val_loss=2.6279830932617188\n",
      "Epoch=445, iteration=0, train_loss=2.43749737739563\n",
      "Epoch=445, iteration=1, train_loss=2.439873218536377\n",
      "Epoch=445, iteration=2, train_loss=2.477860927581787\n",
      "Epoch=445, iteration=3, train_loss=2.63383150100708\n",
      "Epoch=445, val_loss=2.627803325653076\n",
      "Epoch=446, iteration=0, train_loss=2.4372658729553223\n",
      "Epoch=446, iteration=1, train_loss=2.439685344696045\n",
      "Epoch=446, iteration=2, train_loss=2.4776761531829834\n",
      "Epoch=446, iteration=3, train_loss=2.633635997772217\n",
      "Epoch=446, val_loss=2.6276237964630127\n",
      "Epoch=447, iteration=0, train_loss=2.437035322189331\n",
      "Epoch=447, iteration=1, train_loss=2.4394989013671875\n",
      "Epoch=447, iteration=2, train_loss=2.477491855621338\n",
      "Epoch=447, iteration=3, train_loss=2.633441209793091\n",
      "Epoch=447, val_loss=2.6274452209472656\n",
      "Epoch=448, iteration=0, train_loss=2.436805486679077\n",
      "Epoch=448, iteration=1, train_loss=2.439312219619751\n",
      "Epoch=448, iteration=2, train_loss=2.4773080348968506\n",
      "Epoch=448, iteration=3, train_loss=2.633246898651123\n",
      "Epoch=448, val_loss=2.627267360687256\n",
      "Epoch=449, iteration=0, train_loss=2.4365758895874023\n",
      "Epoch=449, iteration=1, train_loss=2.4391262531280518\n",
      "Epoch=449, iteration=2, train_loss=2.4771246910095215\n",
      "Epoch=449, iteration=3, train_loss=2.6330533027648926\n",
      "Epoch=449, val_loss=2.627089738845825\n",
      "Epoch=450, iteration=0, train_loss=2.436347007751465\n",
      "Epoch=450, iteration=1, train_loss=2.4389405250549316\n",
      "Epoch=450, iteration=2, train_loss=2.4769420623779297\n",
      "Epoch=450, iteration=3, train_loss=2.632859945297241\n",
      "Epoch=450, val_loss=2.62691330909729\n",
      "Epoch=451, iteration=0, train_loss=2.4361188411712646\n",
      "Epoch=451, iteration=1, train_loss=2.438755989074707\n",
      "Epoch=451, iteration=2, train_loss=2.476759910583496\n",
      "Epoch=451, iteration=3, train_loss=2.6326675415039062\n",
      "Epoch=451, val_loss=2.6267378330230713\n",
      "Epoch=452, iteration=0, train_loss=2.4358911514282227\n",
      "Epoch=452, iteration=1, train_loss=2.438570976257324\n",
      "Epoch=452, iteration=2, train_loss=2.4765784740448\n",
      "Epoch=452, iteration=3, train_loss=2.6324758529663086\n",
      "Epoch=452, val_loss=2.6265628337860107\n",
      "Epoch=453, iteration=0, train_loss=2.435664176940918\n",
      "Epoch=453, iteration=1, train_loss=2.438387155532837\n",
      "Epoch=453, iteration=2, train_loss=2.4763975143432617\n",
      "Epoch=453, iteration=3, train_loss=2.6322848796844482\n",
      "Epoch=453, val_loss=2.6263880729675293\n",
      "Epoch=454, iteration=0, train_loss=2.4354379177093506\n",
      "Epoch=454, iteration=1, train_loss=2.4382035732269287\n",
      "Epoch=454, iteration=2, train_loss=2.4762165546417236\n",
      "Epoch=454, iteration=3, train_loss=2.632094144821167\n",
      "Epoch=454, val_loss=2.6262145042419434\n",
      "Epoch=455, iteration=0, train_loss=2.4352121353149414\n",
      "Epoch=455, iteration=1, train_loss=2.438020944595337\n",
      "Epoch=455, iteration=2, train_loss=2.476036787033081\n",
      "Epoch=455, iteration=3, train_loss=2.631904363632202\n",
      "Epoch=455, val_loss=2.6260416507720947\n",
      "Epoch=456, iteration=0, train_loss=2.4349868297576904\n",
      "Epoch=456, iteration=1, train_loss=2.437838077545166\n",
      "Epoch=456, iteration=2, train_loss=2.4758574962615967\n",
      "Epoch=456, iteration=3, train_loss=2.6317150592803955\n",
      "Epoch=456, val_loss=2.6258692741394043\n",
      "Epoch=457, iteration=0, train_loss=2.434762477874756\n",
      "Epoch=457, iteration=1, train_loss=2.4376561641693115\n",
      "Epoch=457, iteration=2, train_loss=2.4756784439086914\n",
      "Epoch=457, iteration=3, train_loss=2.6315267086029053\n",
      "Epoch=457, val_loss=2.625697612762451\n",
      "Epoch=458, iteration=0, train_loss=2.4345383644104004\n",
      "Epoch=458, iteration=1, train_loss=2.437474250793457\n",
      "Epoch=458, iteration=2, train_loss=2.4754998683929443\n",
      "Epoch=458, iteration=3, train_loss=2.631338357925415\n",
      "Epoch=458, val_loss=2.6255269050598145\n",
      "Epoch=459, iteration=0, train_loss=2.4343149662017822\n",
      "Epoch=459, iteration=1, train_loss=2.4372928142547607\n",
      "Epoch=459, iteration=2, train_loss=2.4753220081329346\n",
      "Epoch=459, iteration=3, train_loss=2.631150960922241\n",
      "Epoch=459, val_loss=2.625356674194336\n",
      "Epoch=460, iteration=0, train_loss=2.4340922832489014\n",
      "Epoch=460, iteration=1, train_loss=2.437112331390381\n",
      "Epoch=460, iteration=2, train_loss=2.475144624710083\n",
      "Epoch=460, iteration=3, train_loss=2.6309640407562256\n",
      "Epoch=460, val_loss=2.6251869201660156\n",
      "Epoch=461, iteration=0, train_loss=2.433870315551758\n",
      "Epoch=461, iteration=1, train_loss=2.43693208694458\n",
      "Epoch=461, iteration=2, train_loss=2.4749679565429688\n",
      "Epoch=461, iteration=3, train_loss=2.6307783126831055\n",
      "Epoch=461, val_loss=2.6250181198120117\n",
      "Epoch=462, iteration=0, train_loss=2.4336483478546143\n",
      "Epoch=462, iteration=1, train_loss=2.4367525577545166\n",
      "Epoch=462, iteration=2, train_loss=2.4747915267944336\n",
      "Epoch=462, iteration=3, train_loss=2.6305925846099854\n",
      "Epoch=462, val_loss=2.624849796295166\n",
      "Epoch=463, iteration=0, train_loss=2.4334278106689453\n",
      "Epoch=463, iteration=1, train_loss=2.436573028564453\n",
      "Epoch=463, iteration=2, train_loss=2.4746158123016357\n",
      "Epoch=463, iteration=3, train_loss=2.6304078102111816\n",
      "Epoch=463, val_loss=2.6246824264526367\n",
      "Epoch=464, iteration=0, train_loss=2.4332072734832764\n",
      "Epoch=464, iteration=1, train_loss=2.436393976211548\n",
      "Epoch=464, iteration=2, train_loss=2.474439859390259\n",
      "Epoch=464, iteration=3, train_loss=2.630223512649536\n",
      "Epoch=464, val_loss=2.6245152950286865\n",
      "Epoch=465, iteration=0, train_loss=2.4329874515533447\n",
      "Epoch=465, iteration=1, train_loss=2.436216115951538\n",
      "Epoch=465, iteration=2, train_loss=2.4742653369903564\n",
      "Epoch=465, iteration=3, train_loss=2.630039691925049\n",
      "Epoch=465, val_loss=2.6243491172790527\n",
      "Epoch=466, iteration=0, train_loss=2.4327683448791504\n",
      "Epoch=466, iteration=1, train_loss=2.43603777885437\n",
      "Epoch=466, iteration=2, train_loss=2.474090814590454\n",
      "Epoch=466, iteration=3, train_loss=2.6298563480377197\n",
      "Epoch=466, val_loss=2.624183177947998\n",
      "Epoch=467, iteration=0, train_loss=2.4325497150421143\n",
      "Epoch=467, iteration=1, train_loss=2.4358601570129395\n",
      "Epoch=467, iteration=2, train_loss=2.473917245864868\n",
      "Epoch=467, iteration=3, train_loss=2.629673957824707\n",
      "Epoch=467, val_loss=2.624018430709839\n",
      "Epoch=468, iteration=0, train_loss=2.4323318004608154\n",
      "Epoch=468, iteration=1, train_loss=2.435683012008667\n",
      "Epoch=468, iteration=2, train_loss=2.4737436771392822\n",
      "Epoch=468, iteration=3, train_loss=2.6294918060302734\n",
      "Epoch=468, val_loss=2.623853921890259\n",
      "Epoch=469, iteration=0, train_loss=2.432114362716675\n",
      "Epoch=469, iteration=1, train_loss=2.4355063438415527\n",
      "Epoch=469, iteration=2, train_loss=2.4735710620880127\n",
      "Epoch=469, iteration=3, train_loss=2.6293106079101562\n",
      "Epoch=469, val_loss=2.623690366744995\n",
      "Epoch=470, iteration=0, train_loss=2.4318974018096924\n",
      "Epoch=470, iteration=1, train_loss=2.435330390930176\n",
      "Epoch=470, iteration=2, train_loss=2.4733989238739014\n",
      "Epoch=470, iteration=3, train_loss=2.6291298866271973\n",
      "Epoch=470, val_loss=2.6235270500183105\n",
      "Epoch=471, iteration=0, train_loss=2.4316816329956055\n",
      "Epoch=471, iteration=1, train_loss=2.435154438018799\n",
      "Epoch=471, iteration=2, train_loss=2.4732272624969482\n",
      "Epoch=471, iteration=3, train_loss=2.6289494037628174\n",
      "Epoch=471, val_loss=2.6233649253845215\n",
      "Epoch=472, iteration=0, train_loss=2.4314658641815186\n",
      "Epoch=472, iteration=1, train_loss=2.4349794387817383\n",
      "Epoch=472, iteration=2, train_loss=2.473055839538574\n",
      "Epoch=472, iteration=3, train_loss=2.628770112991333\n",
      "Epoch=472, val_loss=2.6232028007507324\n",
      "Epoch=473, iteration=0, train_loss=2.431250810623169\n",
      "Epoch=473, iteration=1, train_loss=2.434804677963257\n",
      "Epoch=473, iteration=2, train_loss=2.4728846549987793\n",
      "Epoch=473, iteration=3, train_loss=2.6285910606384277\n",
      "Epoch=473, val_loss=2.6230411529541016\n",
      "Epoch=474, iteration=0, train_loss=2.4310364723205566\n",
      "Epoch=474, iteration=1, train_loss=2.4346303939819336\n",
      "Epoch=474, iteration=2, train_loss=2.4727141857147217\n",
      "Epoch=474, iteration=3, train_loss=2.6284127235412598\n",
      "Epoch=474, val_loss=2.622880697250366\n",
      "Epoch=475, iteration=0, train_loss=2.4308223724365234\n",
      "Epoch=475, iteration=1, train_loss=2.4344563484191895\n",
      "Epoch=475, iteration=2, train_loss=2.4725441932678223\n",
      "Epoch=475, iteration=3, train_loss=2.628234386444092\n",
      "Epoch=475, val_loss=2.62272047996521\n",
      "Epoch=476, iteration=0, train_loss=2.4306094646453857\n",
      "Epoch=476, iteration=1, train_loss=2.4342830181121826\n",
      "Epoch=476, iteration=2, train_loss=2.47237491607666\n",
      "Epoch=476, iteration=3, train_loss=2.6280570030212402\n",
      "Epoch=476, val_loss=2.622561454772949\n",
      "Epoch=477, iteration=0, train_loss=2.430396795272827\n",
      "Epoch=477, iteration=1, train_loss=2.434109926223755\n",
      "Epoch=477, iteration=2, train_loss=2.4722061157226562\n",
      "Epoch=477, iteration=3, train_loss=2.627880334854126\n",
      "Epoch=477, val_loss=2.6224026679992676\n",
      "Epoch=478, iteration=0, train_loss=2.430185079574585\n",
      "Epoch=478, iteration=1, train_loss=2.4339373111724854\n",
      "Epoch=478, iteration=2, train_loss=2.4720375537872314\n",
      "Epoch=478, iteration=3, train_loss=2.62770414352417\n",
      "Epoch=478, val_loss=2.622244119644165\n",
      "Epoch=479, iteration=0, train_loss=2.4299731254577637\n",
      "Epoch=479, iteration=1, train_loss=2.433764934539795\n",
      "Epoch=479, iteration=2, train_loss=2.471869945526123\n",
      "Epoch=479, iteration=3, train_loss=2.627528429031372\n",
      "Epoch=479, val_loss=2.622086524963379\n",
      "Epoch=480, iteration=0, train_loss=2.4297618865966797\n",
      "Epoch=480, iteration=1, train_loss=2.433593273162842\n",
      "Epoch=480, iteration=2, train_loss=2.4717025756835938\n",
      "Epoch=480, iteration=3, train_loss=2.6273534297943115\n",
      "Epoch=480, val_loss=2.621929407119751\n",
      "Epoch=481, iteration=0, train_loss=2.429551839828491\n",
      "Epoch=481, iteration=1, train_loss=2.433422088623047\n",
      "Epoch=481, iteration=2, train_loss=2.4715356826782227\n",
      "Epoch=481, iteration=3, train_loss=2.6271791458129883\n",
      "Epoch=481, val_loss=2.6217730045318604\n",
      "Epoch=482, iteration=0, train_loss=2.429342031478882\n",
      "Epoch=482, iteration=1, train_loss=2.433251142501831\n",
      "Epoch=482, iteration=2, train_loss=2.4713690280914307\n",
      "Epoch=482, iteration=3, train_loss=2.6270053386688232\n",
      "Epoch=482, val_loss=2.621617078781128\n",
      "Epoch=483, iteration=0, train_loss=2.429133176803589\n",
      "Epoch=483, iteration=1, train_loss=2.4330806732177734\n",
      "Epoch=483, iteration=2, train_loss=2.471203327178955\n",
      "Epoch=483, iteration=3, train_loss=2.6268317699432373\n",
      "Epoch=483, val_loss=2.621461868286133\n",
      "Epoch=484, iteration=0, train_loss=2.428924322128296\n",
      "Epoch=484, iteration=1, train_loss=2.432910442352295\n",
      "Epoch=484, iteration=2, train_loss=2.4710373878479004\n",
      "Epoch=484, iteration=3, train_loss=2.6266586780548096\n",
      "Epoch=484, val_loss=2.621307134628296\n",
      "Epoch=485, iteration=0, train_loss=2.4287161827087402\n",
      "Epoch=485, iteration=1, train_loss=2.432741165161133\n",
      "Epoch=485, iteration=2, train_loss=2.470872402191162\n",
      "Epoch=485, iteration=3, train_loss=2.6264867782592773\n",
      "Epoch=485, val_loss=2.621152877807617\n",
      "Epoch=486, iteration=0, train_loss=2.428508758544922\n",
      "Epoch=486, iteration=1, train_loss=2.4325716495513916\n",
      "Epoch=486, iteration=2, train_loss=2.470707893371582\n",
      "Epoch=486, iteration=3, train_loss=2.626314878463745\n",
      "Epoch=486, val_loss=2.6209990978240967\n",
      "Epoch=487, iteration=0, train_loss=2.428302049636841\n",
      "Epoch=487, iteration=1, train_loss=2.4324028491973877\n",
      "Epoch=487, iteration=2, train_loss=2.470543622970581\n",
      "Epoch=487, iteration=3, train_loss=2.626143455505371\n",
      "Epoch=487, val_loss=2.6208462715148926\n",
      "Epoch=488, iteration=0, train_loss=2.428095579147339\n",
      "Epoch=488, iteration=1, train_loss=2.432234287261963\n",
      "Epoch=488, iteration=2, train_loss=2.4703800678253174\n",
      "Epoch=488, iteration=3, train_loss=2.6259732246398926\n",
      "Epoch=488, val_loss=2.6206936836242676\n",
      "Epoch=489, iteration=0, train_loss=2.427889823913574\n",
      "Epoch=489, iteration=1, train_loss=2.4320666790008545\n",
      "Epoch=489, iteration=2, train_loss=2.470216751098633\n",
      "Epoch=489, iteration=3, train_loss=2.625803232192993\n",
      "Epoch=489, val_loss=2.62054181098938\n",
      "Epoch=490, iteration=0, train_loss=2.427685022354126\n",
      "Epoch=490, iteration=1, train_loss=2.431899070739746\n",
      "Epoch=490, iteration=2, train_loss=2.4700541496276855\n",
      "Epoch=490, iteration=3, train_loss=2.6256332397460938\n",
      "Epoch=490, val_loss=2.6203906536102295\n",
      "Epoch=491, iteration=0, train_loss=2.4274802207946777\n",
      "Epoch=491, iteration=1, train_loss=2.431731939315796\n",
      "Epoch=491, iteration=2, train_loss=2.4698920249938965\n",
      "Epoch=491, iteration=3, train_loss=2.62546443939209\n",
      "Epoch=491, val_loss=2.620239734649658\n",
      "Epoch=492, iteration=0, train_loss=2.427276134490967\n",
      "Epoch=492, iteration=1, train_loss=2.4315648078918457\n",
      "Epoch=492, iteration=2, train_loss=2.4697301387786865\n",
      "Epoch=492, iteration=3, train_loss=2.625296115875244\n",
      "Epoch=492, val_loss=2.620089292526245\n",
      "Epoch=493, iteration=0, train_loss=2.427072763442993\n",
      "Epoch=493, iteration=1, train_loss=2.431398868560791\n",
      "Epoch=493, iteration=2, train_loss=2.4695687294006348\n",
      "Epoch=493, iteration=3, train_loss=2.6251277923583984\n",
      "Epoch=493, val_loss=2.6199398040771484\n",
      "Epoch=494, iteration=0, train_loss=2.426870107650757\n",
      "Epoch=494, iteration=1, train_loss=2.4312326908111572\n",
      "Epoch=494, iteration=2, train_loss=2.4694080352783203\n",
      "Epoch=494, iteration=3, train_loss=2.624960422515869\n",
      "Epoch=494, val_loss=2.61979079246521\n",
      "Epoch=495, iteration=0, train_loss=2.4266674518585205\n",
      "Epoch=495, iteration=1, train_loss=2.4310672283172607\n",
      "Epoch=495, iteration=2, train_loss=2.4692471027374268\n",
      "Epoch=495, iteration=3, train_loss=2.624793767929077\n",
      "Epoch=495, val_loss=2.6196420192718506\n",
      "Epoch=496, iteration=0, train_loss=2.4264657497406006\n",
      "Epoch=496, iteration=1, train_loss=2.4309017658233643\n",
      "Epoch=496, iteration=2, train_loss=2.4690868854522705\n",
      "Epoch=496, iteration=3, train_loss=2.6246273517608643\n",
      "Epoch=496, val_loss=2.6194939613342285\n",
      "Epoch=497, iteration=0, train_loss=2.4262642860412598\n",
      "Epoch=497, iteration=1, train_loss=2.430737018585205\n",
      "Epoch=497, iteration=2, train_loss=2.4689278602600098\n",
      "Epoch=497, iteration=3, train_loss=2.6244614124298096\n",
      "Epoch=497, val_loss=2.6193463802337646\n",
      "Epoch=498, iteration=0, train_loss=2.4260637760162354\n",
      "Epoch=498, iteration=1, train_loss=2.430572509765625\n",
      "Epoch=498, iteration=2, train_loss=2.46876859664917\n",
      "Epoch=498, iteration=3, train_loss=2.624296188354492\n",
      "Epoch=498, val_loss=2.619199275970459\n",
      "Epoch=499, iteration=0, train_loss=2.42586350440979\n",
      "Epoch=499, iteration=1, train_loss=2.4304087162017822\n",
      "Epoch=499, iteration=2, train_loss=2.4686100482940674\n",
      "Epoch=499, iteration=3, train_loss=2.624130964279175\n",
      "Epoch=499, val_loss=2.6190528869628906\n",
      "Epoch=500, iteration=0, train_loss=2.425663948059082\n",
      "Epoch=500, iteration=1, train_loss=2.4302449226379395\n",
      "Epoch=500, iteration=2, train_loss=2.468451499938965\n",
      "Epoch=500, iteration=3, train_loss=2.623967170715332\n",
      "Epoch=500, val_loss=2.6189067363739014\n",
      "Epoch=501, iteration=0, train_loss=2.4254648685455322\n",
      "Epoch=501, iteration=1, train_loss=2.430082082748413\n",
      "Epoch=501, iteration=2, train_loss=2.4682939052581787\n",
      "Epoch=501, iteration=3, train_loss=2.6238033771514893\n",
      "Epoch=501, val_loss=2.6187613010406494\n",
      "Epoch=502, iteration=0, train_loss=2.4252660274505615\n",
      "Epoch=502, iteration=1, train_loss=2.4299190044403076\n",
      "Epoch=502, iteration=2, train_loss=2.4681365489959717\n",
      "Epoch=502, iteration=3, train_loss=2.623640298843384\n",
      "Epoch=502, val_loss=2.6186163425445557\n",
      "Epoch=503, iteration=0, train_loss=2.4250681400299072\n",
      "Epoch=503, iteration=1, train_loss=2.4297564029693604\n",
      "Epoch=503, iteration=2, train_loss=2.4679794311523438\n",
      "Epoch=503, iteration=3, train_loss=2.6234774589538574\n",
      "Epoch=503, val_loss=2.618472099304199\n",
      "Epoch=504, iteration=0, train_loss=2.424870729446411\n",
      "Epoch=504, iteration=1, train_loss=2.4295945167541504\n",
      "Epoch=504, iteration=2, train_loss=2.467822790145874\n",
      "Epoch=504, iteration=3, train_loss=2.6233150959014893\n",
      "Epoch=504, val_loss=2.618328094482422\n",
      "Epoch=505, iteration=0, train_loss=2.4246737957000732\n",
      "Epoch=505, iteration=1, train_loss=2.4294328689575195\n",
      "Epoch=505, iteration=2, train_loss=2.4676668643951416\n",
      "Epoch=505, iteration=3, train_loss=2.6231534481048584\n",
      "Epoch=505, val_loss=2.618184804916382\n",
      "Epoch=506, iteration=0, train_loss=2.4244773387908936\n",
      "Epoch=506, iteration=1, train_loss=2.4292712211608887\n",
      "Epoch=506, iteration=2, train_loss=2.467510938644409\n",
      "Epoch=506, iteration=3, train_loss=2.6229922771453857\n",
      "Epoch=506, val_loss=2.618042230606079\n",
      "Epoch=507, iteration=0, train_loss=2.424281358718872\n",
      "Epoch=507, iteration=1, train_loss=2.429110527038574\n",
      "Epoch=507, iteration=2, train_loss=2.4673562049865723\n",
      "Epoch=507, iteration=3, train_loss=2.622831106185913\n",
      "Epoch=507, val_loss=2.6178994178771973\n",
      "Epoch=508, iteration=0, train_loss=2.424085855484009\n",
      "Epoch=508, iteration=1, train_loss=2.428950071334839\n",
      "Epoch=508, iteration=2, train_loss=2.467200994491577\n",
      "Epoch=508, iteration=3, train_loss=2.622671365737915\n",
      "Epoch=508, val_loss=2.617757558822632\n",
      "Epoch=509, iteration=0, train_loss=2.423891544342041\n",
      "Epoch=509, iteration=1, train_loss=2.4287896156311035\n",
      "Epoch=509, iteration=2, train_loss=2.4670469760894775\n",
      "Epoch=509, iteration=3, train_loss=2.622511148452759\n",
      "Epoch=509, val_loss=2.6176161766052246\n",
      "Epoch=510, iteration=0, train_loss=2.423696994781494\n",
      "Epoch=510, iteration=1, train_loss=2.4286296367645264\n",
      "Epoch=510, iteration=2, train_loss=2.466892719268799\n",
      "Epoch=510, iteration=3, train_loss=2.622352361679077\n",
      "Epoch=510, val_loss=2.6174752712249756\n",
      "Epoch=511, iteration=0, train_loss=2.4235033988952637\n",
      "Epoch=511, iteration=1, train_loss=2.4284703731536865\n",
      "Epoch=511, iteration=2, train_loss=2.4667391777038574\n",
      "Epoch=511, iteration=3, train_loss=2.6221933364868164\n",
      "Epoch=511, val_loss=2.6173348426818848\n",
      "Epoch=512, iteration=0, train_loss=2.4233100414276123\n",
      "Epoch=512, iteration=1, train_loss=2.428311347961426\n",
      "Epoch=512, iteration=2, train_loss=2.466586112976074\n",
      "Epoch=512, iteration=3, train_loss=2.622035264968872\n",
      "Epoch=512, val_loss=2.6171951293945312\n",
      "Epoch=513, iteration=0, train_loss=2.4231173992156982\n",
      "Epoch=513, iteration=1, train_loss=2.428152322769165\n",
      "Epoch=513, iteration=2, train_loss=2.466433048248291\n",
      "Epoch=513, iteration=3, train_loss=2.621877908706665\n",
      "Epoch=513, val_loss=2.617055892944336\n",
      "Epoch=514, iteration=0, train_loss=2.4229252338409424\n",
      "Epoch=514, iteration=1, train_loss=2.4279942512512207\n",
      "Epoch=514, iteration=2, train_loss=2.466280937194824\n",
      "Epoch=514, iteration=3, train_loss=2.621720552444458\n",
      "Epoch=514, val_loss=2.6169168949127197\n",
      "Epoch=515, iteration=0, train_loss=2.4227333068847656\n",
      "Epoch=515, iteration=1, train_loss=2.4278361797332764\n",
      "Epoch=515, iteration=2, train_loss=2.4661288261413574\n",
      "Epoch=515, iteration=3, train_loss=2.621563673019409\n",
      "Epoch=515, val_loss=2.6167783737182617\n",
      "Epoch=516, iteration=0, train_loss=2.4225423336029053\n",
      "Epoch=516, iteration=1, train_loss=2.427678108215332\n",
      "Epoch=516, iteration=2, train_loss=2.465977430343628\n",
      "Epoch=516, iteration=3, train_loss=2.6214075088500977\n",
      "Epoch=516, val_loss=2.616640567779541\n",
      "Epoch=517, iteration=0, train_loss=2.422351598739624\n",
      "Epoch=517, iteration=1, train_loss=2.427520990371704\n",
      "Epoch=517, iteration=2, train_loss=2.4658265113830566\n",
      "Epoch=517, iteration=3, train_loss=2.621251344680786\n",
      "Epoch=517, val_loss=2.6165032386779785\n",
      "Epoch=518, iteration=0, train_loss=2.4221608638763428\n",
      "Epoch=518, iteration=1, train_loss=2.427363634109497\n",
      "Epoch=518, iteration=2, train_loss=2.4656755924224854\n",
      "Epoch=518, iteration=3, train_loss=2.621096134185791\n",
      "Epoch=518, val_loss=2.616365909576416\n",
      "Epoch=519, iteration=0, train_loss=2.421971321105957\n",
      "Epoch=519, iteration=1, train_loss=2.4272069931030273\n",
      "Epoch=519, iteration=2, train_loss=2.4655253887176514\n",
      "Epoch=519, iteration=3, train_loss=2.620941162109375\n",
      "Epoch=519, val_loss=2.61622953414917\n",
      "Epoch=520, iteration=0, train_loss=2.4217822551727295\n",
      "Epoch=520, iteration=1, train_loss=2.427050828933716\n",
      "Epoch=520, iteration=2, train_loss=2.4653751850128174\n",
      "Epoch=520, iteration=3, train_loss=2.6207869052886963\n",
      "Epoch=520, val_loss=2.616093397140503\n",
      "Epoch=521, iteration=0, train_loss=2.421593427658081\n",
      "Epoch=521, iteration=1, train_loss=2.4268946647644043\n",
      "Epoch=521, iteration=2, train_loss=2.4652256965637207\n",
      "Epoch=521, iteration=3, train_loss=2.6206328868865967\n",
      "Epoch=521, val_loss=2.6159579753875732\n",
      "Epoch=522, iteration=0, train_loss=2.421405076980591\n",
      "Epoch=522, iteration=1, train_loss=2.42673921585083\n",
      "Epoch=522, iteration=2, train_loss=2.4650769233703613\n",
      "Epoch=522, iteration=3, train_loss=2.6204795837402344\n",
      "Epoch=522, val_loss=2.6158227920532227\n",
      "Epoch=523, iteration=0, train_loss=2.421217441558838\n",
      "Epoch=523, iteration=1, train_loss=2.426583766937256\n",
      "Epoch=523, iteration=2, train_loss=2.464927911758423\n",
      "Epoch=523, iteration=3, train_loss=2.620326519012451\n",
      "Epoch=523, val_loss=2.6156883239746094\n",
      "Epoch=524, iteration=0, train_loss=2.421029806137085\n",
      "Epoch=524, iteration=1, train_loss=2.426429033279419\n",
      "Epoch=524, iteration=2, train_loss=2.4647793769836426\n",
      "Epoch=524, iteration=3, train_loss=2.6201741695404053\n",
      "Epoch=524, val_loss=2.615553855895996\n",
      "Epoch=525, iteration=0, train_loss=2.4208431243896484\n",
      "Epoch=525, iteration=1, train_loss=2.426274299621582\n",
      "Epoch=525, iteration=2, train_loss=2.4646317958831787\n",
      "Epoch=525, iteration=3, train_loss=2.6200222969055176\n",
      "Epoch=525, val_loss=2.61542010307312\n",
      "Epoch=526, iteration=0, train_loss=2.420656681060791\n",
      "Epoch=526, iteration=1, train_loss=2.426119804382324\n",
      "Epoch=526, iteration=2, train_loss=2.4644839763641357\n",
      "Epoch=526, iteration=3, train_loss=2.619870662689209\n",
      "Epoch=526, val_loss=2.6152870655059814\n",
      "Epoch=527, iteration=0, train_loss=2.420470952987671\n",
      "Epoch=527, iteration=1, train_loss=2.4259655475616455\n",
      "Epoch=527, iteration=2, train_loss=2.464336395263672\n",
      "Epoch=527, iteration=3, train_loss=2.6197195053100586\n",
      "Epoch=527, val_loss=2.6151540279388428\n",
      "Epoch=528, iteration=0, train_loss=2.420285701751709\n",
      "Epoch=528, iteration=1, train_loss=2.425812005996704\n",
      "Epoch=528, iteration=2, train_loss=2.4641900062561035\n",
      "Epoch=528, iteration=3, train_loss=2.6195693016052246\n",
      "Epoch=528, val_loss=2.6150214672088623\n",
      "Epoch=529, iteration=0, train_loss=2.4201009273529053\n",
      "Epoch=529, iteration=1, train_loss=2.425658702850342\n",
      "Epoch=529, iteration=2, train_loss=2.464043140411377\n",
      "Epoch=529, iteration=3, train_loss=2.6194190979003906\n",
      "Epoch=529, val_loss=2.614889621734619\n",
      "Epoch=530, iteration=0, train_loss=2.4199163913726807\n",
      "Epoch=530, iteration=1, train_loss=2.4255056381225586\n",
      "Epoch=530, iteration=2, train_loss=2.463897228240967\n",
      "Epoch=530, iteration=3, train_loss=2.619269609451294\n",
      "Epoch=530, val_loss=2.614758253097534\n",
      "Epoch=531, iteration=0, train_loss=2.419732093811035\n",
      "Epoch=531, iteration=1, train_loss=2.4253528118133545\n",
      "Epoch=531, iteration=2, train_loss=2.4637510776519775\n",
      "Epoch=531, iteration=3, train_loss=2.6191201210021973\n",
      "Epoch=531, val_loss=2.6146271228790283\n",
      "Epoch=532, iteration=0, train_loss=2.419548749923706\n",
      "Epoch=532, iteration=1, train_loss=2.4252002239227295\n",
      "Epoch=532, iteration=2, train_loss=2.4636058807373047\n",
      "Epoch=532, iteration=3, train_loss=2.618971347808838\n",
      "Epoch=532, val_loss=2.6144967079162598\n",
      "Epoch=533, iteration=0, train_loss=2.419365644454956\n",
      "Epoch=533, iteration=1, train_loss=2.425048351287842\n",
      "Epoch=533, iteration=2, train_loss=2.463460683822632\n",
      "Epoch=533, iteration=3, train_loss=2.6188230514526367\n",
      "Epoch=533, val_loss=2.6143665313720703\n",
      "Epoch=534, iteration=0, train_loss=2.419182777404785\n",
      "Epoch=534, iteration=1, train_loss=2.424896240234375\n",
      "Epoch=534, iteration=2, train_loss=2.463315725326538\n",
      "Epoch=534, iteration=3, train_loss=2.6186749935150146\n",
      "Epoch=534, val_loss=2.614237070083618\n",
      "Epoch=535, iteration=0, train_loss=2.4190008640289307\n",
      "Epoch=535, iteration=1, train_loss=2.4247448444366455\n",
      "Epoch=535, iteration=2, train_loss=2.4631714820861816\n",
      "Epoch=535, iteration=3, train_loss=2.61852765083313\n",
      "Epoch=535, val_loss=2.614107608795166\n",
      "Epoch=536, iteration=0, train_loss=2.418818712234497\n",
      "Epoch=536, iteration=1, train_loss=2.424593687057495\n",
      "Epoch=536, iteration=2, train_loss=2.463027238845825\n",
      "Epoch=536, iteration=3, train_loss=2.618380546569824\n",
      "Epoch=536, val_loss=2.613978624343872\n",
      "Epoch=537, iteration=0, train_loss=2.41863751411438\n",
      "Epoch=537, iteration=1, train_loss=2.4244425296783447\n",
      "Epoch=537, iteration=2, train_loss=2.462883710861206\n",
      "Epoch=537, iteration=3, train_loss=2.6182339191436768\n",
      "Epoch=537, val_loss=2.6138503551483154\n",
      "Epoch=538, iteration=0, train_loss=2.418456554412842\n",
      "Epoch=538, iteration=1, train_loss=2.4242918491363525\n",
      "Epoch=538, iteration=2, train_loss=2.462740659713745\n",
      "Epoch=538, iteration=3, train_loss=2.6180880069732666\n",
      "Epoch=538, val_loss=2.613722562789917\n",
      "Epoch=539, iteration=0, train_loss=2.418276071548462\n",
      "Epoch=539, iteration=1, train_loss=2.4241416454315186\n",
      "Epoch=539, iteration=2, train_loss=2.462597608566284\n",
      "Epoch=539, iteration=3, train_loss=2.6179423332214355\n",
      "Epoch=539, val_loss=2.6135945320129395\n",
      "Epoch=540, iteration=0, train_loss=2.4180963039398193\n",
      "Epoch=540, iteration=1, train_loss=2.4239914417266846\n",
      "Epoch=540, iteration=2, train_loss=2.4624547958374023\n",
      "Epoch=540, iteration=3, train_loss=2.6177971363067627\n",
      "Epoch=540, val_loss=2.6134674549102783\n",
      "Epoch=541, iteration=0, train_loss=2.4179165363311768\n",
      "Epoch=541, iteration=1, train_loss=2.423841714859009\n",
      "Epoch=541, iteration=2, train_loss=2.462312936782837\n",
      "Epoch=541, iteration=3, train_loss=2.617652177810669\n",
      "Epoch=541, val_loss=2.6133406162261963\n",
      "Epoch=542, iteration=0, train_loss=2.4177372455596924\n",
      "Epoch=542, iteration=1, train_loss=2.423692226409912\n",
      "Epoch=542, iteration=2, train_loss=2.4621708393096924\n",
      "Epoch=542, iteration=3, train_loss=2.6175079345703125\n",
      "Epoch=542, val_loss=2.6132144927978516\n",
      "Epoch=543, iteration=0, train_loss=2.4175586700439453\n",
      "Epoch=543, iteration=1, train_loss=2.4235432147979736\n",
      "Epoch=543, iteration=2, train_loss=2.462029218673706\n",
      "Epoch=543, iteration=3, train_loss=2.617363929748535\n",
      "Epoch=543, val_loss=2.613088607788086\n",
      "Epoch=544, iteration=0, train_loss=2.4173805713653564\n",
      "Epoch=544, iteration=1, train_loss=2.4233944416046143\n",
      "Epoch=544, iteration=2, train_loss=2.461887836456299\n",
      "Epoch=544, iteration=3, train_loss=2.617220640182495\n",
      "Epoch=544, val_loss=2.6129629611968994\n",
      "Epoch=545, iteration=0, train_loss=2.4172027111053467\n",
      "Epoch=545, iteration=1, train_loss=2.423245668411255\n",
      "Epoch=545, iteration=2, train_loss=2.46174693107605\n",
      "Epoch=545, iteration=3, train_loss=2.617077350616455\n",
      "Epoch=545, val_loss=2.61283802986145\n",
      "Epoch=546, iteration=0, train_loss=2.417024850845337\n",
      "Epoch=546, iteration=1, train_loss=2.4230973720550537\n",
      "Epoch=546, iteration=2, train_loss=2.46160626411438\n",
      "Epoch=546, iteration=3, train_loss=2.6169345378875732\n",
      "Epoch=546, val_loss=2.61271333694458\n",
      "Epoch=547, iteration=0, train_loss=2.4168477058410645\n",
      "Epoch=547, iteration=1, train_loss=2.4229488372802734\n",
      "Epoch=547, iteration=2, train_loss=2.461465835571289\n",
      "Epoch=547, iteration=3, train_loss=2.6167924404144287\n",
      "Epoch=547, val_loss=2.612588882446289\n",
      "Epoch=548, iteration=0, train_loss=2.416670799255371\n",
      "Epoch=548, iteration=1, train_loss=2.4228012561798096\n",
      "Epoch=548, iteration=2, train_loss=2.4613258838653564\n",
      "Epoch=548, iteration=3, train_loss=2.6166505813598633\n",
      "Epoch=548, val_loss=2.6124651432037354\n",
      "Epoch=549, iteration=0, train_loss=2.416494607925415\n",
      "Epoch=549, iteration=1, train_loss=2.4226536750793457\n",
      "Epoch=549, iteration=2, train_loss=2.461186408996582\n",
      "Epoch=549, iteration=3, train_loss=2.616509199142456\n",
      "Epoch=549, val_loss=2.6123416423797607\n",
      "Epoch=550, iteration=0, train_loss=2.416318893432617\n",
      "Epoch=550, iteration=1, train_loss=2.42250657081604\n",
      "Epoch=550, iteration=2, train_loss=2.4610471725463867\n",
      "Epoch=550, iteration=3, train_loss=2.616368293762207\n",
      "Epoch=550, val_loss=2.6122183799743652\n",
      "Epoch=551, iteration=0, train_loss=2.4161431789398193\n",
      "Epoch=551, iteration=1, train_loss=2.4223594665527344\n",
      "Epoch=551, iteration=2, train_loss=2.4609079360961914\n",
      "Epoch=551, iteration=3, train_loss=2.616227626800537\n",
      "Epoch=551, val_loss=2.612095594406128\n",
      "Epoch=552, iteration=0, train_loss=2.4159677028656006\n",
      "Epoch=552, iteration=1, train_loss=2.4222123622894287\n",
      "Epoch=552, iteration=2, train_loss=2.4607696533203125\n",
      "Epoch=552, iteration=3, train_loss=2.6160871982574463\n",
      "Epoch=552, val_loss=2.611973524093628\n",
      "Epoch=553, iteration=0, train_loss=2.4157931804656982\n",
      "Epoch=553, iteration=1, train_loss=2.4220662117004395\n",
      "Epoch=553, iteration=2, train_loss=2.460630416870117\n",
      "Epoch=553, iteration=3, train_loss=2.615947723388672\n",
      "Epoch=553, val_loss=2.611851692199707\n",
      "Epoch=554, iteration=0, train_loss=2.415618896484375\n",
      "Epoch=554, iteration=1, train_loss=2.421919822692871\n",
      "Epoch=554, iteration=2, train_loss=2.4604926109313965\n",
      "Epoch=554, iteration=3, train_loss=2.6158082485198975\n",
      "Epoch=554, val_loss=2.6117300987243652\n",
      "Epoch=555, iteration=0, train_loss=2.415444850921631\n",
      "Epoch=555, iteration=1, train_loss=2.421773910522461\n",
      "Epoch=555, iteration=2, train_loss=2.460354804992676\n",
      "Epoch=555, iteration=3, train_loss=2.6156692504882812\n",
      "Epoch=555, val_loss=2.6116087436676025\n",
      "Epoch=556, iteration=0, train_loss=2.415271043777466\n",
      "Epoch=556, iteration=1, train_loss=2.421627998352051\n",
      "Epoch=556, iteration=2, train_loss=2.4602174758911133\n",
      "Epoch=556, iteration=3, train_loss=2.6155307292938232\n",
      "Epoch=556, val_loss=2.611488103866577\n",
      "Epoch=557, iteration=0, train_loss=2.415097951889038\n",
      "Epoch=557, iteration=1, train_loss=2.421482563018799\n",
      "Epoch=557, iteration=2, train_loss=2.460080146789551\n",
      "Epoch=557, iteration=3, train_loss=2.6153924465179443\n",
      "Epoch=557, val_loss=2.61136794090271\n",
      "Epoch=558, iteration=0, train_loss=2.4149248600006104\n",
      "Epoch=558, iteration=1, train_loss=2.421337127685547\n",
      "Epoch=558, iteration=2, train_loss=2.4599432945251465\n",
      "Epoch=558, iteration=3, train_loss=2.6152546405792236\n",
      "Epoch=558, val_loss=2.6112477779388428\n",
      "Epoch=559, iteration=0, train_loss=2.414752244949341\n",
      "Epoch=559, iteration=1, train_loss=2.421191930770874\n",
      "Epoch=559, iteration=2, train_loss=2.4598066806793213\n",
      "Epoch=559, iteration=3, train_loss=2.615117311477661\n",
      "Epoch=559, val_loss=2.6111278533935547\n",
      "Epoch=560, iteration=0, train_loss=2.4145803451538086\n",
      "Epoch=560, iteration=1, train_loss=2.4210472106933594\n",
      "Epoch=560, iteration=2, train_loss=2.459670066833496\n",
      "Epoch=560, iteration=3, train_loss=2.614980459213257\n",
      "Epoch=560, val_loss=2.611008882522583\n",
      "Epoch=561, iteration=0, train_loss=2.4144084453582764\n",
      "Epoch=561, iteration=1, train_loss=2.420902729034424\n",
      "Epoch=561, iteration=2, train_loss=2.459533929824829\n",
      "Epoch=561, iteration=3, train_loss=2.6148438453674316\n",
      "Epoch=561, val_loss=2.6108899116516113\n",
      "Epoch=562, iteration=0, train_loss=2.4142370223999023\n",
      "Epoch=562, iteration=1, train_loss=2.4207582473754883\n",
      "Epoch=562, iteration=2, train_loss=2.459398031234741\n",
      "Epoch=562, iteration=3, train_loss=2.6147074699401855\n",
      "Epoch=562, val_loss=2.610771417617798\n",
      "Epoch=563, iteration=0, train_loss=2.4140660762786865\n",
      "Epoch=563, iteration=1, train_loss=2.420614719390869\n",
      "Epoch=563, iteration=2, train_loss=2.4592628479003906\n",
      "Epoch=563, iteration=3, train_loss=2.6145718097686768\n",
      "Epoch=563, val_loss=2.6106534004211426\n",
      "Epoch=564, iteration=0, train_loss=2.4138948917388916\n",
      "Epoch=564, iteration=1, train_loss=2.4204704761505127\n",
      "Epoch=564, iteration=2, train_loss=2.459127426147461\n",
      "Epoch=564, iteration=3, train_loss=2.614436149597168\n",
      "Epoch=564, val_loss=2.6105356216430664\n",
      "Epoch=565, iteration=0, train_loss=2.413724422454834\n",
      "Epoch=565, iteration=1, train_loss=2.4203267097473145\n",
      "Epoch=565, iteration=2, train_loss=2.4589927196502686\n",
      "Epoch=565, iteration=3, train_loss=2.6143014430999756\n",
      "Epoch=565, val_loss=2.6104180812835693\n",
      "Epoch=566, iteration=0, train_loss=2.4135544300079346\n",
      "Epoch=566, iteration=1, train_loss=2.4201834201812744\n",
      "Epoch=566, iteration=2, train_loss=2.458857774734497\n",
      "Epoch=566, iteration=3, train_loss=2.6141669750213623\n",
      "Epoch=566, val_loss=2.6103012561798096\n",
      "Epoch=567, iteration=0, train_loss=2.413384437561035\n",
      "Epoch=567, iteration=1, train_loss=2.4200401306152344\n",
      "Epoch=567, iteration=2, train_loss=2.458723545074463\n",
      "Epoch=567, iteration=3, train_loss=2.614032745361328\n",
      "Epoch=567, val_loss=2.61018443107605\n",
      "Epoch=568, iteration=0, train_loss=2.413215398788452\n",
      "Epoch=568, iteration=1, train_loss=2.4198968410491943\n",
      "Epoch=568, iteration=2, train_loss=2.4585893154144287\n",
      "Epoch=568, iteration=3, train_loss=2.613898277282715\n",
      "Epoch=568, val_loss=2.6100680828094482\n",
      "Epoch=569, iteration=0, train_loss=2.413046360015869\n",
      "Epoch=569, iteration=1, train_loss=2.4197540283203125\n",
      "Epoch=569, iteration=2, train_loss=2.458455801010132\n",
      "Epoch=569, iteration=3, train_loss=2.613765001296997\n",
      "Epoch=569, val_loss=2.6099517345428467\n",
      "Epoch=570, iteration=0, train_loss=2.412877082824707\n",
      "Epoch=570, iteration=1, train_loss=2.419611692428589\n",
      "Epoch=570, iteration=2, train_loss=2.458322048187256\n",
      "Epoch=570, iteration=3, train_loss=2.6136317253112793\n",
      "Epoch=570, val_loss=2.6098361015319824\n",
      "Epoch=571, iteration=0, train_loss=2.4127087593078613\n",
      "Epoch=571, iteration=1, train_loss=2.419469118118286\n",
      "Epoch=571, iteration=2, train_loss=2.458188056945801\n",
      "Epoch=571, iteration=3, train_loss=2.6134986877441406\n",
      "Epoch=571, val_loss=2.6097209453582764\n",
      "Epoch=572, iteration=0, train_loss=2.4125404357910156\n",
      "Epoch=572, iteration=1, train_loss=2.4193270206451416\n",
      "Epoch=572, iteration=2, train_loss=2.4580554962158203\n",
      "Epoch=572, iteration=3, train_loss=2.6133663654327393\n",
      "Epoch=572, val_loss=2.6096060276031494\n",
      "Epoch=573, iteration=0, train_loss=2.4123728275299072\n",
      "Epoch=573, iteration=1, train_loss=2.419184923171997\n",
      "Epoch=573, iteration=2, train_loss=2.4579224586486816\n",
      "Epoch=573, iteration=3, train_loss=2.613234519958496\n",
      "Epoch=573, val_loss=2.6094913482666016\n",
      "Epoch=574, iteration=0, train_loss=2.412205696105957\n",
      "Epoch=574, iteration=1, train_loss=2.4190428256988525\n",
      "Epoch=574, iteration=2, train_loss=2.457789897918701\n",
      "Epoch=574, iteration=3, train_loss=2.613102674484253\n",
      "Epoch=574, val_loss=2.609376907348633\n",
      "Epoch=575, iteration=0, train_loss=2.4120380878448486\n",
      "Epoch=575, iteration=1, train_loss=2.4189016819000244\n",
      "Epoch=575, iteration=2, train_loss=2.4576575756073\n",
      "Epoch=575, iteration=3, train_loss=2.612971305847168\n",
      "Epoch=575, val_loss=2.6092629432678223\n",
      "Epoch=576, iteration=0, train_loss=2.4118711948394775\n",
      "Epoch=576, iteration=1, train_loss=2.418760299682617\n",
      "Epoch=576, iteration=2, train_loss=2.4575250148773193\n",
      "Epoch=576, iteration=3, train_loss=2.612840414047241\n",
      "Epoch=576, val_loss=2.609149217605591\n",
      "Epoch=577, iteration=0, train_loss=2.4117045402526855\n",
      "Epoch=577, iteration=1, train_loss=2.41861891746521\n",
      "Epoch=577, iteration=2, train_loss=2.4573934078216553\n",
      "Epoch=577, iteration=3, train_loss=2.6127095222473145\n",
      "Epoch=577, val_loss=2.6090357303619385\n",
      "Epoch=578, iteration=0, train_loss=2.4115381240844727\n",
      "Epoch=578, iteration=1, train_loss=2.41847825050354\n",
      "Epoch=578, iteration=2, train_loss=2.457261800765991\n",
      "Epoch=578, iteration=3, train_loss=2.612579107284546\n",
      "Epoch=578, val_loss=2.6089231967926025\n",
      "Epoch=579, iteration=0, train_loss=2.411372184753418\n",
      "Epoch=579, iteration=1, train_loss=2.418337106704712\n",
      "Epoch=579, iteration=2, train_loss=2.4571304321289062\n",
      "Epoch=579, iteration=3, train_loss=2.6124491691589355\n",
      "Epoch=579, val_loss=2.6088104248046875\n",
      "Epoch=580, iteration=0, train_loss=2.4112062454223633\n",
      "Epoch=580, iteration=1, train_loss=2.418196439743042\n",
      "Epoch=580, iteration=2, train_loss=2.4569993019104004\n",
      "Epoch=580, iteration=3, train_loss=2.6123194694519043\n",
      "Epoch=580, val_loss=2.6086978912353516\n",
      "Epoch=581, iteration=0, train_loss=2.411041259765625\n",
      "Epoch=581, iteration=1, train_loss=2.418055772781372\n",
      "Epoch=581, iteration=2, train_loss=2.4568681716918945\n",
      "Epoch=581, iteration=3, train_loss=2.6121902465820312\n",
      "Epoch=581, val_loss=2.608585834503174\n",
      "Epoch=582, iteration=0, train_loss=2.4108757972717285\n",
      "Epoch=582, iteration=1, train_loss=2.4179158210754395\n",
      "Epoch=582, iteration=2, train_loss=2.456737518310547\n",
      "Epoch=582, iteration=3, train_loss=2.6120615005493164\n",
      "Epoch=582, val_loss=2.6084742546081543\n",
      "Epoch=583, iteration=0, train_loss=2.4107108116149902\n",
      "Epoch=583, iteration=1, train_loss=2.4177756309509277\n",
      "Epoch=583, iteration=2, train_loss=2.456606864929199\n",
      "Epoch=583, iteration=3, train_loss=2.6119327545166016\n",
      "Epoch=583, val_loss=2.6083626747131348\n",
      "Epoch=584, iteration=0, train_loss=2.410545587539673\n",
      "Epoch=584, iteration=1, train_loss=2.417635917663574\n",
      "Epoch=584, iteration=2, train_loss=2.4564766883850098\n",
      "Epoch=584, iteration=3, train_loss=2.611804485321045\n",
      "Epoch=584, val_loss=2.6082520484924316\n",
      "Epoch=585, iteration=0, train_loss=2.41038179397583\n",
      "Epoch=585, iteration=1, train_loss=2.4174959659576416\n",
      "Epoch=585, iteration=2, train_loss=2.4563469886779785\n",
      "Epoch=585, iteration=3, train_loss=2.6116764545440674\n",
      "Epoch=585, val_loss=2.6081411838531494\n",
      "Epoch=586, iteration=0, train_loss=2.410217761993408\n",
      "Epoch=586, iteration=1, train_loss=2.417356252670288\n",
      "Epoch=586, iteration=2, train_loss=2.456216812133789\n",
      "Epoch=586, iteration=3, train_loss=2.6115493774414062\n",
      "Epoch=586, val_loss=2.6080307960510254\n",
      "Epoch=587, iteration=0, train_loss=2.4100537300109863\n",
      "Epoch=587, iteration=1, train_loss=2.4172167778015137\n",
      "Epoch=587, iteration=2, train_loss=2.456087112426758\n",
      "Epoch=587, iteration=3, train_loss=2.611421823501587\n",
      "Epoch=587, val_loss=2.6079204082489014\n",
      "Epoch=588, iteration=0, train_loss=2.4098901748657227\n",
      "Epoch=588, iteration=1, train_loss=2.4170775413513184\n",
      "Epoch=588, iteration=2, train_loss=2.4559578895568848\n",
      "Epoch=588, iteration=3, train_loss=2.611294984817505\n",
      "Epoch=588, val_loss=2.6078109741210938\n",
      "Epoch=589, iteration=0, train_loss=2.409726858139038\n",
      "Epoch=589, iteration=1, train_loss=2.416938543319702\n",
      "Epoch=589, iteration=2, train_loss=2.455828905105591\n",
      "Epoch=589, iteration=3, train_loss=2.6111679077148438\n",
      "Epoch=589, val_loss=2.607701301574707\n",
      "Epoch=590, iteration=0, train_loss=2.4095637798309326\n",
      "Epoch=590, iteration=1, train_loss=2.416799306869507\n",
      "Epoch=590, iteration=2, train_loss=2.4556996822357178\n",
      "Epoch=590, iteration=3, train_loss=2.6110422611236572\n",
      "Epoch=590, val_loss=2.6075921058654785\n",
      "Epoch=591, iteration=0, train_loss=2.4094011783599854\n",
      "Epoch=591, iteration=1, train_loss=2.4166603088378906\n",
      "Epoch=591, iteration=2, train_loss=2.455570936203003\n",
      "Epoch=591, iteration=3, train_loss=2.6109161376953125\n",
      "Epoch=591, val_loss=2.60748291015625\n",
      "Epoch=592, iteration=0, train_loss=2.409238338470459\n",
      "Epoch=592, iteration=1, train_loss=2.4165217876434326\n",
      "Epoch=592, iteration=2, train_loss=2.455442190170288\n",
      "Epoch=592, iteration=3, train_loss=2.610790729522705\n",
      "Epoch=592, val_loss=2.6073741912841797\n",
      "Epoch=593, iteration=0, train_loss=2.409075975418091\n",
      "Epoch=593, iteration=1, train_loss=2.4163832664489746\n",
      "Epoch=593, iteration=2, train_loss=2.4553139209747314\n",
      "Epoch=593, iteration=3, train_loss=2.6106650829315186\n",
      "Epoch=593, val_loss=2.6072657108306885\n",
      "Epoch=594, iteration=0, train_loss=2.4089138507843018\n",
      "Epoch=594, iteration=1, train_loss=2.4162447452545166\n",
      "Epoch=594, iteration=2, train_loss=2.4551854133605957\n",
      "Epoch=594, iteration=3, train_loss=2.6105401515960693\n",
      "Epoch=594, val_loss=2.6071579456329346\n",
      "Epoch=595, iteration=0, train_loss=2.408752202987671\n",
      "Epoch=595, iteration=1, train_loss=2.416106939315796\n",
      "Epoch=595, iteration=2, train_loss=2.4550578594207764\n",
      "Epoch=595, iteration=3, train_loss=2.610415458679199\n",
      "Epoch=595, val_loss=2.6070501804351807\n",
      "Epoch=596, iteration=0, train_loss=2.40859055519104\n",
      "Epoch=596, iteration=1, train_loss=2.415968418121338\n",
      "Epoch=596, iteration=2, train_loss=2.454930067062378\n",
      "Epoch=596, iteration=3, train_loss=2.610291004180908\n",
      "Epoch=596, val_loss=2.606942892074585\n",
      "Epoch=597, iteration=0, train_loss=2.4084291458129883\n",
      "Epoch=597, iteration=1, train_loss=2.4158308506011963\n",
      "Epoch=597, iteration=2, train_loss=2.4548025131225586\n",
      "Epoch=597, iteration=3, train_loss=2.6101670265197754\n",
      "Epoch=597, val_loss=2.60683536529541\n",
      "Epoch=598, iteration=0, train_loss=2.4082679748535156\n",
      "Epoch=598, iteration=1, train_loss=2.4156928062438965\n",
      "Epoch=598, iteration=2, train_loss=2.4546749591827393\n",
      "Epoch=598, iteration=3, train_loss=2.6100430488586426\n",
      "Epoch=598, val_loss=2.6067285537719727\n",
      "Epoch=599, iteration=0, train_loss=2.408107042312622\n",
      "Epoch=599, iteration=1, train_loss=2.415555238723755\n",
      "Epoch=599, iteration=2, train_loss=2.454547643661499\n",
      "Epoch=599, iteration=3, train_loss=2.6099202632904053\n",
      "Epoch=599, val_loss=2.6066219806671143\n",
      "Epoch=600, iteration=0, train_loss=2.4079463481903076\n",
      "Epoch=600, iteration=1, train_loss=2.4154176712036133\n",
      "Epoch=600, iteration=2, train_loss=2.454420566558838\n",
      "Epoch=600, iteration=3, train_loss=2.6097967624664307\n",
      "Epoch=600, val_loss=2.606515407562256\n",
      "Epoch=601, iteration=0, train_loss=2.407785415649414\n",
      "Epoch=601, iteration=1, train_loss=2.415280342102051\n",
      "Epoch=601, iteration=2, train_loss=2.4542934894561768\n",
      "Epoch=601, iteration=3, train_loss=2.6096742153167725\n",
      "Epoch=601, val_loss=2.6064095497131348\n",
      "Epoch=602, iteration=0, train_loss=2.407625198364258\n",
      "Epoch=602, iteration=1, train_loss=2.4151430130004883\n",
      "Epoch=602, iteration=2, train_loss=2.454166889190674\n",
      "Epoch=602, iteration=3, train_loss=2.609551429748535\n",
      "Epoch=602, val_loss=2.6063036918640137\n",
      "Epoch=603, iteration=0, train_loss=2.4074652194976807\n",
      "Epoch=603, iteration=1, train_loss=2.415005922317505\n",
      "Epoch=603, iteration=2, train_loss=2.454040765762329\n",
      "Epoch=603, iteration=3, train_loss=2.609428882598877\n",
      "Epoch=603, val_loss=2.6061980724334717\n",
      "Epoch=604, iteration=0, train_loss=2.4073054790496826\n",
      "Epoch=604, iteration=1, train_loss=2.4148688316345215\n",
      "Epoch=604, iteration=2, train_loss=2.453914165496826\n",
      "Epoch=604, iteration=3, train_loss=2.609306812286377\n",
      "Epoch=604, val_loss=2.606092929840088\n",
      "Epoch=605, iteration=0, train_loss=2.4071462154388428\n",
      "Epoch=605, iteration=1, train_loss=2.4147322177886963\n",
      "Epoch=605, iteration=2, train_loss=2.4537880420684814\n",
      "Epoch=605, iteration=3, train_loss=2.6091856956481934\n",
      "Epoch=605, val_loss=2.605987787246704\n",
      "Epoch=606, iteration=0, train_loss=2.4069864749908447\n",
      "Epoch=606, iteration=1, train_loss=2.414594888687134\n",
      "Epoch=606, iteration=2, train_loss=2.453662157058716\n",
      "Epoch=606, iteration=3, train_loss=2.6090638637542725\n",
      "Epoch=606, val_loss=2.6058828830718994\n",
      "Epoch=607, iteration=0, train_loss=2.406827211380005\n",
      "Epoch=607, iteration=1, train_loss=2.4144582748413086\n",
      "Epoch=607, iteration=2, train_loss=2.45353627204895\n",
      "Epoch=607, iteration=3, train_loss=2.608942747116089\n",
      "Epoch=607, val_loss=2.605778694152832\n",
      "Epoch=608, iteration=0, train_loss=2.406668186187744\n",
      "Epoch=608, iteration=1, train_loss=2.4143216609954834\n",
      "Epoch=608, iteration=2, train_loss=2.4534108638763428\n",
      "Epoch=608, iteration=3, train_loss=2.6088221073150635\n",
      "Epoch=608, val_loss=2.6056745052337646\n",
      "Epoch=609, iteration=0, train_loss=2.4065093994140625\n",
      "Epoch=609, iteration=1, train_loss=2.4141855239868164\n",
      "Epoch=609, iteration=2, train_loss=2.4532852172851562\n",
      "Epoch=609, iteration=3, train_loss=2.608701705932617\n",
      "Epoch=609, val_loss=2.6055705547332764\n",
      "Epoch=610, iteration=0, train_loss=2.406350612640381\n",
      "Epoch=610, iteration=1, train_loss=2.4140491485595703\n",
      "Epoch=610, iteration=2, train_loss=2.453160047531128\n",
      "Epoch=610, iteration=3, train_loss=2.608581304550171\n",
      "Epoch=610, val_loss=2.605466604232788\n",
      "Epoch=611, iteration=0, train_loss=2.4061920642852783\n",
      "Epoch=611, iteration=1, train_loss=2.413912773132324\n",
      "Epoch=611, iteration=2, train_loss=2.4530348777770996\n",
      "Epoch=611, iteration=3, train_loss=2.608461618423462\n",
      "Epoch=611, val_loss=2.605363130569458\n",
      "Epoch=612, iteration=0, train_loss=2.406033992767334\n",
      "Epoch=612, iteration=1, train_loss=2.413776397705078\n",
      "Epoch=612, iteration=2, train_loss=2.452909469604492\n",
      "Epoch=612, iteration=3, train_loss=2.608341693878174\n",
      "Epoch=612, val_loss=2.605260133743286\n",
      "Epoch=613, iteration=0, train_loss=2.4058754444122314\n",
      "Epoch=613, iteration=1, train_loss=2.4136407375335693\n",
      "Epoch=613, iteration=2, train_loss=2.452784776687622\n",
      "Epoch=613, iteration=3, train_loss=2.608222246170044\n",
      "Epoch=613, val_loss=2.6051571369171143\n",
      "Epoch=614, iteration=0, train_loss=2.4057180881500244\n",
      "Epoch=614, iteration=1, train_loss=2.4135046005249023\n",
      "Epoch=614, iteration=2, train_loss=2.452660322189331\n",
      "Epoch=614, iteration=3, train_loss=2.608102798461914\n",
      "Epoch=614, val_loss=2.6050543785095215\n",
      "Epoch=615, iteration=0, train_loss=2.405560255050659\n",
      "Epoch=615, iteration=1, train_loss=2.4133694171905518\n",
      "Epoch=615, iteration=2, train_loss=2.45253586769104\n",
      "Epoch=615, iteration=3, train_loss=2.6079843044281006\n",
      "Epoch=615, val_loss=2.604951858520508\n",
      "Epoch=616, iteration=0, train_loss=2.405402421951294\n",
      "Epoch=616, iteration=1, train_loss=2.413233518600464\n",
      "Epoch=616, iteration=2, train_loss=2.452411413192749\n",
      "Epoch=616, iteration=3, train_loss=2.607866048812866\n",
      "Epoch=616, val_loss=2.6048495769500732\n",
      "Epoch=617, iteration=0, train_loss=2.405245542526245\n",
      "Epoch=617, iteration=1, train_loss=2.413097620010376\n",
      "Epoch=617, iteration=2, train_loss=2.452287197113037\n",
      "Epoch=617, iteration=3, train_loss=2.607747793197632\n",
      "Epoch=617, val_loss=2.6047472953796387\n",
      "Epoch=618, iteration=0, train_loss=2.405087947845459\n",
      "Epoch=618, iteration=1, train_loss=2.4129624366760254\n",
      "Epoch=618, iteration=2, train_loss=2.452162981033325\n",
      "Epoch=618, iteration=3, train_loss=2.6076295375823975\n",
      "Epoch=618, val_loss=2.6046459674835205\n",
      "Epoch=619, iteration=0, train_loss=2.404930830001831\n",
      "Epoch=619, iteration=1, train_loss=2.4128265380859375\n",
      "Epoch=619, iteration=2, train_loss=2.4520392417907715\n",
      "Epoch=619, iteration=3, train_loss=2.6075117588043213\n",
      "Epoch=619, val_loss=2.6045444011688232\n",
      "Epoch=620, iteration=0, train_loss=2.4047741889953613\n",
      "Epoch=620, iteration=1, train_loss=2.412691831588745\n",
      "Epoch=620, iteration=2, train_loss=2.4519152641296387\n",
      "Epoch=620, iteration=3, train_loss=2.607394218444824\n",
      "Epoch=620, val_loss=2.604443311691284\n",
      "Epoch=621, iteration=0, train_loss=2.4046175479888916\n",
      "Epoch=621, iteration=1, train_loss=2.4125564098358154\n",
      "Epoch=621, iteration=2, train_loss=2.451792001724243\n",
      "Epoch=621, iteration=3, train_loss=2.6072769165039062\n",
      "Epoch=621, val_loss=2.604341983795166\n",
      "Epoch=622, iteration=0, train_loss=2.404460906982422\n",
      "Epoch=622, iteration=1, train_loss=2.412421226501465\n",
      "Epoch=622, iteration=2, train_loss=2.4516682624816895\n",
      "Epoch=622, iteration=3, train_loss=2.6071598529815674\n",
      "Epoch=622, val_loss=2.604241371154785\n",
      "Epoch=623, iteration=0, train_loss=2.4043045043945312\n",
      "Epoch=623, iteration=1, train_loss=2.4122862815856934\n",
      "Epoch=623, iteration=2, train_loss=2.451545000076294\n",
      "Epoch=623, iteration=3, train_loss=2.6070432662963867\n",
      "Epoch=623, val_loss=2.6041407585144043\n",
      "Epoch=624, iteration=0, train_loss=2.404148578643799\n",
      "Epoch=624, iteration=1, train_loss=2.4121510982513428\n",
      "Epoch=624, iteration=2, train_loss=2.4514217376708984\n",
      "Epoch=624, iteration=3, train_loss=2.6069271564483643\n",
      "Epoch=624, val_loss=2.6040403842926025\n",
      "Epoch=625, iteration=0, train_loss=2.403991937637329\n",
      "Epoch=625, iteration=1, train_loss=2.4120163917541504\n",
      "Epoch=625, iteration=2, train_loss=2.451298236846924\n",
      "Epoch=625, iteration=3, train_loss=2.6068105697631836\n",
      "Epoch=625, val_loss=2.603940010070801\n",
      "Epoch=626, iteration=0, train_loss=2.403836250305176\n",
      "Epoch=626, iteration=1, train_loss=2.411881446838379\n",
      "Epoch=626, iteration=2, train_loss=2.4511754512786865\n",
      "Epoch=626, iteration=3, train_loss=2.6066949367523193\n",
      "Epoch=626, val_loss=2.6038403511047363\n",
      "Epoch=627, iteration=0, train_loss=2.4036808013916016\n",
      "Epoch=627, iteration=1, train_loss=2.4117467403411865\n",
      "Epoch=627, iteration=2, train_loss=2.451052665710449\n",
      "Epoch=627, iteration=3, train_loss=2.606579065322876\n",
      "Epoch=627, val_loss=2.603740692138672\n",
      "Epoch=628, iteration=0, train_loss=2.403524875640869\n",
      "Epoch=628, iteration=1, train_loss=2.4116122722625732\n",
      "Epoch=628, iteration=2, train_loss=2.450930118560791\n",
      "Epoch=628, iteration=3, train_loss=2.606463670730591\n",
      "Epoch=628, val_loss=2.6036412715911865\n",
      "Epoch=629, iteration=0, train_loss=2.403369665145874\n",
      "Epoch=629, iteration=1, train_loss=2.411478042602539\n",
      "Epoch=629, iteration=2, train_loss=2.450807809829712\n",
      "Epoch=629, iteration=3, train_loss=2.6063482761383057\n",
      "Epoch=629, val_loss=2.6035420894622803\n",
      "Epoch=630, iteration=0, train_loss=2.4032142162323\n",
      "Epoch=630, iteration=1, train_loss=2.4113433361053467\n",
      "Epoch=630, iteration=2, train_loss=2.450685739517212\n",
      "Epoch=630, iteration=3, train_loss=2.606233596801758\n",
      "Epoch=630, val_loss=2.603442907333374\n",
      "Epoch=631, iteration=0, train_loss=2.403059244155884\n",
      "Epoch=631, iteration=1, train_loss=2.4112088680267334\n",
      "Epoch=631, iteration=2, train_loss=2.4505629539489746\n",
      "Epoch=631, iteration=3, train_loss=2.606118679046631\n",
      "Epoch=631, val_loss=2.603343963623047\n",
      "Epoch=632, iteration=0, train_loss=2.4029042720794678\n",
      "Epoch=632, iteration=1, train_loss=2.41107439994812\n",
      "Epoch=632, iteration=2, train_loss=2.4504408836364746\n",
      "Epoch=632, iteration=3, train_loss=2.606004476547241\n",
      "Epoch=632, val_loss=2.603245496749878\n",
      "Epoch=633, iteration=0, train_loss=2.402749538421631\n",
      "Epoch=633, iteration=1, train_loss=2.410940170288086\n",
      "Epoch=633, iteration=2, train_loss=2.4503190517425537\n",
      "Epoch=633, iteration=3, train_loss=2.6058902740478516\n",
      "Epoch=633, val_loss=2.603147029876709\n",
      "Epoch=634, iteration=0, train_loss=2.402594804763794\n",
      "Epoch=634, iteration=1, train_loss=2.410806179046631\n",
      "Epoch=634, iteration=2, train_loss=2.4501969814300537\n",
      "Epoch=634, iteration=3, train_loss=2.605776071548462\n",
      "Epoch=634, val_loss=2.603048801422119\n",
      "Epoch=635, iteration=0, train_loss=2.402440309524536\n",
      "Epoch=635, iteration=1, train_loss=2.4106719493865967\n",
      "Epoch=635, iteration=2, train_loss=2.450075387954712\n",
      "Epoch=635, iteration=3, train_loss=2.6056625843048096\n",
      "Epoch=635, val_loss=2.6029510498046875\n",
      "Epoch=636, iteration=0, train_loss=2.4022858142852783\n",
      "Epoch=636, iteration=1, train_loss=2.4105384349823\n",
      "Epoch=636, iteration=2, train_loss=2.449953556060791\n",
      "Epoch=636, iteration=3, train_loss=2.605548858642578\n",
      "Epoch=636, val_loss=2.602853298187256\n",
      "Epoch=637, iteration=0, train_loss=2.4021317958831787\n",
      "Epoch=637, iteration=1, train_loss=2.4104042053222656\n",
      "Epoch=637, iteration=2, train_loss=2.4498322010040283\n",
      "Epoch=637, iteration=3, train_loss=2.605435848236084\n",
      "Epoch=637, val_loss=2.6027560234069824\n",
      "Epoch=638, iteration=0, train_loss=2.4019775390625\n",
      "Epoch=638, iteration=1, train_loss=2.4102702140808105\n",
      "Epoch=638, iteration=2, train_loss=2.4497108459472656\n",
      "Epoch=638, iteration=3, train_loss=2.6053225994110107\n",
      "Epoch=638, val_loss=2.602658271789551\n",
      "Epoch=639, iteration=0, train_loss=2.4018237590789795\n",
      "Epoch=639, iteration=1, train_loss=2.4101364612579346\n",
      "Epoch=639, iteration=2, train_loss=2.449589490890503\n",
      "Epoch=639, iteration=3, train_loss=2.6052098274230957\n",
      "Epoch=639, val_loss=2.6025612354278564\n",
      "Epoch=640, iteration=0, train_loss=2.401669979095459\n",
      "Epoch=640, iteration=1, train_loss=2.4100024700164795\n",
      "Epoch=640, iteration=2, train_loss=2.4494683742523193\n",
      "Epoch=640, iteration=3, train_loss=2.605097532272339\n",
      "Epoch=640, val_loss=2.602464437484741\n",
      "Epoch=641, iteration=0, train_loss=2.4015159606933594\n",
      "Epoch=641, iteration=1, train_loss=2.4098691940307617\n",
      "Epoch=641, iteration=2, train_loss=2.4493472576141357\n",
      "Epoch=641, iteration=3, train_loss=2.604985237121582\n",
      "Epoch=641, val_loss=2.602367401123047\n",
      "Epoch=642, iteration=0, train_loss=2.401362419128418\n",
      "Epoch=642, iteration=1, train_loss=2.4097354412078857\n",
      "Epoch=642, iteration=2, train_loss=2.449226140975952\n",
      "Epoch=642, iteration=3, train_loss=2.6048731803894043\n",
      "Epoch=642, val_loss=2.6022708415985107\n",
      "Epoch=643, iteration=0, train_loss=2.4012091159820557\n",
      "Epoch=643, iteration=1, train_loss=2.409602165222168\n",
      "Epoch=643, iteration=2, train_loss=2.449105739593506\n",
      "Epoch=643, iteration=3, train_loss=2.6047611236572266\n",
      "Epoch=643, val_loss=2.6021745204925537\n",
      "Epoch=644, iteration=0, train_loss=2.4010558128356934\n",
      "Epoch=644, iteration=1, train_loss=2.409468650817871\n",
      "Epoch=644, iteration=2, train_loss=2.4489848613739014\n",
      "Epoch=644, iteration=3, train_loss=2.604649543762207\n",
      "Epoch=644, val_loss=2.6020779609680176\n",
      "Epoch=645, iteration=0, train_loss=2.40090274810791\n",
      "Epoch=645, iteration=1, train_loss=2.409335136413574\n",
      "Epoch=645, iteration=2, train_loss=2.448864698410034\n",
      "Epoch=645, iteration=3, train_loss=2.6045382022857666\n",
      "Epoch=645, val_loss=2.601982355117798\n",
      "Epoch=646, iteration=0, train_loss=2.400749444961548\n",
      "Epoch=646, iteration=1, train_loss=2.4092020988464355\n",
      "Epoch=646, iteration=2, train_loss=2.448744297027588\n",
      "Epoch=646, iteration=3, train_loss=2.604426860809326\n",
      "Epoch=646, val_loss=2.601886749267578\n",
      "Epoch=647, iteration=0, train_loss=2.4005966186523438\n",
      "Epoch=647, iteration=1, train_loss=2.4090683460235596\n",
      "Epoch=647, iteration=2, train_loss=2.4486241340637207\n",
      "Epoch=647, iteration=3, train_loss=2.604315996170044\n",
      "Epoch=647, val_loss=2.6017909049987793\n",
      "Epoch=648, iteration=0, train_loss=2.4004437923431396\n",
      "Epoch=648, iteration=1, train_loss=2.408935308456421\n",
      "Epoch=648, iteration=2, train_loss=2.4485037326812744\n",
      "Epoch=648, iteration=3, train_loss=2.6042051315307617\n",
      "Epoch=648, val_loss=2.6016952991485596\n",
      "Epoch=649, iteration=0, train_loss=2.4002914428710938\n",
      "Epoch=649, iteration=1, train_loss=2.4088022708892822\n",
      "Epoch=649, iteration=2, train_loss=2.4483835697174072\n",
      "Epoch=649, iteration=3, train_loss=2.6040947437286377\n",
      "Epoch=649, val_loss=2.601600408554077\n",
      "Epoch=650, iteration=0, train_loss=2.4001388549804688\n",
      "Epoch=650, iteration=1, train_loss=2.4086692333221436\n",
      "Epoch=650, iteration=2, train_loss=2.448263645172119\n",
      "Epoch=650, iteration=3, train_loss=2.6039845943450928\n",
      "Epoch=650, val_loss=2.6015052795410156\n",
      "Epoch=651, iteration=0, train_loss=2.399986743927002\n",
      "Epoch=651, iteration=1, train_loss=2.408536195755005\n",
      "Epoch=651, iteration=2, train_loss=2.44814395904541\n",
      "Epoch=651, iteration=3, train_loss=2.603874444961548\n",
      "Epoch=651, val_loss=2.601410388946533\n",
      "Epoch=652, iteration=0, train_loss=2.399834394454956\n",
      "Epoch=652, iteration=1, train_loss=2.408403158187866\n",
      "Epoch=652, iteration=2, train_loss=2.448024272918701\n",
      "Epoch=652, iteration=3, train_loss=2.603764772415161\n",
      "Epoch=652, val_loss=2.601315975189209\n",
      "Epoch=653, iteration=0, train_loss=2.39968204498291\n",
      "Epoch=653, iteration=1, train_loss=2.4082703590393066\n",
      "Epoch=653, iteration=2, train_loss=2.447904586791992\n",
      "Epoch=653, iteration=3, train_loss=2.6036550998687744\n",
      "Epoch=653, val_loss=2.6012213230133057\n",
      "Epoch=654, iteration=0, train_loss=2.3995299339294434\n",
      "Epoch=654, iteration=1, train_loss=2.408137559890747\n",
      "Epoch=654, iteration=2, train_loss=2.447784900665283\n",
      "Epoch=654, iteration=3, train_loss=2.603545665740967\n",
      "Epoch=654, val_loss=2.6011269092559814\n",
      "Epoch=655, iteration=0, train_loss=2.3993780612945557\n",
      "Epoch=655, iteration=1, train_loss=2.4080052375793457\n",
      "Epoch=655, iteration=2, train_loss=2.4476659297943115\n",
      "Epoch=655, iteration=3, train_loss=2.6034364700317383\n",
      "Epoch=655, val_loss=2.6010327339172363\n",
      "Epoch=656, iteration=0, train_loss=2.3992269039154053\n",
      "Epoch=656, iteration=1, train_loss=2.407872438430786\n",
      "Epoch=656, iteration=2, train_loss=2.4475462436676025\n",
      "Epoch=656, iteration=3, train_loss=2.603327512741089\n",
      "Epoch=656, val_loss=2.600938558578491\n",
      "Epoch=657, iteration=0, train_loss=2.3990750312805176\n",
      "Epoch=657, iteration=1, train_loss=2.4077398777008057\n",
      "Epoch=657, iteration=2, train_loss=2.447427272796631\n",
      "Epoch=657, iteration=3, train_loss=2.6032190322875977\n",
      "Epoch=657, val_loss=2.600844621658325\n",
      "Epoch=658, iteration=0, train_loss=2.398923873901367\n",
      "Epoch=658, iteration=1, train_loss=2.407607316970825\n",
      "Epoch=658, iteration=2, train_loss=2.447308301925659\n",
      "Epoch=658, iteration=3, train_loss=2.6031100749969482\n",
      "Epoch=658, val_loss=2.6007511615753174\n",
      "Epoch=659, iteration=0, train_loss=2.3987720012664795\n",
      "Epoch=659, iteration=1, train_loss=2.407474994659424\n",
      "Epoch=659, iteration=2, train_loss=2.4471893310546875\n",
      "Epoch=659, iteration=3, train_loss=2.603001832962036\n",
      "Epoch=659, val_loss=2.6006577014923096\n",
      "Epoch=660, iteration=0, train_loss=2.398620843887329\n",
      "Epoch=660, iteration=1, train_loss=2.4073424339294434\n",
      "Epoch=660, iteration=2, train_loss=2.447070360183716\n",
      "Epoch=660, iteration=3, train_loss=2.602893829345703\n",
      "Epoch=660, val_loss=2.60056471824646\n",
      "Epoch=661, iteration=0, train_loss=2.398469924926758\n",
      "Epoch=661, iteration=1, train_loss=2.407210111618042\n",
      "Epoch=661, iteration=2, train_loss=2.4469518661499023\n",
      "Epoch=661, iteration=3, train_loss=2.602786064147949\n",
      "Epoch=661, val_loss=2.600471258163452\n",
      "Epoch=662, iteration=0, train_loss=2.3983192443847656\n",
      "Epoch=662, iteration=1, train_loss=2.4070775508880615\n",
      "Epoch=662, iteration=2, train_loss=2.4468328952789307\n",
      "Epoch=662, iteration=3, train_loss=2.602678060531616\n",
      "Epoch=662, val_loss=2.6003785133361816\n",
      "Epoch=663, iteration=0, train_loss=2.3981683254241943\n",
      "Epoch=663, iteration=1, train_loss=2.4069454669952393\n",
      "Epoch=663, iteration=2, train_loss=2.4467148780822754\n",
      "Epoch=663, iteration=3, train_loss=2.6025705337524414\n",
      "Epoch=663, val_loss=2.600285530090332\n",
      "Epoch=664, iteration=0, train_loss=2.398017406463623\n",
      "Epoch=664, iteration=1, train_loss=2.406813383102417\n",
      "Epoch=664, iteration=2, train_loss=2.446596384048462\n",
      "Epoch=664, iteration=3, train_loss=2.6024630069732666\n",
      "Epoch=664, val_loss=2.6001930236816406\n",
      "Epoch=665, iteration=0, train_loss=2.397867202758789\n",
      "Epoch=665, iteration=1, train_loss=2.4066812992095947\n",
      "Epoch=665, iteration=2, train_loss=2.4464783668518066\n",
      "Epoch=665, iteration=3, train_loss=2.602356195449829\n",
      "Epoch=665, val_loss=2.6001007556915283\n",
      "Epoch=666, iteration=0, train_loss=2.397716522216797\n",
      "Epoch=666, iteration=1, train_loss=2.4065494537353516\n",
      "Epoch=666, iteration=2, train_loss=2.4463601112365723\n",
      "Epoch=666, iteration=3, train_loss=2.6022491455078125\n",
      "Epoch=666, val_loss=2.600008249282837\n",
      "Epoch=667, iteration=0, train_loss=2.397566556930542\n",
      "Epoch=667, iteration=1, train_loss=2.40641713142395\n",
      "Epoch=667, iteration=2, train_loss=2.446242094039917\n",
      "Epoch=667, iteration=3, train_loss=2.602142810821533\n",
      "Epoch=667, val_loss=2.5999159812927246\n",
      "Epoch=668, iteration=0, train_loss=2.397416114807129\n",
      "Epoch=668, iteration=1, train_loss=2.406285524368286\n",
      "Epoch=668, iteration=2, train_loss=2.4461238384246826\n",
      "Epoch=668, iteration=3, train_loss=2.6020359992980957\n",
      "Epoch=668, val_loss=2.5998241901397705\n",
      "Epoch=669, iteration=0, train_loss=2.397266149520874\n",
      "Epoch=669, iteration=1, train_loss=2.406153678894043\n",
      "Epoch=669, iteration=2, train_loss=2.4460062980651855\n",
      "Epoch=669, iteration=3, train_loss=2.6019296646118164\n",
      "Epoch=669, val_loss=2.5997323989868164\n",
      "Epoch=670, iteration=0, train_loss=2.3971164226531982\n",
      "Epoch=670, iteration=1, train_loss=2.4060218334198\n",
      "Epoch=670, iteration=2, train_loss=2.4458887577056885\n",
      "Epoch=670, iteration=3, train_loss=2.601823329925537\n",
      "Epoch=670, val_loss=2.5996406078338623\n",
      "Epoch=671, iteration=0, train_loss=2.3969664573669434\n",
      "Epoch=671, iteration=1, train_loss=2.4058899879455566\n",
      "Epoch=671, iteration=2, train_loss=2.4457712173461914\n",
      "Epoch=671, iteration=3, train_loss=2.601717710494995\n",
      "Epoch=671, val_loss=2.5995495319366455\n",
      "Epoch=672, iteration=0, train_loss=2.396817207336426\n",
      "Epoch=672, iteration=1, train_loss=2.4057586193084717\n",
      "Epoch=672, iteration=2, train_loss=2.4456536769866943\n",
      "Epoch=672, iteration=3, train_loss=2.601612091064453\n",
      "Epoch=672, val_loss=2.5994577407836914\n",
      "Epoch=673, iteration=0, train_loss=2.39666748046875\n",
      "Epoch=673, iteration=1, train_loss=2.4056270122528076\n",
      "Epoch=673, iteration=2, train_loss=2.4455363750457764\n",
      "Epoch=673, iteration=3, train_loss=2.6015067100524902\n",
      "Epoch=673, val_loss=2.5993669033050537\n",
      "Epoch=674, iteration=0, train_loss=2.3965179920196533\n",
      "Epoch=674, iteration=1, train_loss=2.4054956436157227\n",
      "Epoch=674, iteration=2, train_loss=2.4454195499420166\n",
      "Epoch=674, iteration=3, train_loss=2.6014013290405273\n",
      "Epoch=674, val_loss=2.599276065826416\n",
      "Epoch=675, iteration=0, train_loss=2.3963687419891357\n",
      "Epoch=675, iteration=1, train_loss=2.4053642749786377\n",
      "Epoch=675, iteration=2, train_loss=2.4453020095825195\n",
      "Epoch=675, iteration=3, train_loss=2.6012959480285645\n",
      "Epoch=675, val_loss=2.599184989929199\n",
      "Epoch=676, iteration=0, train_loss=2.3962199687957764\n",
      "Epoch=676, iteration=1, train_loss=2.405233144760132\n",
      "Epoch=676, iteration=2, train_loss=2.4451849460601807\n",
      "Epoch=676, iteration=3, train_loss=2.601191282272339\n",
      "Epoch=676, val_loss=2.5990943908691406\n",
      "Epoch=677, iteration=0, train_loss=2.396070718765259\n",
      "Epoch=677, iteration=1, train_loss=2.405102014541626\n",
      "Epoch=677, iteration=2, train_loss=2.445068597793579\n",
      "Epoch=677, iteration=3, train_loss=2.601086378097534\n",
      "Epoch=677, val_loss=2.599003553390503\n",
      "Epoch=678, iteration=0, train_loss=2.3959217071533203\n",
      "Epoch=678, iteration=1, train_loss=2.404970407485962\n",
      "Epoch=678, iteration=2, train_loss=2.4449517726898193\n",
      "Epoch=678, iteration=3, train_loss=2.6009819507598877\n",
      "Epoch=678, val_loss=2.5989134311676025\n",
      "Epoch=679, iteration=0, train_loss=2.395772933959961\n",
      "Epoch=679, iteration=1, train_loss=2.404839277267456\n",
      "Epoch=679, iteration=2, train_loss=2.4448354244232178\n",
      "Epoch=679, iteration=3, train_loss=2.6008777618408203\n",
      "Epoch=679, val_loss=2.598823070526123\n",
      "Epoch=680, iteration=0, train_loss=2.395624876022339\n",
      "Epoch=680, iteration=1, train_loss=2.40470814704895\n",
      "Epoch=680, iteration=2, train_loss=2.444718599319458\n",
      "Epoch=680, iteration=3, train_loss=2.600773572921753\n",
      "Epoch=680, val_loss=2.5987327098846436\n",
      "Epoch=681, iteration=0, train_loss=2.3954763412475586\n",
      "Epoch=681, iteration=1, train_loss=2.4045772552490234\n",
      "Epoch=681, iteration=2, train_loss=2.4446022510528564\n",
      "Epoch=681, iteration=3, train_loss=2.6006696224212646\n",
      "Epoch=681, val_loss=2.5986428260803223\n",
      "Epoch=682, iteration=0, train_loss=2.3953282833099365\n",
      "Epoch=682, iteration=1, train_loss=2.4044463634490967\n",
      "Epoch=682, iteration=2, train_loss=2.444485664367676\n",
      "Epoch=682, iteration=3, train_loss=2.6005656719207764\n",
      "Epoch=682, val_loss=2.598552942276001\n",
      "Epoch=683, iteration=0, train_loss=2.3951799869537354\n",
      "Epoch=683, iteration=1, train_loss=2.404315710067749\n",
      "Epoch=683, iteration=2, train_loss=2.4443695545196533\n",
      "Epoch=683, iteration=3, train_loss=2.6004621982574463\n",
      "Epoch=683, val_loss=2.5984630584716797\n",
      "Epoch=684, iteration=0, train_loss=2.3950321674346924\n",
      "Epoch=684, iteration=1, train_loss=2.4041852951049805\n",
      "Epoch=684, iteration=2, train_loss=2.444253921508789\n",
      "Epoch=684, iteration=3, train_loss=2.600358724594116\n",
      "Epoch=684, val_loss=2.5983734130859375\n",
      "Epoch=685, iteration=0, train_loss=2.3948843479156494\n",
      "Epoch=685, iteration=1, train_loss=2.4040544033050537\n",
      "Epoch=685, iteration=2, train_loss=2.4441378116607666\n",
      "Epoch=685, iteration=3, train_loss=2.6002554893493652\n",
      "Epoch=685, val_loss=2.5982842445373535\n",
      "Epoch=686, iteration=0, train_loss=2.3947370052337646\n",
      "Epoch=686, iteration=1, train_loss=2.403923988342285\n",
      "Epoch=686, iteration=2, train_loss=2.4440219402313232\n",
      "Epoch=686, iteration=3, train_loss=2.6001522541046143\n",
      "Epoch=686, val_loss=2.5981950759887695\n",
      "Epoch=687, iteration=0, train_loss=2.3945891857147217\n",
      "Epoch=687, iteration=1, train_loss=2.4037933349609375\n",
      "Epoch=687, iteration=2, train_loss=2.443906307220459\n",
      "Epoch=687, iteration=3, train_loss=2.6000494956970215\n",
      "Epoch=687, val_loss=2.5981059074401855\n",
      "Epoch=688, iteration=0, train_loss=2.394441843032837\n",
      "Epoch=688, iteration=1, train_loss=2.403663158416748\n",
      "Epoch=688, iteration=2, train_loss=2.443790912628174\n",
      "Epoch=688, iteration=3, train_loss=2.599947214126587\n",
      "Epoch=688, val_loss=2.5980167388916016\n",
      "Epoch=689, iteration=0, train_loss=2.3942949771881104\n",
      "Epoch=689, iteration=1, train_loss=2.4035325050354004\n",
      "Epoch=689, iteration=2, train_loss=2.4436757564544678\n",
      "Epoch=689, iteration=3, train_loss=2.599844455718994\n",
      "Epoch=689, val_loss=2.597928047180176\n",
      "Epoch=690, iteration=0, train_loss=2.3941476345062256\n",
      "Epoch=690, iteration=1, train_loss=2.40340256690979\n",
      "Epoch=690, iteration=2, train_loss=2.4435603618621826\n",
      "Epoch=690, iteration=3, train_loss=2.5997421741485596\n",
      "Epoch=690, val_loss=2.59783935546875\n",
      "Epoch=691, iteration=0, train_loss=2.394001007080078\n",
      "Epoch=691, iteration=1, train_loss=2.4032721519470215\n",
      "Epoch=691, iteration=2, train_loss=2.4434449672698975\n",
      "Epoch=691, iteration=3, train_loss=2.599640130996704\n",
      "Epoch=691, val_loss=2.5977509021759033\n",
      "Epoch=692, iteration=0, train_loss=2.3938539028167725\n",
      "Epoch=692, iteration=1, train_loss=2.4031424522399902\n",
      "Epoch=692, iteration=2, train_loss=2.4433302879333496\n",
      "Epoch=692, iteration=3, train_loss=2.5995383262634277\n",
      "Epoch=692, val_loss=2.5976626873016357\n",
      "Epoch=693, iteration=0, train_loss=2.393707513809204\n",
      "Epoch=693, iteration=1, train_loss=2.40301251411438\n",
      "Epoch=693, iteration=2, train_loss=2.4432153701782227\n",
      "Epoch=693, iteration=3, train_loss=2.5994365215301514\n",
      "Epoch=693, val_loss=2.59757399559021\n",
      "Epoch=694, iteration=0, train_loss=2.3935608863830566\n",
      "Epoch=694, iteration=1, train_loss=2.4028823375701904\n",
      "Epoch=694, iteration=2, train_loss=2.4431004524230957\n",
      "Epoch=694, iteration=3, train_loss=2.599335193634033\n",
      "Epoch=694, val_loss=2.5974857807159424\n",
      "Epoch=695, iteration=0, train_loss=2.3934147357940674\n",
      "Epoch=695, iteration=1, train_loss=2.4027528762817383\n",
      "Epoch=695, iteration=2, train_loss=2.442986249923706\n",
      "Epoch=695, iteration=3, train_loss=2.599233865737915\n",
      "Epoch=695, val_loss=2.597398281097412\n",
      "Epoch=696, iteration=0, train_loss=2.3932688236236572\n",
      "Epoch=696, iteration=1, train_loss=2.402622938156128\n",
      "Epoch=696, iteration=2, train_loss=2.442871332168579\n",
      "Epoch=696, iteration=3, train_loss=2.5991322994232178\n",
      "Epoch=696, val_loss=2.5973098278045654\n",
      "Epoch=697, iteration=0, train_loss=2.393122911453247\n",
      "Epoch=697, iteration=1, train_loss=2.4024932384490967\n",
      "Epoch=697, iteration=2, train_loss=2.4427576065063477\n",
      "Epoch=697, iteration=3, train_loss=2.5990312099456787\n",
      "Epoch=697, val_loss=2.597221851348877\n",
      "Epoch=698, iteration=0, train_loss=2.392976999282837\n",
      "Epoch=698, iteration=1, train_loss=2.4023640155792236\n",
      "Epoch=698, iteration=2, train_loss=2.4426426887512207\n",
      "Epoch=698, iteration=3, train_loss=2.598930597305298\n",
      "Epoch=698, val_loss=2.597134590148926\n",
      "Epoch=699, iteration=0, train_loss=2.392831325531006\n",
      "Epoch=699, iteration=1, train_loss=2.4022347927093506\n",
      "Epoch=699, iteration=2, train_loss=2.44252872467041\n",
      "Epoch=699, iteration=3, train_loss=2.598829984664917\n",
      "Epoch=699, val_loss=2.5970473289489746\n",
      "Epoch=700, iteration=0, train_loss=2.392686128616333\n",
      "Epoch=700, iteration=1, train_loss=2.4021055698394775\n",
      "Epoch=700, iteration=2, train_loss=2.4424147605895996\n",
      "Epoch=700, iteration=3, train_loss=2.5987296104431152\n",
      "Epoch=700, val_loss=2.5969595909118652\n",
      "Epoch=701, iteration=0, train_loss=2.392540693283081\n",
      "Epoch=701, iteration=1, train_loss=2.4019763469696045\n",
      "Epoch=701, iteration=2, train_loss=2.4423012733459473\n",
      "Epoch=701, iteration=3, train_loss=2.5986294746398926\n",
      "Epoch=701, val_loss=2.596872329711914\n",
      "Epoch=702, iteration=0, train_loss=2.3923957347869873\n",
      "Epoch=702, iteration=1, train_loss=2.4018471240997314\n",
      "Epoch=702, iteration=2, train_loss=2.442187547683716\n",
      "Epoch=702, iteration=3, train_loss=2.59852933883667\n",
      "Epoch=702, val_loss=2.596785306930542\n",
      "Epoch=703, iteration=0, train_loss=2.3922507762908936\n",
      "Epoch=703, iteration=1, train_loss=2.4017181396484375\n",
      "Epoch=703, iteration=2, train_loss=2.4420738220214844\n",
      "Epoch=703, iteration=3, train_loss=2.5984294414520264\n",
      "Epoch=703, val_loss=2.59669828414917\n",
      "Epoch=704, iteration=0, train_loss=2.392106294631958\n",
      "Epoch=704, iteration=1, train_loss=2.4015893936157227\n",
      "Epoch=704, iteration=2, train_loss=2.441960573196411\n",
      "Epoch=704, iteration=3, train_loss=2.598330020904541\n",
      "Epoch=704, val_loss=2.596611261367798\n",
      "Epoch=705, iteration=0, train_loss=2.3919618129730225\n",
      "Epoch=705, iteration=1, train_loss=2.401460647583008\n",
      "Epoch=705, iteration=2, train_loss=2.441847324371338\n",
      "Epoch=705, iteration=3, train_loss=2.5982298851013184\n",
      "Epoch=705, val_loss=2.596524715423584\n",
      "Epoch=706, iteration=0, train_loss=2.391817331314087\n",
      "Epoch=706, iteration=1, train_loss=2.4013314247131348\n",
      "Epoch=706, iteration=2, train_loss=2.4417340755462646\n",
      "Epoch=706, iteration=3, train_loss=2.598130702972412\n",
      "Epoch=706, val_loss=2.596437931060791\n",
      "Epoch=707, iteration=0, train_loss=2.3916730880737305\n",
      "Epoch=707, iteration=1, train_loss=2.4012036323547363\n",
      "Epoch=707, iteration=2, train_loss=2.4416213035583496\n",
      "Epoch=707, iteration=3, train_loss=2.598031520843506\n",
      "Epoch=707, val_loss=2.5963516235351562\n",
      "Epoch=708, iteration=0, train_loss=2.391529083251953\n",
      "Epoch=708, iteration=1, train_loss=2.4010748863220215\n",
      "Epoch=708, iteration=2, train_loss=2.4415085315704346\n",
      "Epoch=708, iteration=3, train_loss=2.5979321002960205\n",
      "Epoch=708, val_loss=2.5962650775909424\n",
      "Epoch=709, iteration=0, train_loss=2.391385316848755\n",
      "Epoch=709, iteration=1, train_loss=2.400946617126465\n",
      "Epoch=709, iteration=2, train_loss=2.4413957595825195\n",
      "Epoch=709, iteration=3, train_loss=2.5978333950042725\n",
      "Epoch=709, val_loss=2.5961790084838867\n",
      "Epoch=710, iteration=0, train_loss=2.391242027282715\n",
      "Epoch=710, iteration=1, train_loss=2.400818347930908\n",
      "Epoch=710, iteration=2, train_loss=2.4412829875946045\n",
      "Epoch=710, iteration=3, train_loss=2.5977346897125244\n",
      "Epoch=710, val_loss=2.596092939376831\n",
      "Epoch=711, iteration=0, train_loss=2.3910984992980957\n",
      "Epoch=711, iteration=1, train_loss=2.4006903171539307\n",
      "Epoch=711, iteration=2, train_loss=2.4411706924438477\n",
      "Epoch=711, iteration=3, train_loss=2.5976362228393555\n",
      "Epoch=711, val_loss=2.5960066318511963\n",
      "Epoch=712, iteration=0, train_loss=2.3909552097320557\n",
      "Epoch=712, iteration=1, train_loss=2.400562286376953\n",
      "Epoch=712, iteration=2, train_loss=2.441058397293091\n",
      "Epoch=712, iteration=3, train_loss=2.5975377559661865\n",
      "Epoch=712, val_loss=2.5959205627441406\n",
      "Epoch=713, iteration=0, train_loss=2.3908119201660156\n",
      "Epoch=713, iteration=1, train_loss=2.400434732437134\n",
      "Epoch=713, iteration=2, train_loss=2.440946340560913\n",
      "Epoch=713, iteration=3, train_loss=2.5974392890930176\n",
      "Epoch=713, val_loss=2.595834970474243\n",
      "Epoch=714, iteration=0, train_loss=2.390669107437134\n",
      "Epoch=714, iteration=1, train_loss=2.4003069400787354\n",
      "Epoch=714, iteration=2, train_loss=2.4408342838287354\n",
      "Epoch=714, iteration=3, train_loss=2.597341537475586\n",
      "Epoch=714, val_loss=2.5957493782043457\n",
      "Epoch=715, iteration=0, train_loss=2.390526533126831\n",
      "Epoch=715, iteration=1, train_loss=2.400179386138916\n",
      "Epoch=715, iteration=2, train_loss=2.440722703933716\n",
      "Epoch=715, iteration=3, train_loss=2.5972437858581543\n",
      "Epoch=715, val_loss=2.5956637859344482\n",
      "Epoch=716, iteration=0, train_loss=2.390383720397949\n",
      "Epoch=716, iteration=1, train_loss=2.4000518321990967\n",
      "Epoch=716, iteration=2, train_loss=2.4406111240386963\n",
      "Epoch=716, iteration=3, train_loss=2.5971460342407227\n",
      "Epoch=716, val_loss=2.595578193664551\n",
      "Epoch=717, iteration=0, train_loss=2.390242099761963\n",
      "Epoch=717, iteration=1, train_loss=2.3999242782592773\n",
      "Epoch=717, iteration=2, train_loss=2.4404995441436768\n",
      "Epoch=717, iteration=3, train_loss=2.597048282623291\n",
      "Epoch=717, val_loss=2.5954930782318115\n",
      "Epoch=718, iteration=0, train_loss=2.39009952545166\n",
      "Epoch=718, iteration=1, train_loss=2.399796962738037\n",
      "Epoch=718, iteration=2, train_loss=2.4403879642486572\n",
      "Epoch=718, iteration=3, train_loss=2.5969510078430176\n",
      "Epoch=718, val_loss=2.5954079627990723\n",
      "Epoch=719, iteration=0, train_loss=2.389957904815674\n",
      "Epoch=719, iteration=1, train_loss=2.399669647216797\n",
      "Epoch=719, iteration=2, train_loss=2.440277099609375\n",
      "Epoch=719, iteration=3, train_loss=2.596853733062744\n",
      "Epoch=719, val_loss=2.595322608947754\n",
      "Epoch=720, iteration=0, train_loss=2.3898160457611084\n",
      "Epoch=720, iteration=1, train_loss=2.399542808532715\n",
      "Epoch=720, iteration=2, train_loss=2.4401657581329346\n",
      "Epoch=720, iteration=3, train_loss=2.59675669670105\n",
      "Epoch=720, val_loss=2.595237970352173\n",
      "Epoch=721, iteration=0, train_loss=2.389674663543701\n",
      "Epoch=721, iteration=1, train_loss=2.399416208267212\n",
      "Epoch=721, iteration=2, train_loss=2.4400553703308105\n",
      "Epoch=721, iteration=3, train_loss=2.5966598987579346\n",
      "Epoch=721, val_loss=2.5951530933380127\n",
      "Epoch=722, iteration=0, train_loss=2.3895339965820312\n",
      "Epoch=722, iteration=1, train_loss=2.39928936958313\n",
      "Epoch=722, iteration=2, train_loss=2.43994402885437\n",
      "Epoch=722, iteration=3, train_loss=2.5965633392333984\n",
      "Epoch=722, val_loss=2.5950679779052734\n",
      "Epoch=723, iteration=0, train_loss=2.389392852783203\n",
      "Epoch=723, iteration=1, train_loss=2.399162769317627\n",
      "Epoch=723, iteration=2, train_loss=2.439833879470825\n",
      "Epoch=723, iteration=3, train_loss=2.5964667797088623\n",
      "Epoch=723, val_loss=2.5949835777282715\n",
      "Epoch=724, iteration=0, train_loss=2.389251947402954\n",
      "Epoch=724, iteration=1, train_loss=2.3990366458892822\n",
      "Epoch=724, iteration=2, train_loss=2.439723253250122\n",
      "Epoch=724, iteration=3, train_loss=2.596370220184326\n",
      "Epoch=724, val_loss=2.5948991775512695\n",
      "Epoch=725, iteration=0, train_loss=2.3891115188598633\n",
      "Epoch=725, iteration=1, train_loss=2.3989105224609375\n",
      "Epoch=725, iteration=2, train_loss=2.4396133422851562\n",
      "Epoch=725, iteration=3, train_loss=2.5962741374969482\n",
      "Epoch=725, val_loss=2.5948145389556885\n",
      "Epoch=726, iteration=0, train_loss=2.3889710903167725\n",
      "Epoch=726, iteration=1, train_loss=2.3987841606140137\n",
      "Epoch=726, iteration=2, train_loss=2.4395031929016113\n",
      "Epoch=726, iteration=3, train_loss=2.5961782932281494\n",
      "Epoch=726, val_loss=2.5947301387786865\n",
      "Epoch=727, iteration=0, train_loss=2.3888309001922607\n",
      "Epoch=727, iteration=1, train_loss=2.398658275604248\n",
      "Epoch=727, iteration=2, train_loss=2.4393937587738037\n",
      "Epoch=727, iteration=3, train_loss=2.5960824489593506\n",
      "Epoch=727, val_loss=2.5946459770202637\n",
      "Epoch=728, iteration=0, train_loss=2.388690948486328\n",
      "Epoch=728, iteration=1, train_loss=2.3985326290130615\n",
      "Epoch=728, iteration=2, train_loss=2.4392833709716797\n",
      "Epoch=728, iteration=3, train_loss=2.5959866046905518\n",
      "Epoch=728, val_loss=2.594561815261841\n",
      "Epoch=729, iteration=0, train_loss=2.3885512351989746\n",
      "Epoch=729, iteration=1, train_loss=2.398406744003296\n",
      "Epoch=729, iteration=2, train_loss=2.439173936843872\n",
      "Epoch=729, iteration=3, train_loss=2.595891237258911\n",
      "Epoch=729, val_loss=2.594478130340576\n",
      "Epoch=730, iteration=0, train_loss=2.3884117603302\n",
      "Epoch=730, iteration=1, train_loss=2.3982810974121094\n",
      "Epoch=730, iteration=2, train_loss=2.4390645027160645\n",
      "Epoch=730, iteration=3, train_loss=2.5957956314086914\n",
      "Epoch=730, val_loss=2.5943942070007324\n",
      "Epoch=731, iteration=0, train_loss=2.388272762298584\n",
      "Epoch=731, iteration=1, train_loss=2.398155927658081\n",
      "Epoch=731, iteration=2, train_loss=2.438955307006836\n",
      "Epoch=731, iteration=3, train_loss=2.595700740814209\n",
      "Epoch=731, val_loss=2.5943105220794678\n",
      "Epoch=732, iteration=0, train_loss=2.3881335258483887\n",
      "Epoch=732, iteration=1, train_loss=2.3980305194854736\n",
      "Epoch=732, iteration=2, train_loss=2.4388463497161865\n",
      "Epoch=732, iteration=3, train_loss=2.5956056118011475\n",
      "Epoch=732, val_loss=2.594226598739624\n",
      "Epoch=733, iteration=0, train_loss=2.3879950046539307\n",
      "Epoch=733, iteration=1, train_loss=2.3979055881500244\n",
      "Epoch=733, iteration=2, train_loss=2.438737392425537\n",
      "Epoch=733, iteration=3, train_loss=2.595510482788086\n",
      "Epoch=733, val_loss=2.5941431522369385\n",
      "Epoch=734, iteration=0, train_loss=2.3878562450408936\n",
      "Epoch=734, iteration=1, train_loss=2.397780418395996\n",
      "Epoch=734, iteration=2, train_loss=2.438628911972046\n",
      "Epoch=734, iteration=3, train_loss=2.5954160690307617\n",
      "Epoch=734, val_loss=2.594059705734253\n",
      "Epoch=735, iteration=0, train_loss=2.3877179622650146\n",
      "Epoch=735, iteration=1, train_loss=2.397655487060547\n",
      "Epoch=735, iteration=2, train_loss=2.4385201930999756\n",
      "Epoch=735, iteration=3, train_loss=2.5953216552734375\n",
      "Epoch=735, val_loss=2.5939764976501465\n",
      "Epoch=736, iteration=0, train_loss=2.387579917907715\n",
      "Epoch=736, iteration=1, train_loss=2.3975307941436768\n",
      "Epoch=736, iteration=2, train_loss=2.4384114742279053\n",
      "Epoch=736, iteration=3, train_loss=2.595227003097534\n",
      "Epoch=736, val_loss=2.593893051147461\n",
      "Epoch=737, iteration=0, train_loss=2.387441873550415\n",
      "Epoch=737, iteration=1, train_loss=2.3974063396453857\n",
      "Epoch=737, iteration=2, train_loss=2.4383034706115723\n",
      "Epoch=737, iteration=3, train_loss=2.59513258934021\n",
      "Epoch=737, val_loss=2.5938098430633545\n",
      "Epoch=738, iteration=0, train_loss=2.3873043060302734\n",
      "Epoch=738, iteration=1, train_loss=2.397282361984253\n",
      "Epoch=738, iteration=2, train_loss=2.4381959438323975\n",
      "Epoch=738, iteration=3, train_loss=2.595038652420044\n",
      "Epoch=738, val_loss=2.593726873397827\n",
      "Epoch=739, iteration=0, train_loss=2.38716721534729\n",
      "Epoch=739, iteration=1, train_loss=2.397158145904541\n",
      "Epoch=739, iteration=2, train_loss=2.4380877017974854\n",
      "Epoch=739, iteration=3, train_loss=2.594944477081299\n",
      "Epoch=739, val_loss=2.5936439037323\n",
      "Epoch=740, iteration=0, train_loss=2.3870298862457275\n",
      "Epoch=740, iteration=1, train_loss=2.397033929824829\n",
      "Epoch=740, iteration=2, train_loss=2.4379799365997314\n",
      "Epoch=740, iteration=3, train_loss=2.594851016998291\n",
      "Epoch=740, val_loss=2.5935614109039307\n",
      "Epoch=741, iteration=0, train_loss=2.3868932723999023\n",
      "Epoch=741, iteration=1, train_loss=2.3969101905822754\n",
      "Epoch=741, iteration=2, train_loss=2.4378726482391357\n",
      "Epoch=741, iteration=3, train_loss=2.594757556915283\n",
      "Epoch=741, val_loss=2.5934786796569824\n",
      "Epoch=742, iteration=0, train_loss=2.386756420135498\n",
      "Epoch=742, iteration=1, train_loss=2.3967864513397217\n",
      "Epoch=742, iteration=2, train_loss=2.437765121459961\n",
      "Epoch=742, iteration=3, train_loss=2.5946638584136963\n",
      "Epoch=742, val_loss=2.593395948410034\n",
      "Epoch=743, iteration=0, train_loss=2.386620044708252\n",
      "Epoch=743, iteration=1, train_loss=2.396663188934326\n",
      "Epoch=743, iteration=2, train_loss=2.4376578330993652\n",
      "Epoch=743, iteration=3, train_loss=2.5945706367492676\n",
      "Epoch=743, val_loss=2.593313217163086\n",
      "Epoch=744, iteration=0, train_loss=2.386483669281006\n",
      "Epoch=744, iteration=1, train_loss=2.3965396881103516\n",
      "Epoch=744, iteration=2, train_loss=2.4375507831573486\n",
      "Epoch=744, iteration=3, train_loss=2.594477415084839\n",
      "Epoch=744, val_loss=2.593230962753296\n",
      "Epoch=745, iteration=0, train_loss=2.386348009109497\n",
      "Epoch=745, iteration=1, train_loss=2.396416664123535\n",
      "Epoch=745, iteration=2, train_loss=2.4374444484710693\n",
      "Epoch=745, iteration=3, train_loss=2.59438419342041\n",
      "Epoch=745, val_loss=2.593148708343506\n",
      "Epoch=746, iteration=0, train_loss=2.3862123489379883\n",
      "Epoch=746, iteration=1, train_loss=2.396293878555298\n",
      "Epoch=746, iteration=2, train_loss=2.437337636947632\n",
      "Epoch=746, iteration=3, train_loss=2.5942914485931396\n",
      "Epoch=746, val_loss=2.5930662155151367\n",
      "Epoch=747, iteration=0, train_loss=2.3860771656036377\n",
      "Epoch=747, iteration=1, train_loss=2.3961710929870605\n",
      "Epoch=747, iteration=2, train_loss=2.4372313022613525\n",
      "Epoch=747, iteration=3, train_loss=2.5941991806030273\n",
      "Epoch=747, val_loss=2.592984199523926\n",
      "Epoch=748, iteration=0, train_loss=2.385941982269287\n",
      "Epoch=748, iteration=1, train_loss=2.3960485458374023\n",
      "Epoch=748, iteration=2, train_loss=2.4371252059936523\n",
      "Epoch=748, iteration=3, train_loss=2.594106674194336\n",
      "Epoch=748, val_loss=2.592902183532715\n",
      "Epoch=749, iteration=0, train_loss=2.3858072757720947\n",
      "Epoch=749, iteration=1, train_loss=2.395925998687744\n",
      "Epoch=749, iteration=2, train_loss=2.437018871307373\n",
      "Epoch=749, iteration=3, train_loss=2.5940139293670654\n",
      "Epoch=749, val_loss=2.592820167541504\n",
      "Epoch=750, iteration=0, train_loss=2.3856728076934814\n",
      "Epoch=750, iteration=1, train_loss=2.395803928375244\n",
      "Epoch=750, iteration=2, train_loss=2.436913251876831\n",
      "Epoch=750, iteration=3, train_loss=2.5939218997955322\n",
      "Epoch=750, val_loss=2.592738151550293\n",
      "Epoch=751, iteration=0, train_loss=2.385538101196289\n",
      "Epoch=751, iteration=1, train_loss=2.395681619644165\n",
      "Epoch=751, iteration=2, train_loss=2.43680739402771\n",
      "Epoch=751, iteration=3, train_loss=2.59382963180542\n",
      "Epoch=751, val_loss=2.5926568508148193\n",
      "Epoch=752, iteration=0, train_loss=2.385404109954834\n",
      "Epoch=752, iteration=1, train_loss=2.395559549331665\n",
      "Epoch=752, iteration=2, train_loss=2.436702251434326\n",
      "Epoch=752, iteration=3, train_loss=2.593738079071045\n",
      "Epoch=752, val_loss=2.5925748348236084\n",
      "Epoch=753, iteration=0, train_loss=2.385270357131958\n",
      "Epoch=753, iteration=1, train_loss=2.3954379558563232\n",
      "Epoch=753, iteration=2, train_loss=2.436596632003784\n",
      "Epoch=753, iteration=3, train_loss=2.5936460494995117\n",
      "Epoch=753, val_loss=2.5924932956695557\n",
      "Epoch=754, iteration=0, train_loss=2.385136842727661\n",
      "Epoch=754, iteration=1, train_loss=2.3953166007995605\n",
      "Epoch=754, iteration=2, train_loss=2.4364917278289795\n",
      "Epoch=754, iteration=3, train_loss=2.5935544967651367\n",
      "Epoch=754, val_loss=2.592411994934082\n",
      "Epoch=755, iteration=0, train_loss=2.3850035667419434\n",
      "Epoch=755, iteration=1, train_loss=2.395195245742798\n",
      "Epoch=755, iteration=2, train_loss=2.436386823654175\n",
      "Epoch=755, iteration=3, train_loss=2.5934629440307617\n",
      "Epoch=755, val_loss=2.5923306941986084\n",
      "Epoch=756, iteration=0, train_loss=2.384870767593384\n",
      "Epoch=756, iteration=1, train_loss=2.3950741291046143\n",
      "Epoch=756, iteration=2, train_loss=2.4362823963165283\n",
      "Epoch=756, iteration=3, train_loss=2.593371868133545\n",
      "Epoch=756, val_loss=2.5922491550445557\n",
      "Epoch=757, iteration=0, train_loss=2.384737968444824\n",
      "Epoch=757, iteration=1, train_loss=2.3949530124664307\n",
      "Epoch=757, iteration=2, train_loss=2.4361774921417236\n",
      "Epoch=757, iteration=3, train_loss=2.593280553817749\n",
      "Epoch=757, val_loss=2.5921683311462402\n",
      "Epoch=758, iteration=0, train_loss=2.3846054077148438\n",
      "Epoch=758, iteration=1, train_loss=2.394831895828247\n",
      "Epoch=758, iteration=2, train_loss=2.436073064804077\n",
      "Epoch=758, iteration=3, train_loss=2.5931894779205322\n",
      "Epoch=758, val_loss=2.5920870304107666\n",
      "Epoch=759, iteration=0, train_loss=2.3844735622406006\n",
      "Epoch=759, iteration=1, train_loss=2.394711494445801\n",
      "Epoch=759, iteration=2, train_loss=2.435969114303589\n",
      "Epoch=759, iteration=3, train_loss=2.5930988788604736\n",
      "Epoch=759, val_loss=2.592006206512451\n",
      "Epoch=760, iteration=0, train_loss=2.3843414783477783\n",
      "Epoch=760, iteration=1, train_loss=2.3945910930633545\n",
      "Epoch=760, iteration=2, train_loss=2.4358649253845215\n",
      "Epoch=760, iteration=3, train_loss=2.593008041381836\n",
      "Epoch=760, val_loss=2.5919253826141357\n",
      "Epoch=761, iteration=0, train_loss=2.3842098712921143\n",
      "Epoch=761, iteration=1, train_loss=2.3944709300994873\n",
      "Epoch=761, iteration=2, train_loss=2.4357614517211914\n",
      "Epoch=761, iteration=3, train_loss=2.5929174423217773\n",
      "Epoch=761, val_loss=2.5918445587158203\n",
      "Epoch=762, iteration=0, train_loss=2.3840787410736084\n",
      "Epoch=762, iteration=1, train_loss=2.394351005554199\n",
      "Epoch=762, iteration=2, train_loss=2.4356577396392822\n",
      "Epoch=762, iteration=3, train_loss=2.592827081680298\n",
      "Epoch=762, val_loss=2.591763734817505\n",
      "Epoch=763, iteration=0, train_loss=2.3839476108551025\n",
      "Epoch=763, iteration=1, train_loss=2.394231081008911\n",
      "Epoch=763, iteration=2, train_loss=2.4355545043945312\n",
      "Epoch=763, iteration=3, train_loss=2.5927367210388184\n",
      "Epoch=763, val_loss=2.5916831493377686\n",
      "Epoch=764, iteration=0, train_loss=2.383816957473755\n",
      "Epoch=764, iteration=1, train_loss=2.394111394882202\n",
      "Epoch=764, iteration=2, train_loss=2.4354512691497803\n",
      "Epoch=764, iteration=3, train_loss=2.592646598815918\n",
      "Epoch=764, val_loss=2.5916025638580322\n",
      "Epoch=765, iteration=0, train_loss=2.3836863040924072\n",
      "Epoch=765, iteration=1, train_loss=2.3939919471740723\n",
      "Epoch=765, iteration=2, train_loss=2.4353485107421875\n",
      "Epoch=765, iteration=3, train_loss=2.5925567150115967\n",
      "Epoch=765, val_loss=2.591522455215454\n",
      "Epoch=766, iteration=0, train_loss=2.3835561275482178\n",
      "Epoch=766, iteration=1, train_loss=2.3938727378845215\n",
      "Epoch=766, iteration=2, train_loss=2.4352455139160156\n",
      "Epoch=766, iteration=3, train_loss=2.5924670696258545\n",
      "Epoch=766, val_loss=2.591442346572876\n",
      "Epoch=767, iteration=0, train_loss=2.3834261894226074\n",
      "Epoch=767, iteration=1, train_loss=2.393754243850708\n",
      "Epoch=767, iteration=2, train_loss=2.435142755508423\n",
      "Epoch=767, iteration=3, train_loss=2.5923774242401123\n",
      "Epoch=767, val_loss=2.5913619995117188\n",
      "Epoch=768, iteration=0, train_loss=2.3832967281341553\n",
      "Epoch=768, iteration=1, train_loss=2.3936350345611572\n",
      "Epoch=768, iteration=2, train_loss=2.4350407123565674\n",
      "Epoch=768, iteration=3, train_loss=2.592287540435791\n",
      "Epoch=768, val_loss=2.5912818908691406\n",
      "Epoch=769, iteration=0, train_loss=2.383167028427124\n",
      "Epoch=769, iteration=1, train_loss=2.393516778945923\n",
      "Epoch=769, iteration=2, train_loss=2.4349381923675537\n",
      "Epoch=769, iteration=3, train_loss=2.592198610305786\n",
      "Epoch=769, val_loss=2.5912017822265625\n",
      "Epoch=770, iteration=0, train_loss=2.383037805557251\n",
      "Epoch=770, iteration=1, train_loss=2.393397808074951\n",
      "Epoch=770, iteration=2, train_loss=2.4348366260528564\n",
      "Epoch=770, iteration=3, train_loss=2.5921096801757812\n",
      "Epoch=770, val_loss=2.5911219120025635\n",
      "Epoch=771, iteration=0, train_loss=2.3829092979431152\n",
      "Epoch=771, iteration=1, train_loss=2.393279790878296\n",
      "Epoch=771, iteration=2, train_loss=2.434734344482422\n",
      "Epoch=771, iteration=3, train_loss=2.592020273208618\n",
      "Epoch=771, val_loss=2.5910420417785645\n",
      "Epoch=772, iteration=0, train_loss=2.3827807903289795\n",
      "Epoch=772, iteration=1, train_loss=2.3931617736816406\n",
      "Epoch=772, iteration=2, train_loss=2.4346330165863037\n",
      "Epoch=772, iteration=3, train_loss=2.5919315814971924\n",
      "Epoch=772, val_loss=2.5909621715545654\n",
      "Epoch=773, iteration=0, train_loss=2.382652521133423\n",
      "Epoch=773, iteration=1, train_loss=2.3930439949035645\n",
      "Epoch=773, iteration=2, train_loss=2.4345316886901855\n",
      "Epoch=773, iteration=3, train_loss=2.5918426513671875\n",
      "Epoch=773, val_loss=2.5908827781677246\n",
      "Epoch=774, iteration=0, train_loss=2.3825244903564453\n",
      "Epoch=774, iteration=1, train_loss=2.3929264545440674\n",
      "Epoch=774, iteration=2, train_loss=2.4344301223754883\n",
      "Epoch=774, iteration=3, train_loss=2.591754198074341\n",
      "Epoch=774, val_loss=2.5908031463623047\n",
      "Epoch=775, iteration=0, train_loss=2.382396936416626\n",
      "Epoch=775, iteration=1, train_loss=2.3928089141845703\n",
      "Epoch=775, iteration=2, train_loss=2.4343295097351074\n",
      "Epoch=775, iteration=3, train_loss=2.591665744781494\n",
      "Epoch=775, val_loss=2.5907235145568848\n",
      "Epoch=776, iteration=0, train_loss=2.3822696208953857\n",
      "Epoch=776, iteration=1, train_loss=2.3926918506622314\n",
      "Epoch=776, iteration=2, train_loss=2.4342286586761475\n",
      "Epoch=776, iteration=3, train_loss=2.5915772914886475\n",
      "Epoch=776, val_loss=2.590644121170044\n",
      "Epoch=777, iteration=0, train_loss=2.3821425437927246\n",
      "Epoch=777, iteration=1, train_loss=2.3925747871398926\n",
      "Epoch=777, iteration=2, train_loss=2.4341280460357666\n",
      "Epoch=777, iteration=3, train_loss=2.59148907661438\n",
      "Epoch=777, val_loss=2.5905649662017822\n",
      "Epoch=778, iteration=0, train_loss=2.3820157051086426\n",
      "Epoch=778, iteration=1, train_loss=2.392458200454712\n",
      "Epoch=778, iteration=2, train_loss=2.434027671813965\n",
      "Epoch=778, iteration=3, train_loss=2.5914008617401123\n",
      "Epoch=778, val_loss=2.5904858112335205\n",
      "Epoch=779, iteration=0, train_loss=2.381889581680298\n",
      "Epoch=779, iteration=1, train_loss=2.3923418521881104\n",
      "Epoch=779, iteration=2, train_loss=2.433927536010742\n",
      "Epoch=779, iteration=3, train_loss=2.591313123703003\n",
      "Epoch=779, val_loss=2.590406656265259\n",
      "Epoch=780, iteration=0, train_loss=2.381763219833374\n",
      "Epoch=780, iteration=1, train_loss=2.392225503921509\n",
      "Epoch=780, iteration=2, train_loss=2.4338276386260986\n",
      "Epoch=780, iteration=3, train_loss=2.5912253856658936\n",
      "Epoch=780, val_loss=2.590327739715576\n",
      "Epoch=781, iteration=0, train_loss=2.3816375732421875\n",
      "Epoch=781, iteration=1, train_loss=2.3921093940734863\n",
      "Epoch=781, iteration=2, train_loss=2.433727979660034\n",
      "Epoch=781, iteration=3, train_loss=2.5911378860473633\n",
      "Epoch=781, val_loss=2.5902485847473145\n",
      "Epoch=782, iteration=0, train_loss=2.381511688232422\n",
      "Epoch=782, iteration=1, train_loss=2.391993761062622\n",
      "Epoch=782, iteration=2, train_loss=2.4336278438568115\n",
      "Epoch=782, iteration=3, train_loss=2.591050386428833\n",
      "Epoch=782, val_loss=2.590169906616211\n",
      "Epoch=783, iteration=0, train_loss=2.3813865184783936\n",
      "Epoch=783, iteration=1, train_loss=2.3918776512145996\n",
      "Epoch=783, iteration=2, train_loss=2.4335286617279053\n",
      "Epoch=783, iteration=3, train_loss=2.590963125228882\n",
      "Epoch=783, val_loss=2.5900912284851074\n",
      "Epoch=784, iteration=0, train_loss=2.3812618255615234\n",
      "Epoch=784, iteration=1, train_loss=2.3917622566223145\n",
      "Epoch=784, iteration=2, train_loss=2.433429718017578\n",
      "Epoch=784, iteration=3, train_loss=2.5908758640289307\n",
      "Epoch=784, val_loss=2.590012788772583\n",
      "Epoch=785, iteration=0, train_loss=2.381136894226074\n",
      "Epoch=785, iteration=1, train_loss=2.3916471004486084\n",
      "Epoch=785, iteration=2, train_loss=2.433330774307251\n",
      "Epoch=785, iteration=3, train_loss=2.5907888412475586\n",
      "Epoch=785, val_loss=2.5899338722229004\n",
      "Epoch=786, iteration=0, train_loss=2.381012439727783\n",
      "Epoch=786, iteration=1, train_loss=2.3915319442749023\n",
      "Epoch=786, iteration=2, train_loss=2.433231830596924\n",
      "Epoch=786, iteration=3, train_loss=2.5907022953033447\n",
      "Epoch=786, val_loss=2.589855432510376\n",
      "Epoch=787, iteration=0, train_loss=2.3808884620666504\n",
      "Epoch=787, iteration=1, train_loss=2.3914167881011963\n",
      "Epoch=787, iteration=2, train_loss=2.433133602142334\n",
      "Epoch=787, iteration=3, train_loss=2.5906152725219727\n",
      "Epoch=787, val_loss=2.5897772312164307\n",
      "Epoch=788, iteration=0, train_loss=2.3807644844055176\n",
      "Epoch=788, iteration=1, train_loss=2.3913025856018066\n",
      "Epoch=788, iteration=2, train_loss=2.433034896850586\n",
      "Epoch=788, iteration=3, train_loss=2.590528726577759\n",
      "Epoch=788, val_loss=2.5896987915039062\n",
      "Epoch=789, iteration=0, train_loss=2.380640983581543\n",
      "Epoch=789, iteration=1, train_loss=2.391188144683838\n",
      "Epoch=789, iteration=2, train_loss=2.432936906814575\n",
      "Epoch=789, iteration=3, train_loss=2.590442419052124\n",
      "Epoch=789, val_loss=2.58962082862854\n",
      "Epoch=790, iteration=0, train_loss=2.3805177211761475\n",
      "Epoch=790, iteration=1, train_loss=2.3910739421844482\n",
      "Epoch=790, iteration=2, train_loss=2.4328389167785645\n",
      "Epoch=790, iteration=3, train_loss=2.5903561115264893\n",
      "Epoch=790, val_loss=2.5895426273345947\n",
      "Epoch=791, iteration=0, train_loss=2.380394697189331\n",
      "Epoch=791, iteration=1, train_loss=2.3909599781036377\n",
      "Epoch=791, iteration=2, train_loss=2.432741165161133\n",
      "Epoch=791, iteration=3, train_loss=2.5902695655822754\n",
      "Epoch=791, val_loss=2.5894649028778076\n",
      "Epoch=792, iteration=0, train_loss=2.380272388458252\n",
      "Epoch=792, iteration=1, train_loss=2.3908464908599854\n",
      "Epoch=792, iteration=2, train_loss=2.4326436519622803\n",
      "Epoch=792, iteration=3, train_loss=2.590183973312378\n",
      "Epoch=792, val_loss=2.5893867015838623\n",
      "Epoch=793, iteration=0, train_loss=2.380150318145752\n",
      "Epoch=793, iteration=1, train_loss=2.390732765197754\n",
      "Epoch=793, iteration=2, train_loss=2.432546377182007\n",
      "Epoch=793, iteration=3, train_loss=2.5900979042053223\n",
      "Epoch=793, val_loss=2.5893092155456543\n",
      "Epoch=794, iteration=0, train_loss=2.380028247833252\n",
      "Epoch=794, iteration=1, train_loss=2.3906197547912598\n",
      "Epoch=794, iteration=2, train_loss=2.4324493408203125\n",
      "Epoch=794, iteration=3, train_loss=2.590012550354004\n",
      "Epoch=794, val_loss=2.5892317295074463\n",
      "Epoch=795, iteration=0, train_loss=2.379906177520752\n",
      "Epoch=795, iteration=1, train_loss=2.3905065059661865\n",
      "Epoch=795, iteration=2, train_loss=2.432352304458618\n",
      "Epoch=795, iteration=3, train_loss=2.5899269580841064\n",
      "Epoch=795, val_loss=2.589154005050659\n",
      "Epoch=796, iteration=0, train_loss=2.3797850608825684\n",
      "Epoch=796, iteration=1, train_loss=2.3903937339782715\n",
      "Epoch=796, iteration=2, train_loss=2.432255506515503\n",
      "Epoch=796, iteration=3, train_loss=2.589841365814209\n",
      "Epoch=796, val_loss=2.589076280593872\n",
      "Epoch=797, iteration=0, train_loss=2.3796639442443848\n",
      "Epoch=797, iteration=1, train_loss=2.3902814388275146\n",
      "Epoch=797, iteration=2, train_loss=2.432159185409546\n",
      "Epoch=797, iteration=3, train_loss=2.5897557735443115\n",
      "Epoch=797, val_loss=2.588999032974243\n",
      "Epoch=798, iteration=0, train_loss=2.379542827606201\n",
      "Epoch=798, iteration=1, train_loss=2.3901686668395996\n",
      "Epoch=798, iteration=2, train_loss=2.432062864303589\n",
      "Epoch=798, iteration=3, train_loss=2.5896706581115723\n",
      "Epoch=798, val_loss=2.5889217853546143\n",
      "Epoch=799, iteration=0, train_loss=2.379422664642334\n",
      "Epoch=799, iteration=1, train_loss=2.390056610107422\n",
      "Epoch=799, iteration=2, train_loss=2.431966781616211\n",
      "Epoch=799, iteration=3, train_loss=2.589586019515991\n",
      "Epoch=799, val_loss=2.5888442993164062\n",
      "Epoch=800, iteration=0, train_loss=2.3793022632598877\n",
      "Epoch=800, iteration=1, train_loss=2.389944553375244\n",
      "Epoch=800, iteration=2, train_loss=2.431870937347412\n",
      "Epoch=800, iteration=3, train_loss=2.589501142501831\n",
      "Epoch=800, val_loss=2.5887670516967773\n",
      "Epoch=801, iteration=0, train_loss=2.379182815551758\n",
      "Epoch=801, iteration=1, train_loss=2.3898329734802246\n",
      "Epoch=801, iteration=2, train_loss=2.4317753314971924\n",
      "Epoch=801, iteration=3, train_loss=2.589416027069092\n",
      "Epoch=801, val_loss=2.5886905193328857\n",
      "Epoch=802, iteration=0, train_loss=2.3790628910064697\n",
      "Epoch=802, iteration=1, train_loss=2.4048986434936523\n",
      "Epoch=802, iteration=2, train_loss=2.4394772052764893\n",
      "Epoch=802, iteration=3, train_loss=2.564473867416382\n",
      "Epoch=802, val_loss=2.589700222015381\n",
      "Epoch=803, iteration=0, train_loss=2.374786853790283\n",
      "Epoch=803, iteration=1, train_loss=2.4003043174743652\n",
      "Epoch=803, iteration=2, train_loss=2.4366695880889893\n",
      "Epoch=803, iteration=3, train_loss=2.5667476654052734\n",
      "Epoch=803, val_loss=2.5911669731140137\n",
      "Epoch=804, iteration=0, train_loss=2.3722105026245117\n",
      "Epoch=804, iteration=1, train_loss=2.397465705871582\n",
      "Epoch=804, iteration=2, train_loss=2.4350829124450684\n",
      "Epoch=804, iteration=3, train_loss=2.5688130855560303\n",
      "Epoch=804, val_loss=2.592622756958008\n",
      "Epoch=805, iteration=0, train_loss=2.370556116104126\n",
      "Epoch=805, iteration=1, train_loss=2.395617961883545\n",
      "Epoch=805, iteration=2, train_loss=2.43414044380188\n",
      "Epoch=805, iteration=3, train_loss=2.570549726486206\n",
      "Epoch=805, val_loss=2.5939085483551025\n",
      "Epoch=806, iteration=0, train_loss=2.3694303035736084\n",
      "Epoch=806, iteration=1, train_loss=2.394357681274414\n",
      "Epoch=806, iteration=2, train_loss=2.433549642562866\n",
      "Epoch=806, iteration=3, train_loss=2.5719621181488037\n",
      "Epoch=806, val_loss=2.5949923992156982\n",
      "Epoch=807, iteration=0, train_loss=2.368624210357666\n",
      "Epoch=807, iteration=1, train_loss=2.393462657928467\n",
      "Epoch=807, iteration=2, train_loss=2.4331603050231934\n",
      "Epoch=807, iteration=3, train_loss=2.573096990585327\n",
      "Epoch=807, val_loss=2.5958874225616455\n",
      "Epoch=808, iteration=0, train_loss=2.3680222034454346\n",
      "Epoch=808, iteration=1, train_loss=2.39280366897583\n",
      "Epoch=808, iteration=2, train_loss=2.4328906536102295\n",
      "Epoch=808, iteration=3, train_loss=2.574007987976074\n",
      "Epoch=808, val_loss=2.5966241359710693\n",
      "Epoch=809, iteration=0, train_loss=2.3675575256347656\n",
      "Epoch=809, iteration=1, train_loss=2.3923046588897705\n",
      "Epoch=809, iteration=2, train_loss=2.4326961040496826\n",
      "Epoch=809, iteration=3, train_loss=2.5747430324554443\n",
      "Epoch=809, val_loss=2.597231149673462\n",
      "Epoch=810, iteration=0, train_loss=2.367189645767212\n",
      "Epoch=810, iteration=1, train_loss=2.3919167518615723\n",
      "Epoch=810, iteration=2, train_loss=2.432549476623535\n",
      "Epoch=810, iteration=3, train_loss=2.575340986251831\n",
      "Epoch=810, val_loss=2.597733974456787\n",
      "Epoch=811, iteration=0, train_loss=2.3668909072875977\n",
      "Epoch=811, iteration=1, train_loss=2.3916075229644775\n",
      "Epoch=811, iteration=2, train_loss=2.432436227798462\n",
      "Epoch=811, iteration=3, train_loss=2.5758326053619385\n",
      "Epoch=811, val_loss=2.598154067993164\n",
      "Epoch=812, iteration=0, train_loss=2.366645097732544\n",
      "Epoch=812, iteration=1, train_loss=2.391357421875\n",
      "Epoch=812, iteration=2, train_loss=2.432344913482666\n",
      "Epoch=812, iteration=3, train_loss=2.5762405395507812\n",
      "Epoch=812, val_loss=2.5985076427459717\n",
      "Epoch=813, iteration=0, train_loss=2.3664393424987793\n",
      "Epoch=813, iteration=1, train_loss=2.3911502361297607\n",
      "Epoch=813, iteration=2, train_loss=2.4322702884674072\n",
      "Epoch=813, iteration=3, train_loss=2.576582431793213\n",
      "Epoch=813, val_loss=2.5988078117370605\n",
      "Epoch=814, iteration=0, train_loss=2.366264820098877\n",
      "Epoch=814, iteration=1, train_loss=2.390976905822754\n",
      "Epoch=814, iteration=2, train_loss=2.4322071075439453\n",
      "Epoch=814, iteration=3, train_loss=2.5768721103668213\n",
      "Epoch=814, val_loss=2.5990641117095947\n",
      "Epoch=815, iteration=0, train_loss=2.3661153316497803\n",
      "Epoch=815, iteration=1, train_loss=2.390829563140869\n",
      "Epoch=815, iteration=2, train_loss=2.43215274810791\n",
      "Epoch=815, iteration=3, train_loss=2.5771188735961914\n",
      "Epoch=815, val_loss=2.5992846488952637\n",
      "Epoch=816, iteration=0, train_loss=2.365985631942749\n",
      "Epoch=816, iteration=1, train_loss=2.390702962875366\n",
      "Epoch=816, iteration=2, train_loss=2.432105779647827\n",
      "Epoch=816, iteration=3, train_loss=2.577331304550171\n",
      "Epoch=816, val_loss=2.599475622177124\n",
      "Epoch=817, iteration=0, train_loss=2.3658721446990967\n",
      "Epoch=817, iteration=1, train_loss=2.390592336654663\n",
      "Epoch=817, iteration=2, train_loss=2.4320640563964844\n",
      "Epoch=817, iteration=3, train_loss=2.5775156021118164\n",
      "Epoch=817, val_loss=2.599642276763916\n",
      "Epoch=818, iteration=0, train_loss=2.365772247314453\n",
      "Epoch=818, iteration=1, train_loss=2.3904950618743896\n",
      "Epoch=818, iteration=2, train_loss=2.4320261478424072\n",
      "Epoch=818, iteration=3, train_loss=2.577676296234131\n",
      "Epoch=818, val_loss=2.599787712097168\n",
      "Epoch=819, iteration=0, train_loss=2.365683078765869\n",
      "Epoch=819, iteration=1, train_loss=2.390409231185913\n",
      "Epoch=819, iteration=2, train_loss=2.4319915771484375\n",
      "Epoch=819, iteration=3, train_loss=2.577816963195801\n",
      "Epoch=819, val_loss=2.5999162197113037\n",
      "Epoch=820, iteration=0, train_loss=2.365603446960449\n",
      "Epoch=820, iteration=1, train_loss=2.390331983566284\n",
      "Epoch=820, iteration=2, train_loss=2.431960344314575\n",
      "Epoch=820, iteration=3, train_loss=2.5779411792755127\n",
      "Epoch=820, val_loss=2.600029706954956\n",
      "Epoch=821, iteration=0, train_loss=2.365532159805298\n",
      "Epoch=821, iteration=1, train_loss=2.3902626037597656\n",
      "Epoch=821, iteration=2, train_loss=2.431931257247925\n",
      "Epoch=821, iteration=3, train_loss=2.5780510902404785\n",
      "Epoch=821, val_loss=2.6001298427581787\n",
      "Epoch=822, iteration=0, train_loss=2.365467071533203\n",
      "Epoch=822, iteration=1, train_loss=2.3901991844177246\n",
      "Epoch=822, iteration=2, train_loss=2.431903839111328\n",
      "Epoch=822, iteration=3, train_loss=2.5781490802764893\n",
      "Epoch=822, val_loss=2.600219249725342\n",
      "Epoch=823, iteration=0, train_loss=2.365407943725586\n",
      "Epoch=823, iteration=1, train_loss=2.390141725540161\n",
      "Epoch=823, iteration=2, train_loss=2.431877613067627\n",
      "Epoch=823, iteration=3, train_loss=2.5782358646392822\n",
      "Epoch=823, val_loss=2.600299119949341\n",
      "Epoch=824, iteration=0, train_loss=2.3653531074523926\n",
      "Epoch=824, iteration=1, train_loss=2.3900887966156006\n",
      "Epoch=824, iteration=2, train_loss=2.431854009628296\n",
      "Epoch=824, iteration=3, train_loss=2.5783138275146484\n",
      "Epoch=824, val_loss=2.600370407104492\n",
      "Epoch=825, iteration=0, train_loss=2.3653032779693604\n",
      "Epoch=825, iteration=1, train_loss=2.3900399208068848\n",
      "Epoch=825, iteration=2, train_loss=2.431830883026123\n",
      "Epoch=825, iteration=3, train_loss=2.5783839225769043\n",
      "Epoch=825, val_loss=2.600435256958008\n",
      "Epoch=826, iteration=0, train_loss=2.3652567863464355\n",
      "Epoch=826, iteration=1, train_loss=2.3899948596954346\n",
      "Epoch=826, iteration=2, train_loss=2.4318089485168457\n",
      "Epoch=826, iteration=3, train_loss=2.578447103500366\n",
      "Epoch=826, val_loss=2.6004927158355713\n",
      "Epoch=827, iteration=0, train_loss=2.36521315574646\n",
      "Epoch=827, iteration=1, train_loss=2.3899526596069336\n",
      "Epoch=827, iteration=2, train_loss=2.431788206100464\n",
      "Epoch=827, iteration=3, train_loss=2.5785040855407715\n",
      "Epoch=827, val_loss=2.6005449295043945\n",
      "Epoch=828, iteration=0, train_loss=2.3651726245880127\n",
      "Epoch=828, iteration=1, train_loss=2.389913320541382\n",
      "Epoch=828, iteration=2, train_loss=2.4317679405212402\n",
      "Epoch=828, iteration=3, train_loss=2.5785560607910156\n",
      "Epoch=828, val_loss=2.600592613220215\n",
      "Epoch=829, iteration=0, train_loss=2.3651347160339355\n",
      "Epoch=829, iteration=1, train_loss=2.389876127243042\n",
      "Epoch=829, iteration=2, train_loss=2.431748867034912\n",
      "Epoch=829, iteration=3, train_loss=2.5786027908325195\n",
      "Epoch=829, val_loss=2.600635290145874\n",
      "Epoch=830, iteration=0, train_loss=2.365098714828491\n",
      "Epoch=830, iteration=1, train_loss=2.389841079711914\n",
      "Epoch=830, iteration=2, train_loss=2.431730270385742\n",
      "Epoch=830, iteration=3, train_loss=2.5786452293395996\n",
      "Epoch=830, val_loss=2.6006739139556885\n",
      "Epoch=831, iteration=0, train_loss=2.3650643825531006\n",
      "Epoch=831, iteration=1, train_loss=2.389807939529419\n",
      "Epoch=831, iteration=2, train_loss=2.4317119121551514\n",
      "Epoch=831, iteration=3, train_loss=2.578683614730835\n",
      "Epoch=831, val_loss=2.6007096767425537\n",
      "Epoch=832, iteration=0, train_loss=2.365032196044922\n",
      "Epoch=832, iteration=1, train_loss=2.3897767066955566\n",
      "Epoch=832, iteration=2, train_loss=2.431694746017456\n",
      "Epoch=832, iteration=3, train_loss=2.5787193775177\n",
      "Epoch=832, val_loss=2.6007416248321533\n",
      "Epoch=833, iteration=0, train_loss=2.365001678466797\n",
      "Epoch=833, iteration=1, train_loss=2.389746904373169\n",
      "Epoch=833, iteration=2, train_loss=2.43167781829834\n",
      "Epoch=833, iteration=3, train_loss=2.5787513256073\n",
      "Epoch=833, val_loss=2.600771188735962\n",
      "Epoch=834, iteration=0, train_loss=2.3649721145629883\n",
      "Epoch=834, iteration=1, train_loss=2.3897180557250977\n",
      "Epoch=834, iteration=2, train_loss=2.4316608905792236\n",
      "Epoch=834, iteration=3, train_loss=2.5787806510925293\n",
      "Epoch=834, val_loss=2.6007978916168213\n",
      "Epoch=835, iteration=0, train_loss=2.3649439811706543\n",
      "Epoch=835, iteration=1, train_loss=2.389690637588501\n",
      "Epoch=835, iteration=2, train_loss=2.431644916534424\n",
      "Epoch=835, iteration=3, train_loss=2.5788075923919678\n",
      "Epoch=835, val_loss=2.600822687149048\n",
      "Epoch=836, iteration=0, train_loss=2.3649168014526367\n",
      "Epoch=836, iteration=1, train_loss=2.389664888381958\n",
      "Epoch=836, iteration=2, train_loss=2.431629180908203\n",
      "Epoch=836, iteration=3, train_loss=2.578831911087036\n",
      "Epoch=836, val_loss=2.600844621658325\n",
      "Epoch=837, iteration=0, train_loss=2.3648903369903564\n",
      "Epoch=837, iteration=1, train_loss=2.389639139175415\n",
      "Epoch=837, iteration=2, train_loss=2.4316141605377197\n",
      "Epoch=837, iteration=3, train_loss=2.57885479927063\n",
      "Epoch=837, val_loss=2.600865364074707\n",
      "Epoch=838, iteration=0, train_loss=2.364865303039551\n",
      "Epoch=838, iteration=1, train_loss=2.389615058898926\n",
      "Epoch=838, iteration=2, train_loss=2.4315989017486572\n",
      "Epoch=838, iteration=3, train_loss=2.5788755416870117\n",
      "Epoch=838, val_loss=2.600884199142456\n",
      "Epoch=839, iteration=0, train_loss=2.364840507507324\n",
      "Epoch=839, iteration=1, train_loss=2.389591693878174\n",
      "Epoch=839, iteration=2, train_loss=2.431583881378174\n",
      "Epoch=839, iteration=3, train_loss=2.5788943767547607\n",
      "Epoch=839, val_loss=2.6009011268615723\n",
      "Epoch=840, iteration=0, train_loss=2.3648171424865723\n",
      "Epoch=840, iteration=1, train_loss=2.389569044113159\n",
      "Epoch=840, iteration=2, train_loss=2.4315693378448486\n",
      "Epoch=840, iteration=3, train_loss=2.578911542892456\n",
      "Epoch=840, val_loss=2.600917100906372\n",
      "Epoch=841, iteration=0, train_loss=2.3647942543029785\n",
      "Epoch=841, iteration=1, train_loss=2.3895468711853027\n",
      "Epoch=841, iteration=2, train_loss=2.4315555095672607\n",
      "Epoch=841, iteration=3, train_loss=2.578927755355835\n",
      "Epoch=841, val_loss=2.6009316444396973\n",
      "Epoch=842, iteration=0, train_loss=2.3647713661193848\n",
      "Epoch=842, iteration=1, train_loss=2.3895254135131836\n",
      "Epoch=842, iteration=2, train_loss=2.4315412044525146\n",
      "Epoch=842, iteration=3, train_loss=2.578942060470581\n",
      "Epoch=842, val_loss=2.600944757461548\n",
      "Epoch=843, iteration=0, train_loss=2.3647496700286865\n",
      "Epoch=843, iteration=1, train_loss=2.3895044326782227\n",
      "Epoch=843, iteration=2, train_loss=2.431527853012085\n",
      "Epoch=843, iteration=3, train_loss=2.5789551734924316\n",
      "Epoch=843, val_loss=2.600956439971924\n",
      "Epoch=844, iteration=0, train_loss=2.3647282123565674\n",
      "Epoch=844, iteration=1, train_loss=2.389484405517578\n",
      "Epoch=844, iteration=2, train_loss=2.431513786315918\n",
      "Epoch=844, iteration=3, train_loss=2.578967809677124\n",
      "Epoch=844, val_loss=2.6009676456451416\n",
      "Epoch=845, iteration=0, train_loss=2.3647074699401855\n",
      "Epoch=845, iteration=1, train_loss=2.3894643783569336\n",
      "Epoch=845, iteration=2, train_loss=2.4315009117126465\n",
      "Epoch=845, iteration=3, train_loss=2.5789787769317627\n",
      "Epoch=845, val_loss=2.600977659225464\n",
      "Epoch=846, iteration=0, train_loss=2.364687204360962\n",
      "Epoch=846, iteration=1, train_loss=2.3894453048706055\n",
      "Epoch=846, iteration=2, train_loss=2.431487560272217\n",
      "Epoch=846, iteration=3, train_loss=2.578988790512085\n",
      "Epoch=846, val_loss=2.6009867191314697\n",
      "Epoch=847, iteration=0, train_loss=2.3646674156188965\n",
      "Epoch=847, iteration=1, train_loss=2.3894262313842773\n",
      "Epoch=847, iteration=2, train_loss=2.431474447250366\n",
      "Epoch=847, iteration=3, train_loss=2.578998327255249\n",
      "Epoch=847, val_loss=2.6009950637817383\n",
      "Epoch=848, iteration=0, train_loss=2.364647388458252\n",
      "Epoch=848, iteration=1, train_loss=2.3894073963165283\n",
      "Epoch=848, iteration=2, train_loss=2.4314613342285156\n",
      "Epoch=848, iteration=3, train_loss=2.5790066719055176\n",
      "Epoch=848, val_loss=2.6010026931762695\n",
      "Epoch=849, iteration=0, train_loss=2.3646280765533447\n",
      "Epoch=849, iteration=1, train_loss=2.3893895149230957\n",
      "Epoch=849, iteration=2, train_loss=2.4314486980438232\n",
      "Epoch=849, iteration=3, train_loss=2.579014539718628\n",
      "Epoch=849, val_loss=2.6010093688964844\n",
      "Epoch=850, iteration=0, train_loss=2.3646092414855957\n",
      "Epoch=850, iteration=1, train_loss=2.389371633529663\n",
      "Epoch=850, iteration=2, train_loss=2.431436061859131\n",
      "Epoch=850, iteration=3, train_loss=2.579021453857422\n",
      "Epoch=850, val_loss=2.601015567779541\n",
      "Epoch=851, iteration=0, train_loss=2.3645904064178467\n",
      "Epoch=851, iteration=1, train_loss=2.3893539905548096\n",
      "Epoch=851, iteration=2, train_loss=2.4314234256744385\n",
      "Epoch=851, iteration=3, train_loss=2.5790276527404785\n",
      "Epoch=851, val_loss=2.6010210514068604\n",
      "Epoch=852, iteration=0, train_loss=2.364572286605835\n",
      "Epoch=852, iteration=1, train_loss=2.3893368244171143\n",
      "Epoch=852, iteration=2, train_loss=2.4314112663269043\n",
      "Epoch=852, iteration=3, train_loss=2.579033374786377\n",
      "Epoch=852, val_loss=2.6010262966156006\n",
      "Epoch=853, iteration=0, train_loss=2.364553928375244\n",
      "Epoch=853, iteration=1, train_loss=2.389320135116577\n",
      "Epoch=853, iteration=2, train_loss=2.43139910697937\n",
      "Epoch=853, iteration=3, train_loss=2.579038381576538\n",
      "Epoch=853, val_loss=2.601030111312866\n",
      "Epoch=854, iteration=0, train_loss=2.3645360469818115\n",
      "Epoch=854, iteration=1, train_loss=2.389303207397461\n",
      "Epoch=854, iteration=2, train_loss=2.431387186050415\n",
      "Epoch=854, iteration=3, train_loss=2.579042673110962\n",
      "Epoch=854, val_loss=2.601034164428711\n",
      "Epoch=855, iteration=0, train_loss=2.364518165588379\n",
      "Epoch=855, iteration=1, train_loss=2.389286518096924\n",
      "Epoch=855, iteration=2, train_loss=2.4313747882843018\n",
      "Epoch=855, iteration=3, train_loss=2.5790469646453857\n",
      "Epoch=855, val_loss=2.6010379791259766\n",
      "Epoch=856, iteration=0, train_loss=2.3645007610321045\n",
      "Epoch=856, iteration=1, train_loss=2.389270305633545\n",
      "Epoch=856, iteration=2, train_loss=2.4313628673553467\n",
      "Epoch=856, iteration=3, train_loss=2.579050064086914\n",
      "Epoch=856, val_loss=2.601040840148926\n",
      "Epoch=857, iteration=0, train_loss=2.36448335647583\n",
      "Epoch=857, iteration=1, train_loss=2.389254570007324\n",
      "Epoch=857, iteration=2, train_loss=2.43135142326355\n",
      "Epoch=857, iteration=3, train_loss=2.5790536403656006\n",
      "Epoch=857, val_loss=2.601043462753296\n",
      "Epoch=858, iteration=0, train_loss=2.3644661903381348\n",
      "Epoch=858, iteration=1, train_loss=2.3892385959625244\n",
      "Epoch=858, iteration=2, train_loss=2.4313395023345947\n",
      "Epoch=858, iteration=3, train_loss=2.5790560245513916\n",
      "Epoch=858, val_loss=2.601045608520508\n",
      "Epoch=859, iteration=0, train_loss=2.3644492626190186\n",
      "Epoch=859, iteration=1, train_loss=2.389223098754883\n",
      "Epoch=859, iteration=2, train_loss=2.4313275814056396\n",
      "Epoch=859, iteration=3, train_loss=2.5790584087371826\n",
      "Epoch=859, val_loss=2.6010475158691406\n",
      "Epoch=860, iteration=0, train_loss=2.3644323348999023\n",
      "Epoch=860, iteration=1, train_loss=2.389207601547241\n",
      "Epoch=860, iteration=2, train_loss=2.4313161373138428\n",
      "Epoch=860, iteration=3, train_loss=2.5790600776672363\n",
      "Epoch=860, val_loss=2.6010489463806152\n",
      "Epoch=861, iteration=0, train_loss=2.3644158840179443\n",
      "Epoch=861, iteration=1, train_loss=2.3891918659210205\n",
      "Epoch=861, iteration=2, train_loss=2.431304931640625\n",
      "Epoch=861, iteration=3, train_loss=2.579061269760132\n",
      "Epoch=861, val_loss=2.6010499000549316\n",
      "Epoch=862, iteration=0, train_loss=2.3643996715545654\n",
      "Epoch=862, iteration=1, train_loss=2.389176845550537\n",
      "Epoch=862, iteration=2, train_loss=2.431293249130249\n",
      "Epoch=862, iteration=3, train_loss=2.5790629386901855\n",
      "Epoch=862, val_loss=2.601051092147827\n",
      "Epoch=863, iteration=0, train_loss=2.3643832206726074\n",
      "Epoch=863, iteration=1, train_loss=2.3891618251800537\n",
      "Epoch=863, iteration=2, train_loss=2.431281805038452\n",
      "Epoch=863, iteration=3, train_loss=2.5790634155273438\n",
      "Epoch=863, val_loss=2.6010513305664062\n",
      "Epoch=864, iteration=0, train_loss=2.3643670082092285\n",
      "Epoch=864, iteration=1, train_loss=2.3891472816467285\n",
      "Epoch=864, iteration=2, train_loss=2.4312705993652344\n",
      "Epoch=864, iteration=3, train_loss=2.579063892364502\n",
      "Epoch=864, val_loss=2.6010513305664062\n",
      "Epoch=865, iteration=0, train_loss=2.3643510341644287\n",
      "Epoch=865, iteration=1, train_loss=2.389132261276245\n",
      "Epoch=865, iteration=2, train_loss=2.4312593936920166\n",
      "Epoch=865, iteration=3, train_loss=2.579064130783081\n",
      "Epoch=865, val_loss=2.6010515689849854\n",
      "Epoch=866, iteration=0, train_loss=2.364335298538208\n",
      "Epoch=866, iteration=1, train_loss=2.389117956161499\n",
      "Epoch=866, iteration=2, train_loss=2.431248188018799\n",
      "Epoch=866, iteration=3, train_loss=2.579064130783081\n",
      "Epoch=866, val_loss=2.6010515689849854\n",
      "Epoch=867, iteration=0, train_loss=2.364319324493408\n",
      "Epoch=867, iteration=1, train_loss=2.3891029357910156\n",
      "Epoch=867, iteration=2, train_loss=2.431236982345581\n",
      "Epoch=867, iteration=3, train_loss=2.579063892364502\n",
      "Epoch=867, val_loss=2.601050615310669\n",
      "Epoch=868, iteration=0, train_loss=2.3643035888671875\n",
      "Epoch=868, iteration=1, train_loss=2.3890891075134277\n",
      "Epoch=868, iteration=2, train_loss=2.4312260150909424\n",
      "Epoch=868, iteration=3, train_loss=2.5790631771087646\n",
      "Epoch=868, val_loss=2.6010501384735107\n",
      "Epoch=869, iteration=0, train_loss=2.364287853240967\n",
      "Epoch=869, iteration=1, train_loss=2.3890745639801025\n",
      "Epoch=869, iteration=2, train_loss=2.4312148094177246\n",
      "Epoch=869, iteration=3, train_loss=2.5790622234344482\n",
      "Epoch=869, val_loss=2.6010491847991943\n",
      "Epoch=870, iteration=0, train_loss=2.364272356033325\n",
      "Epoch=870, iteration=1, train_loss=2.3890602588653564\n",
      "Epoch=870, iteration=2, train_loss=2.431203842163086\n",
      "Epoch=870, iteration=3, train_loss=2.5790610313415527\n",
      "Epoch=870, val_loss=2.601047992706299\n",
      "Epoch=871, iteration=0, train_loss=2.3642568588256836\n",
      "Epoch=871, iteration=1, train_loss=2.3890459537506104\n",
      "Epoch=871, iteration=2, train_loss=2.4311931133270264\n",
      "Epoch=871, iteration=3, train_loss=2.5790598392486572\n",
      "Epoch=871, val_loss=2.601046562194824\n",
      "Epoch=872, iteration=0, train_loss=2.3642418384552\n",
      "Epoch=872, iteration=1, train_loss=2.3890321254730225\n",
      "Epoch=872, iteration=2, train_loss=2.4311819076538086\n",
      "Epoch=872, iteration=3, train_loss=2.5790584087371826\n",
      "Epoch=872, val_loss=2.6010451316833496\n",
      "Epoch=873, iteration=0, train_loss=2.3642265796661377\n",
      "Epoch=873, iteration=1, train_loss=2.3890182971954346\n",
      "Epoch=873, iteration=2, train_loss=2.431171178817749\n",
      "Epoch=873, iteration=3, train_loss=2.57905650138855\n",
      "Epoch=873, val_loss=2.601043462753296\n",
      "Epoch=874, iteration=0, train_loss=2.364211320877075\n",
      "Epoch=874, iteration=1, train_loss=2.389004707336426\n",
      "Epoch=874, iteration=2, train_loss=2.4311604499816895\n",
      "Epoch=874, iteration=3, train_loss=2.579055070877075\n",
      "Epoch=874, val_loss=2.601041555404663\n",
      "Epoch=875, iteration=0, train_loss=2.364196538925171\n",
      "Epoch=875, iteration=1, train_loss=2.388990879058838\n",
      "Epoch=875, iteration=2, train_loss=2.4311492443084717\n",
      "Epoch=875, iteration=3, train_loss=2.5790531635284424\n",
      "Epoch=875, val_loss=2.6010396480560303\n",
      "Epoch=876, iteration=0, train_loss=2.3641815185546875\n",
      "Epoch=876, iteration=1, train_loss=2.388977289199829\n",
      "Epoch=876, iteration=2, train_loss=2.431138515472412\n",
      "Epoch=876, iteration=3, train_loss=2.5790505409240723\n",
      "Epoch=876, val_loss=2.60103702545166\n",
      "Epoch=877, iteration=0, train_loss=2.364166498184204\n",
      "Epoch=877, iteration=1, train_loss=2.388963460922241\n",
      "Epoch=877, iteration=2, train_loss=2.4311280250549316\n",
      "Epoch=877, iteration=3, train_loss=2.5790486335754395\n",
      "Epoch=877, val_loss=2.6010348796844482\n",
      "Epoch=878, iteration=0, train_loss=2.3641517162323\n",
      "Epoch=878, iteration=1, train_loss=2.3889501094818115\n",
      "Epoch=878, iteration=2, train_loss=2.431117296218872\n",
      "Epoch=878, iteration=3, train_loss=2.5790460109710693\n",
      "Epoch=878, val_loss=2.6010324954986572\n",
      "Epoch=879, iteration=0, train_loss=2.3641366958618164\n",
      "Epoch=879, iteration=1, train_loss=2.3889365196228027\n",
      "Epoch=879, iteration=2, train_loss=2.4311065673828125\n",
      "Epoch=879, iteration=3, train_loss=2.57904314994812\n",
      "Epoch=879, val_loss=2.601029872894287\n",
      "Epoch=880, iteration=0, train_loss=2.3641223907470703\n",
      "Epoch=880, iteration=1, train_loss=2.388923406600952\n",
      "Epoch=880, iteration=2, train_loss=2.431095838546753\n",
      "Epoch=880, iteration=3, train_loss=2.579040288925171\n",
      "Epoch=880, val_loss=2.601027011871338\n",
      "Epoch=881, iteration=0, train_loss=2.364107847213745\n",
      "Epoch=881, iteration=1, train_loss=2.3889100551605225\n",
      "Epoch=881, iteration=2, train_loss=2.4310855865478516\n",
      "Epoch=881, iteration=3, train_loss=2.579037666320801\n",
      "Epoch=881, val_loss=2.6010241508483887\n",
      "Epoch=882, iteration=0, train_loss=2.364093065261841\n",
      "Epoch=882, iteration=1, train_loss=2.3888967037200928\n",
      "Epoch=882, iteration=2, train_loss=2.431075096130371\n",
      "Epoch=882, iteration=3, train_loss=2.5790345668792725\n",
      "Epoch=882, val_loss=2.6010208129882812\n",
      "Epoch=883, iteration=0, train_loss=2.3640785217285156\n",
      "Epoch=883, iteration=1, train_loss=2.3888838291168213\n",
      "Epoch=883, iteration=2, train_loss=2.4310643672943115\n",
      "Epoch=883, iteration=3, train_loss=2.579030990600586\n",
      "Epoch=883, val_loss=2.601017951965332\n",
      "Epoch=884, iteration=0, train_loss=2.3640639781951904\n",
      "Epoch=884, iteration=1, train_loss=2.3888704776763916\n",
      "Epoch=884, iteration=2, train_loss=2.431053876876831\n",
      "Epoch=884, iteration=3, train_loss=2.5790276527404785\n",
      "Epoch=884, val_loss=2.6010146141052246\n",
      "Epoch=885, iteration=0, train_loss=2.3640494346618652\n",
      "Epoch=885, iteration=1, train_loss=2.38885760307312\n",
      "Epoch=885, iteration=2, train_loss=2.4310436248779297\n",
      "Epoch=885, iteration=3, train_loss=2.579024314880371\n",
      "Epoch=885, val_loss=2.601011276245117\n",
      "Epoch=886, iteration=0, train_loss=2.3640356063842773\n",
      "Epoch=886, iteration=1, train_loss=2.3888444900512695\n",
      "Epoch=886, iteration=2, train_loss=2.43103289604187\n",
      "Epoch=886, iteration=3, train_loss=2.5790209770202637\n",
      "Epoch=886, val_loss=2.6010076999664307\n",
      "Epoch=887, iteration=0, train_loss=2.364020824432373\n",
      "Epoch=887, iteration=1, train_loss=2.388831615447998\n",
      "Epoch=887, iteration=2, train_loss=2.4310226440429688\n",
      "Epoch=887, iteration=3, train_loss=2.579017162322998\n",
      "Epoch=887, val_loss=2.6010046005249023\n",
      "Epoch=888, iteration=0, train_loss=2.364006757736206\n",
      "Epoch=888, iteration=1, train_loss=2.3888187408447266\n",
      "Epoch=888, iteration=2, train_loss=2.4310121536254883\n",
      "Epoch=888, iteration=3, train_loss=2.5790133476257324\n",
      "Epoch=888, val_loss=2.6010007858276367\n",
      "Epoch=889, iteration=0, train_loss=2.36399245262146\n",
      "Epoch=889, iteration=1, train_loss=2.388805627822876\n",
      "Epoch=889, iteration=2, train_loss=2.431001901626587\n",
      "Epoch=889, iteration=3, train_loss=2.5790092945098877\n",
      "Epoch=889, val_loss=2.600996971130371\n",
      "Epoch=890, iteration=0, train_loss=2.363978385925293\n",
      "Epoch=890, iteration=1, train_loss=2.3887929916381836\n",
      "Epoch=890, iteration=2, train_loss=2.4309911727905273\n",
      "Epoch=890, iteration=3, train_loss=2.579005479812622\n",
      "Epoch=890, val_loss=2.6009931564331055\n",
      "Epoch=891, iteration=0, train_loss=2.363964080810547\n",
      "Epoch=891, iteration=1, train_loss=2.388779878616333\n",
      "Epoch=891, iteration=2, train_loss=2.430980920791626\n",
      "Epoch=891, iteration=3, train_loss=2.5790011882781982\n",
      "Epoch=891, val_loss=2.6009891033172607\n",
      "Epoch=892, iteration=0, train_loss=2.363950252532959\n",
      "Epoch=892, iteration=1, train_loss=2.3887674808502197\n",
      "Epoch=892, iteration=2, train_loss=2.4309706687927246\n",
      "Epoch=892, iteration=3, train_loss=2.5789971351623535\n",
      "Epoch=892, val_loss=2.600985050201416\n",
      "Epoch=893, iteration=0, train_loss=2.363936185836792\n",
      "Epoch=893, iteration=1, train_loss=2.3887546062469482\n",
      "Epoch=893, iteration=2, train_loss=2.430960178375244\n",
      "Epoch=893, iteration=3, train_loss=2.5789928436279297\n",
      "Epoch=893, val_loss=2.6009809970855713\n",
      "Epoch=894, iteration=0, train_loss=2.363922119140625\n",
      "Epoch=894, iteration=1, train_loss=2.388741970062256\n",
      "Epoch=894, iteration=2, train_loss=2.430950164794922\n",
      "Epoch=894, iteration=3, train_loss=2.578988552093506\n",
      "Epoch=894, val_loss=2.6009767055511475\n",
      "Epoch=895, iteration=0, train_loss=2.363908290863037\n",
      "Epoch=895, iteration=1, train_loss=2.3887293338775635\n",
      "Epoch=895, iteration=2, train_loss=2.4309396743774414\n",
      "Epoch=895, iteration=3, train_loss=2.578984022140503\n",
      "Epoch=895, val_loss=2.6009726524353027\n",
      "Epoch=896, iteration=0, train_loss=2.36389422416687\n",
      "Epoch=896, iteration=1, train_loss=2.388716697692871\n",
      "Epoch=896, iteration=2, train_loss=2.430929660797119\n",
      "Epoch=896, iteration=3, train_loss=2.5789794921875\n",
      "Epoch=896, val_loss=2.6009681224823\n",
      "Epoch=897, iteration=0, train_loss=2.3638806343078613\n",
      "Epoch=897, iteration=1, train_loss=2.388704299926758\n",
      "Epoch=897, iteration=2, train_loss=2.4309191703796387\n",
      "Epoch=897, iteration=3, train_loss=2.578974723815918\n",
      "Epoch=897, val_loss=2.600963830947876\n",
      "Epoch=898, iteration=0, train_loss=2.3638663291931152\n",
      "Epoch=898, iteration=1, train_loss=2.3886916637420654\n",
      "Epoch=898, iteration=2, train_loss=2.4309091567993164\n",
      "Epoch=898, iteration=3, train_loss=2.578970193862915\n",
      "Epoch=898, val_loss=2.600959300994873\n",
      "Epoch=899, iteration=0, train_loss=2.3638529777526855\n",
      "Epoch=899, iteration=1, train_loss=2.388679265975952\n",
      "Epoch=899, iteration=2, train_loss=2.430898904800415\n",
      "Epoch=899, iteration=3, train_loss=2.578965663909912\n",
      "Epoch=899, val_loss=2.60095477104187\n",
      "Epoch=900, iteration=0, train_loss=2.3638391494750977\n",
      "Epoch=900, iteration=1, train_loss=2.388666868209839\n",
      "Epoch=900, iteration=2, train_loss=2.4308886528015137\n",
      "Epoch=900, iteration=3, train_loss=2.57896089553833\n",
      "Epoch=900, val_loss=2.6009504795074463\n",
      "Epoch=901, iteration=0, train_loss=2.3638250827789307\n",
      "Epoch=901, iteration=1, train_loss=2.3886544704437256\n",
      "Epoch=901, iteration=2, train_loss=2.4308786392211914\n",
      "Epoch=901, iteration=3, train_loss=2.578955888748169\n",
      "Epoch=901, val_loss=2.6009457111358643\n",
      "Epoch=902, iteration=0, train_loss=2.363811492919922\n",
      "Epoch=902, iteration=1, train_loss=2.3886420726776123\n",
      "Epoch=902, iteration=2, train_loss=2.43086838722229\n",
      "Epoch=902, iteration=3, train_loss=2.578950881958008\n",
      "Epoch=902, val_loss=2.6009409427642822\n",
      "Epoch=903, iteration=0, train_loss=2.363797903060913\n",
      "Epoch=903, iteration=1, train_loss=2.388629674911499\n",
      "Epoch=903, iteration=2, train_loss=2.4308581352233887\n",
      "Epoch=903, iteration=3, train_loss=2.5789458751678467\n",
      "Epoch=903, val_loss=2.6009361743927\n",
      "Epoch=904, iteration=0, train_loss=2.3637845516204834\n",
      "Epoch=904, iteration=1, train_loss=2.388617515563965\n",
      "Epoch=904, iteration=2, train_loss=2.4308481216430664\n",
      "Epoch=904, iteration=3, train_loss=2.5789411067962646\n",
      "Epoch=904, val_loss=2.6009316444396973\n",
      "Epoch=905, iteration=0, train_loss=2.3637704849243164\n",
      "Epoch=905, iteration=1, train_loss=2.3886048793792725\n",
      "Epoch=905, iteration=2, train_loss=2.430838108062744\n",
      "Epoch=905, iteration=3, train_loss=2.5789358615875244\n",
      "Epoch=905, val_loss=2.600926637649536\n",
      "Epoch=906, iteration=0, train_loss=2.3637571334838867\n",
      "Epoch=906, iteration=1, train_loss=2.3885927200317383\n",
      "Epoch=906, iteration=2, train_loss=2.4308276176452637\n",
      "Epoch=906, iteration=3, train_loss=2.5789308547973633\n",
      "Epoch=906, val_loss=2.600921869277954\n",
      "Epoch=907, iteration=0, train_loss=2.363743543624878\n",
      "Epoch=907, iteration=1, train_loss=2.388580322265625\n",
      "Epoch=907, iteration=2, train_loss=2.4308178424835205\n",
      "Epoch=907, iteration=3, train_loss=2.578925371170044\n",
      "Epoch=907, val_loss=2.600916862487793\n",
      "Epoch=908, iteration=0, train_loss=2.363729953765869\n",
      "Epoch=908, iteration=1, train_loss=2.3885679244995117\n",
      "Epoch=908, iteration=2, train_loss=2.430807590484619\n",
      "Epoch=908, iteration=3, train_loss=2.5789201259613037\n",
      "Epoch=908, val_loss=2.6009116172790527\n",
      "Epoch=909, iteration=0, train_loss=2.3637166023254395\n",
      "Epoch=909, iteration=1, train_loss=2.3885562419891357\n",
      "Epoch=909, iteration=2, train_loss=2.430797576904297\n",
      "Epoch=909, iteration=3, train_loss=2.5789148807525635\n",
      "Epoch=909, val_loss=2.6009066104888916\n",
      "Epoch=910, iteration=0, train_loss=2.3637030124664307\n",
      "Epoch=910, iteration=1, train_loss=2.3885438442230225\n",
      "Epoch=910, iteration=2, train_loss=2.4307878017425537\n",
      "Epoch=910, iteration=3, train_loss=2.5789096355438232\n",
      "Epoch=910, val_loss=2.6009013652801514\n",
      "Epoch=911, iteration=0, train_loss=2.363689661026001\n",
      "Epoch=911, iteration=1, train_loss=2.388531446456909\n",
      "Epoch=911, iteration=2, train_loss=2.4307777881622314\n",
      "Epoch=911, iteration=3, train_loss=2.578904151916504\n",
      "Epoch=911, val_loss=2.6008963584899902\n",
      "Epoch=912, iteration=0, train_loss=2.363676071166992\n",
      "Epoch=912, iteration=1, train_loss=2.388519525527954\n",
      "Epoch=912, iteration=2, train_loss=2.43076753616333\n",
      "Epoch=912, iteration=3, train_loss=2.5788986682891846\n",
      "Epoch=912, val_loss=2.60089111328125\n",
      "Epoch=913, iteration=0, train_loss=2.3636627197265625\n",
      "Epoch=913, iteration=1, train_loss=2.388507604598999\n",
      "Epoch=913, iteration=2, train_loss=2.430757522583008\n",
      "Epoch=913, iteration=3, train_loss=2.578892946243286\n",
      "Epoch=913, val_loss=2.6008856296539307\n",
      "Epoch=914, iteration=0, train_loss=2.363649606704712\n",
      "Epoch=914, iteration=1, train_loss=2.388495445251465\n",
      "Epoch=914, iteration=2, train_loss=2.4307475090026855\n",
      "Epoch=914, iteration=3, train_loss=2.578887701034546\n",
      "Epoch=914, val_loss=2.6008806228637695\n",
      "Epoch=915, iteration=0, train_loss=2.363636016845703\n",
      "Epoch=915, iteration=1, train_loss=2.3884832859039307\n",
      "Epoch=915, iteration=2, train_loss=2.4307374954223633\n",
      "Epoch=915, iteration=3, train_loss=2.5788817405700684\n",
      "Epoch=915, val_loss=2.60087513923645\n",
      "Epoch=916, iteration=0, train_loss=2.3636229038238525\n",
      "Epoch=916, iteration=1, train_loss=2.3884711265563965\n",
      "Epoch=916, iteration=2, train_loss=2.430727481842041\n",
      "Epoch=916, iteration=3, train_loss=2.57887601852417\n",
      "Epoch=916, val_loss=2.600869655609131\n",
      "Epoch=917, iteration=0, train_loss=2.3636093139648438\n",
      "Epoch=917, iteration=1, train_loss=2.3884592056274414\n",
      "Epoch=917, iteration=2, train_loss=2.4307174682617188\n",
      "Epoch=917, iteration=3, train_loss=2.5788705348968506\n",
      "Epoch=917, val_loss=2.6008641719818115\n",
      "Epoch=918, iteration=0, train_loss=2.3635964393615723\n",
      "Epoch=918, iteration=1, train_loss=2.3884472846984863\n",
      "Epoch=918, iteration=2, train_loss=2.4307074546813965\n",
      "Epoch=918, iteration=3, train_loss=2.578864574432373\n",
      "Epoch=918, val_loss=2.6008589267730713\n",
      "Epoch=919, iteration=0, train_loss=2.3635830879211426\n",
      "Epoch=919, iteration=1, train_loss=2.3884353637695312\n",
      "Epoch=919, iteration=2, train_loss=2.4306976795196533\n",
      "Epoch=919, iteration=3, train_loss=2.5788586139678955\n",
      "Epoch=919, val_loss=2.600853204727173\n",
      "Epoch=920, iteration=0, train_loss=2.363569736480713\n",
      "Epoch=920, iteration=1, train_loss=2.388423204421997\n",
      "Epoch=920, iteration=2, train_loss=2.43068790435791\n",
      "Epoch=920, iteration=3, train_loss=2.578852891921997\n",
      "Epoch=920, val_loss=2.6008477210998535\n",
      "Epoch=921, iteration=0, train_loss=2.3635566234588623\n",
      "Epoch=921, iteration=1, train_loss=2.3884117603302\n",
      "Epoch=921, iteration=2, train_loss=2.4306774139404297\n",
      "Epoch=921, iteration=3, train_loss=2.5788469314575195\n",
      "Epoch=921, val_loss=2.600842237472534\n",
      "Epoch=922, iteration=0, train_loss=2.3635432720184326\n",
      "Epoch=922, iteration=1, train_loss=2.388399600982666\n",
      "Epoch=922, iteration=2, train_loss=2.4306681156158447\n",
      "Epoch=922, iteration=3, train_loss=2.578840970993042\n",
      "Epoch=922, val_loss=2.6008362770080566\n",
      "Epoch=923, iteration=0, train_loss=2.3635306358337402\n",
      "Epoch=923, iteration=1, train_loss=2.38838791847229\n",
      "Epoch=923, iteration=2, train_loss=2.4306576251983643\n",
      "Epoch=923, iteration=3, train_loss=2.5788350105285645\n",
      "Epoch=923, val_loss=2.6008307933807373\n",
      "Epoch=924, iteration=0, train_loss=2.3635170459747314\n",
      "Epoch=924, iteration=1, train_loss=2.388375997543335\n",
      "Epoch=924, iteration=2, train_loss=2.430647850036621\n",
      "Epoch=924, iteration=3, train_loss=2.578829050064087\n",
      "Epoch=924, val_loss=2.600825071334839\n",
      "Epoch=925, iteration=0, train_loss=2.36350417137146\n",
      "Epoch=925, iteration=1, train_loss=2.38836407661438\n",
      "Epoch=925, iteration=2, train_loss=2.430637836456299\n",
      "Epoch=925, iteration=3, train_loss=2.5788230895996094\n",
      "Epoch=925, val_loss=2.6008193492889404\n",
      "Epoch=926, iteration=0, train_loss=2.3634908199310303\n",
      "Epoch=926, iteration=1, train_loss=2.388352155685425\n",
      "Epoch=926, iteration=2, train_loss=2.4306282997131348\n",
      "Epoch=926, iteration=3, train_loss=2.5788166522979736\n",
      "Epoch=926, val_loss=2.600813388824463\n",
      "Epoch=927, iteration=0, train_loss=2.3634777069091797\n",
      "Epoch=927, iteration=1, train_loss=2.3883402347564697\n",
      "Epoch=927, iteration=2, train_loss=2.4306182861328125\n",
      "Epoch=927, iteration=3, train_loss=2.578810930252075\n",
      "Epoch=927, val_loss=2.6008076667785645\n",
      "Epoch=928, iteration=0, train_loss=2.363464832305908\n",
      "Epoch=928, iteration=1, train_loss=2.3883285522460938\n",
      "Epoch=928, iteration=2, train_loss=2.4306087493896484\n",
      "Epoch=928, iteration=3, train_loss=2.5788047313690186\n",
      "Epoch=928, val_loss=2.600801944732666\n",
      "Epoch=929, iteration=0, train_loss=2.3634517192840576\n",
      "Epoch=929, iteration=1, train_loss=2.3883166313171387\n",
      "Epoch=929, iteration=2, train_loss=2.430598497390747\n",
      "Epoch=929, iteration=3, train_loss=2.578798532485962\n",
      "Epoch=929, val_loss=2.6007959842681885\n",
      "Epoch=930, iteration=0, train_loss=2.363438844680786\n",
      "Epoch=930, iteration=1, train_loss=2.3883047103881836\n",
      "Epoch=930, iteration=2, train_loss=2.430588722229004\n",
      "Epoch=930, iteration=3, train_loss=2.5787923336029053\n",
      "Epoch=930, val_loss=2.60079026222229\n",
      "Epoch=931, iteration=0, train_loss=2.3634254932403564\n",
      "Epoch=931, iteration=1, train_loss=2.3882927894592285\n",
      "Epoch=931, iteration=2, train_loss=2.4305789470672607\n",
      "Epoch=931, iteration=3, train_loss=2.5787858963012695\n",
      "Epoch=931, val_loss=2.6007843017578125\n",
      "Epoch=932, iteration=0, train_loss=2.363412618637085\n",
      "Epoch=932, iteration=1, train_loss=2.3882815837860107\n",
      "Epoch=932, iteration=2, train_loss=2.4305691719055176\n",
      "Epoch=932, iteration=3, train_loss=2.578780174255371\n",
      "Epoch=932, val_loss=2.600778341293335\n",
      "Epoch=933, iteration=0, train_loss=2.3633997440338135\n",
      "Epoch=933, iteration=1, train_loss=2.3882699012756348\n",
      "Epoch=933, iteration=2, train_loss=2.4305593967437744\n",
      "Epoch=933, iteration=3, train_loss=2.5787734985351562\n",
      "Epoch=933, val_loss=2.6007723808288574\n",
      "Epoch=934, iteration=0, train_loss=2.363386869430542\n",
      "Epoch=934, iteration=1, train_loss=2.3882579803466797\n",
      "Epoch=934, iteration=2, train_loss=2.430549383163452\n",
      "Epoch=934, iteration=3, train_loss=2.5787672996520996\n",
      "Epoch=934, val_loss=2.60076642036438\n",
      "Epoch=935, iteration=0, train_loss=2.3633737564086914\n",
      "Epoch=935, iteration=1, train_loss=2.388246536254883\n",
      "Epoch=935, iteration=2, train_loss=2.430539846420288\n",
      "Epoch=935, iteration=3, train_loss=2.5787606239318848\n",
      "Epoch=935, val_loss=2.6007602214813232\n",
      "Epoch=936, iteration=0, train_loss=2.36336088180542\n",
      "Epoch=936, iteration=1, train_loss=2.388234853744507\n",
      "Epoch=936, iteration=2, train_loss=2.430530071258545\n",
      "Epoch=936, iteration=3, train_loss=2.578754186630249\n",
      "Epoch=936, val_loss=2.6007542610168457\n",
      "Epoch=937, iteration=0, train_loss=2.3633480072021484\n",
      "Epoch=937, iteration=1, train_loss=2.3882229328155518\n",
      "Epoch=937, iteration=2, train_loss=2.4305200576782227\n",
      "Epoch=937, iteration=3, train_loss=2.5787479877471924\n",
      "Epoch=937, val_loss=2.600748062133789\n",
      "Epoch=938, iteration=0, train_loss=2.363334894180298\n",
      "Epoch=938, iteration=1, train_loss=2.388211488723755\n",
      "Epoch=938, iteration=2, train_loss=2.4305102825164795\n",
      "Epoch=938, iteration=3, train_loss=2.5787417888641357\n",
      "Epoch=938, val_loss=2.6007423400878906\n",
      "Epoch=939, iteration=0, train_loss=2.3633220195770264\n",
      "Epoch=939, iteration=1, train_loss=2.388199806213379\n",
      "Epoch=939, iteration=2, train_loss=2.4305007457733154\n",
      "Epoch=939, iteration=3, train_loss=2.578735113143921\n",
      "Epoch=939, val_loss=2.600735902786255\n",
      "Epoch=940, iteration=0, train_loss=2.363309383392334\n",
      "Epoch=940, iteration=1, train_loss=2.388187885284424\n",
      "Epoch=940, iteration=2, train_loss=2.430490732192993\n",
      "Epoch=940, iteration=3, train_loss=2.578728675842285\n",
      "Epoch=940, val_loss=2.6007299423217773\n",
      "Epoch=941, iteration=0, train_loss=2.3632965087890625\n",
      "Epoch=941, iteration=1, train_loss=2.388176202774048\n",
      "Epoch=941, iteration=2, train_loss=2.430480718612671\n",
      "Epoch=941, iteration=3, train_loss=2.5787220001220703\n",
      "Epoch=941, val_loss=2.6007237434387207\n",
      "Epoch=942, iteration=0, train_loss=2.363283634185791\n",
      "Epoch=942, iteration=1, train_loss=2.388164520263672\n",
      "Epoch=942, iteration=2, train_loss=2.430471181869507\n",
      "Epoch=942, iteration=3, train_loss=2.5787158012390137\n",
      "Epoch=942, val_loss=2.600717544555664\n",
      "Epoch=943, iteration=0, train_loss=2.3632705211639404\n",
      "Epoch=943, iteration=1, train_loss=2.388153314590454\n",
      "Epoch=943, iteration=2, train_loss=2.4304616451263428\n",
      "Epoch=943, iteration=3, train_loss=2.5787088871002197\n",
      "Epoch=943, val_loss=2.6007113456726074\n",
      "Epoch=944, iteration=0, train_loss=2.363257884979248\n",
      "Epoch=944, iteration=1, train_loss=2.388141393661499\n",
      "Epoch=944, iteration=2, train_loss=2.4304518699645996\n",
      "Epoch=944, iteration=3, train_loss=2.578702449798584\n",
      "Epoch=944, val_loss=2.6007049083709717\n",
      "Epoch=945, iteration=0, train_loss=2.3632452487945557\n",
      "Epoch=945, iteration=1, train_loss=2.3881301879882812\n",
      "Epoch=945, iteration=2, train_loss=2.4304416179656982\n",
      "Epoch=945, iteration=3, train_loss=2.578695774078369\n",
      "Epoch=945, val_loss=2.600698947906494\n",
      "Epoch=946, iteration=0, train_loss=2.363232374191284\n",
      "Epoch=946, iteration=1, train_loss=2.388118028640747\n",
      "Epoch=946, iteration=2, train_loss=2.4304323196411133\n",
      "Epoch=946, iteration=3, train_loss=2.5786890983581543\n",
      "Epoch=946, val_loss=2.6006927490234375\n",
      "Epoch=947, iteration=0, train_loss=2.363219738006592\n",
      "Epoch=947, iteration=1, train_loss=2.3881068229675293\n",
      "Epoch=947, iteration=2, train_loss=2.430422782897949\n",
      "Epoch=947, iteration=3, train_loss=2.5786824226379395\n",
      "Epoch=947, val_loss=2.6006863117218018\n",
      "Epoch=948, iteration=0, train_loss=2.3632068634033203\n",
      "Epoch=948, iteration=1, train_loss=2.3880956172943115\n",
      "Epoch=948, iteration=2, train_loss=2.430412530899048\n",
      "Epoch=948, iteration=3, train_loss=2.578676223754883\n",
      "Epoch=948, val_loss=2.600680112838745\n",
      "Epoch=949, iteration=0, train_loss=2.3631937503814697\n",
      "Epoch=949, iteration=1, train_loss=2.3880836963653564\n",
      "Epoch=949, iteration=2, train_loss=2.430403232574463\n",
      "Epoch=949, iteration=3, train_loss=2.578669309616089\n",
      "Epoch=949, val_loss=2.6006736755371094\n",
      "Epoch=950, iteration=0, train_loss=2.3631813526153564\n",
      "Epoch=950, iteration=1, train_loss=2.3880724906921387\n",
      "Epoch=950, iteration=2, train_loss=2.4303932189941406\n",
      "Epoch=950, iteration=3, train_loss=2.578662872314453\n",
      "Epoch=950, val_loss=2.6006674766540527\n",
      "Epoch=951, iteration=0, train_loss=2.363168478012085\n",
      "Epoch=951, iteration=1, train_loss=2.388061046600342\n",
      "Epoch=951, iteration=2, train_loss=2.4303836822509766\n",
      "Epoch=951, iteration=3, train_loss=2.5786561965942383\n",
      "Epoch=951, val_loss=2.600661039352417\n",
      "Epoch=952, iteration=0, train_loss=2.3631560802459717\n",
      "Epoch=952, iteration=1, train_loss=2.3880491256713867\n",
      "Epoch=952, iteration=2, train_loss=2.4303741455078125\n",
      "Epoch=952, iteration=3, train_loss=2.5786490440368652\n",
      "Epoch=952, val_loss=2.6006546020507812\n",
      "Epoch=953, iteration=0, train_loss=2.3631432056427\n",
      "Epoch=953, iteration=1, train_loss=2.388037919998169\n",
      "Epoch=953, iteration=2, train_loss=2.4303643703460693\n",
      "Epoch=953, iteration=3, train_loss=2.5786423683166504\n",
      "Epoch=953, val_loss=2.6006484031677246\n",
      "Epoch=954, iteration=0, train_loss=2.363130569458008\n",
      "Epoch=954, iteration=1, train_loss=2.388026237487793\n",
      "Epoch=954, iteration=2, train_loss=2.4303548336029053\n",
      "Epoch=954, iteration=3, train_loss=2.5786354541778564\n",
      "Epoch=954, val_loss=2.6006417274475098\n",
      "Epoch=955, iteration=0, train_loss=2.3631179332733154\n",
      "Epoch=955, iteration=1, train_loss=2.388015031814575\n",
      "Epoch=955, iteration=2, train_loss=2.430345058441162\n",
      "Epoch=955, iteration=3, train_loss=2.5786285400390625\n",
      "Epoch=955, val_loss=2.600635290145874\n",
      "Epoch=956, iteration=0, train_loss=2.363105297088623\n",
      "Epoch=956, iteration=1, train_loss=2.388003349304199\n",
      "Epoch=956, iteration=2, train_loss=2.430335283279419\n",
      "Epoch=956, iteration=3, train_loss=2.5786218643188477\n",
      "Epoch=956, val_loss=2.6006290912628174\n",
      "Epoch=957, iteration=0, train_loss=2.3630924224853516\n",
      "Epoch=957, iteration=1, train_loss=2.3879919052124023\n",
      "Epoch=957, iteration=2, train_loss=2.430325746536255\n",
      "Epoch=957, iteration=3, train_loss=2.578615188598633\n",
      "Epoch=957, val_loss=2.6006224155426025\n",
      "Epoch=958, iteration=0, train_loss=2.3630802631378174\n",
      "Epoch=958, iteration=1, train_loss=2.3879804611206055\n",
      "Epoch=958, iteration=2, train_loss=2.4303159713745117\n",
      "Epoch=958, iteration=3, train_loss=2.578608512878418\n",
      "Epoch=958, val_loss=2.600615978240967\n",
      "Epoch=959, iteration=0, train_loss=2.363067388534546\n",
      "Epoch=959, iteration=1, train_loss=2.3879692554473877\n",
      "Epoch=959, iteration=2, train_loss=2.4303064346313477\n",
      "Epoch=959, iteration=3, train_loss=2.578601598739624\n",
      "Epoch=959, val_loss=2.600609302520752\n",
      "Epoch=960, iteration=0, train_loss=2.3630549907684326\n",
      "Epoch=960, iteration=1, train_loss=2.38795804977417\n",
      "Epoch=960, iteration=2, train_loss=2.4302966594696045\n",
      "Epoch=960, iteration=3, train_loss=2.57859468460083\n",
      "Epoch=960, val_loss=2.600602865219116\n",
      "Epoch=961, iteration=0, train_loss=2.363042116165161\n",
      "Epoch=961, iteration=1, train_loss=2.387946367263794\n",
      "Epoch=961, iteration=2, train_loss=2.4302871227264404\n",
      "Epoch=961, iteration=3, train_loss=2.578587532043457\n",
      "Epoch=961, val_loss=2.6005964279174805\n",
      "Epoch=962, iteration=0, train_loss=2.363029718399048\n",
      "Epoch=962, iteration=1, train_loss=2.387934923171997\n",
      "Epoch=962, iteration=2, train_loss=2.4302773475646973\n",
      "Epoch=962, iteration=3, train_loss=2.578580617904663\n",
      "Epoch=962, val_loss=2.6005897521972656\n",
      "Epoch=963, iteration=0, train_loss=2.3630170822143555\n",
      "Epoch=963, iteration=1, train_loss=2.3879237174987793\n",
      "Epoch=963, iteration=2, train_loss=2.430267810821533\n",
      "Epoch=963, iteration=3, train_loss=2.5785739421844482\n",
      "Epoch=963, val_loss=2.60058331489563\n",
      "Epoch=964, iteration=0, train_loss=2.363004684448242\n",
      "Epoch=964, iteration=1, train_loss=2.3879120349884033\n",
      "Epoch=964, iteration=2, train_loss=2.430258274078369\n",
      "Epoch=964, iteration=3, train_loss=2.5785670280456543\n",
      "Epoch=964, val_loss=2.600576877593994\n",
      "Epoch=965, iteration=0, train_loss=2.36299204826355\n",
      "Epoch=965, iteration=1, train_loss=2.3879008293151855\n",
      "Epoch=965, iteration=2, train_loss=2.430248498916626\n",
      "Epoch=965, iteration=3, train_loss=2.5785598754882812\n",
      "Epoch=965, val_loss=2.6005699634552\n",
      "Epoch=966, iteration=0, train_loss=2.3629791736602783\n",
      "Epoch=966, iteration=1, train_loss=2.3878896236419678\n",
      "Epoch=966, iteration=2, train_loss=2.430238962173462\n",
      "Epoch=966, iteration=3, train_loss=2.5785531997680664\n",
      "Epoch=966, val_loss=2.6005635261535645\n",
      "Epoch=967, iteration=0, train_loss=2.362966775894165\n",
      "Epoch=967, iteration=1, train_loss=2.387878179550171\n",
      "Epoch=967, iteration=2, train_loss=2.430229425430298\n",
      "Epoch=967, iteration=3, train_loss=2.5785460472106934\n",
      "Epoch=967, val_loss=2.6005570888519287\n",
      "Epoch=968, iteration=0, train_loss=2.3629541397094727\n",
      "Epoch=968, iteration=1, train_loss=2.387866973876953\n",
      "Epoch=968, iteration=2, train_loss=2.430219888687134\n",
      "Epoch=968, iteration=3, train_loss=2.5785391330718994\n",
      "Epoch=968, val_loss=2.6005501747131348\n",
      "Epoch=969, iteration=0, train_loss=2.3629417419433594\n",
      "Epoch=969, iteration=1, train_loss=2.387855291366577\n",
      "Epoch=969, iteration=2, train_loss=2.4302103519439697\n",
      "Epoch=969, iteration=3, train_loss=2.5785319805145264\n",
      "Epoch=969, val_loss=2.60054349899292\n",
      "Epoch=970, iteration=0, train_loss=2.362928867340088\n",
      "Epoch=970, iteration=1, train_loss=2.3878443241119385\n",
      "Epoch=970, iteration=2, train_loss=2.4302005767822266\n",
      "Epoch=970, iteration=3, train_loss=2.5785250663757324\n",
      "Epoch=970, val_loss=2.600536823272705\n",
      "Epoch=971, iteration=0, train_loss=2.3629167079925537\n",
      "Epoch=971, iteration=1, train_loss=2.3878333568573\n",
      "Epoch=971, iteration=2, train_loss=2.4301908016204834\n",
      "Epoch=971, iteration=3, train_loss=2.5785176753997803\n",
      "Epoch=971, val_loss=2.6005303859710693\n",
      "Epoch=972, iteration=0, train_loss=2.3629040718078613\n",
      "Epoch=972, iteration=1, train_loss=2.387821674346924\n",
      "Epoch=972, iteration=2, train_loss=2.4301812648773193\n",
      "Epoch=972, iteration=3, train_loss=2.5785109996795654\n",
      "Epoch=972, val_loss=2.6005234718322754\n",
      "Epoch=973, iteration=0, train_loss=2.362891912460327\n",
      "Epoch=973, iteration=1, train_loss=2.387810468673706\n",
      "Epoch=973, iteration=2, train_loss=2.4301719665527344\n",
      "Epoch=973, iteration=3, train_loss=2.5785038471221924\n",
      "Epoch=973, val_loss=2.6005167961120605\n",
      "Epoch=974, iteration=0, train_loss=2.3628792762756348\n",
      "Epoch=974, iteration=1, train_loss=2.3877992630004883\n",
      "Epoch=974, iteration=2, train_loss=2.4301624298095703\n",
      "Epoch=974, iteration=3, train_loss=2.5784969329833984\n",
      "Epoch=974, val_loss=2.6005101203918457\n",
      "Epoch=975, iteration=0, train_loss=2.3628668785095215\n",
      "Epoch=975, iteration=1, train_loss=2.3877878189086914\n",
      "Epoch=975, iteration=2, train_loss=2.4301528930664062\n",
      "Epoch=975, iteration=3, train_loss=2.5784897804260254\n",
      "Epoch=975, val_loss=2.600503444671631\n",
      "Epoch=976, iteration=0, train_loss=2.362854242324829\n",
      "Epoch=976, iteration=1, train_loss=2.3877766132354736\n",
      "Epoch=976, iteration=2, train_loss=2.430143117904663\n",
      "Epoch=976, iteration=3, train_loss=2.5784823894500732\n",
      "Epoch=976, val_loss=2.600496530532837\n",
      "Epoch=977, iteration=0, train_loss=2.362842082977295\n",
      "Epoch=977, iteration=1, train_loss=2.3877651691436768\n",
      "Epoch=977, iteration=2, train_loss=2.430133581161499\n",
      "Epoch=977, iteration=3, train_loss=2.5784752368927\n",
      "Epoch=977, val_loss=2.600490093231201\n",
      "Epoch=978, iteration=0, train_loss=2.3628292083740234\n",
      "Epoch=978, iteration=1, train_loss=2.387754201889038\n",
      "Epoch=978, iteration=2, train_loss=2.430124044418335\n",
      "Epoch=978, iteration=3, train_loss=2.5784683227539062\n",
      "Epoch=978, val_loss=2.600482940673828\n",
      "Epoch=979, iteration=0, train_loss=2.3628170490264893\n",
      "Epoch=979, iteration=1, train_loss=2.3877429962158203\n",
      "Epoch=979, iteration=2, train_loss=2.43011474609375\n",
      "Epoch=979, iteration=3, train_loss=2.578461170196533\n",
      "Epoch=979, val_loss=2.6004765033721924\n",
      "Epoch=980, iteration=0, train_loss=2.362804651260376\n",
      "Epoch=980, iteration=1, train_loss=2.3877313137054443\n",
      "Epoch=980, iteration=2, train_loss=2.430104970932007\n",
      "Epoch=980, iteration=3, train_loss=2.57845401763916\n",
      "Epoch=980, val_loss=2.6004693508148193\n",
      "Epoch=981, iteration=0, train_loss=2.3627922534942627\n",
      "Epoch=981, iteration=1, train_loss=2.3877201080322266\n",
      "Epoch=981, iteration=2, train_loss=2.430095672607422\n",
      "Epoch=981, iteration=3, train_loss=2.578447103500366\n",
      "Epoch=981, val_loss=2.6004626750946045\n",
      "Epoch=982, iteration=0, train_loss=2.3627798557281494\n",
      "Epoch=982, iteration=1, train_loss=2.387708902359009\n",
      "Epoch=982, iteration=2, train_loss=2.430086374282837\n",
      "Epoch=982, iteration=3, train_loss=2.578439474105835\n",
      "Epoch=982, val_loss=2.6004559993743896\n",
      "Epoch=983, iteration=0, train_loss=2.362767457962036\n",
      "Epoch=983, iteration=1, train_loss=2.38769793510437\n",
      "Epoch=983, iteration=2, train_loss=2.4300765991210938\n",
      "Epoch=983, iteration=3, train_loss=2.578432559967041\n",
      "Epoch=983, val_loss=2.600449323654175\n",
      "Epoch=984, iteration=0, train_loss=2.362755298614502\n",
      "Epoch=984, iteration=1, train_loss=2.3876864910125732\n",
      "Epoch=984, iteration=2, train_loss=2.4300668239593506\n",
      "Epoch=984, iteration=3, train_loss=2.578425407409668\n",
      "Epoch=984, val_loss=2.600442409515381\n",
      "Epoch=985, iteration=0, train_loss=2.3627424240112305\n",
      "Epoch=985, iteration=1, train_loss=2.3876755237579346\n",
      "Epoch=985, iteration=2, train_loss=2.4300575256347656\n",
      "Epoch=985, iteration=3, train_loss=2.578418016433716\n",
      "Epoch=985, val_loss=2.600435972213745\n",
      "Epoch=986, iteration=0, train_loss=2.3627302646636963\n",
      "Epoch=986, iteration=1, train_loss=2.387664318084717\n",
      "Epoch=986, iteration=2, train_loss=2.4300479888916016\n",
      "Epoch=986, iteration=3, train_loss=2.5784108638763428\n",
      "Epoch=986, val_loss=2.600428581237793\n",
      "Epoch=987, iteration=0, train_loss=2.362718105316162\n",
      "Epoch=987, iteration=1, train_loss=2.387653112411499\n",
      "Epoch=987, iteration=2, train_loss=2.4300382137298584\n",
      "Epoch=987, iteration=3, train_loss=2.5784034729003906\n",
      "Epoch=987, val_loss=2.600421905517578\n",
      "Epoch=988, iteration=0, train_loss=2.362705707550049\n",
      "Epoch=988, iteration=1, train_loss=2.3876419067382812\n",
      "Epoch=988, iteration=2, train_loss=2.4300286769866943\n",
      "Epoch=988, iteration=3, train_loss=2.5783965587615967\n",
      "Epoch=988, val_loss=2.600414991378784\n",
      "Epoch=989, iteration=0, train_loss=2.3626933097839355\n",
      "Epoch=989, iteration=1, train_loss=2.3876304626464844\n",
      "Epoch=989, iteration=2, train_loss=2.4300193786621094\n",
      "Epoch=989, iteration=3, train_loss=2.5783891677856445\n",
      "Epoch=989, val_loss=2.6004080772399902\n",
      "Epoch=990, iteration=0, train_loss=2.3626809120178223\n",
      "Epoch=990, iteration=1, train_loss=2.3876194953918457\n",
      "Epoch=990, iteration=2, train_loss=2.4300098419189453\n",
      "Epoch=990, iteration=3, train_loss=2.5783820152282715\n",
      "Epoch=990, val_loss=2.6004011631011963\n",
      "Epoch=991, iteration=0, train_loss=2.362668514251709\n",
      "Epoch=991, iteration=1, train_loss=2.387608289718628\n",
      "Epoch=991, iteration=2, train_loss=2.4300003051757812\n",
      "Epoch=991, iteration=3, train_loss=2.5783743858337402\n",
      "Epoch=991, val_loss=2.6003940105438232\n",
      "Epoch=992, iteration=0, train_loss=2.3626561164855957\n",
      "Epoch=992, iteration=1, train_loss=2.3875973224639893\n",
      "Epoch=992, iteration=2, train_loss=2.4299910068511963\n",
      "Epoch=992, iteration=3, train_loss=2.578367233276367\n",
      "Epoch=992, val_loss=2.6003873348236084\n",
      "Epoch=993, iteration=0, train_loss=2.3626437187194824\n",
      "Epoch=993, iteration=1, train_loss=2.3875861167907715\n",
      "Epoch=993, iteration=2, train_loss=2.4299817085266113\n",
      "Epoch=993, iteration=3, train_loss=2.578360080718994\n",
      "Epoch=993, val_loss=2.6003804206848145\n",
      "Epoch=994, iteration=0, train_loss=2.3626317977905273\n",
      "Epoch=994, iteration=1, train_loss=2.3875746726989746\n",
      "Epoch=994, iteration=2, train_loss=2.429971694946289\n",
      "Epoch=994, iteration=3, train_loss=2.5783531665802\n",
      "Epoch=994, val_loss=2.6003735065460205\n",
      "Epoch=995, iteration=0, train_loss=2.362619400024414\n",
      "Epoch=995, iteration=1, train_loss=2.387563705444336\n",
      "Epoch=995, iteration=2, train_loss=2.429962396621704\n",
      "Epoch=995, iteration=3, train_loss=2.578345537185669\n",
      "Epoch=995, val_loss=2.6003668308258057\n",
      "Epoch=996, iteration=0, train_loss=2.362607479095459\n",
      "Epoch=996, iteration=1, train_loss=2.3875527381896973\n",
      "Epoch=996, iteration=2, train_loss=2.42995285987854\n",
      "Epoch=996, iteration=3, train_loss=2.578338146209717\n",
      "Epoch=996, val_loss=2.6003596782684326\n",
      "Epoch=997, iteration=0, train_loss=2.3625948429107666\n",
      "Epoch=997, iteration=1, train_loss=2.3875415325164795\n",
      "Epoch=997, iteration=2, train_loss=2.429943323135376\n",
      "Epoch=997, iteration=3, train_loss=2.5783307552337646\n",
      "Epoch=997, val_loss=2.6003527641296387\n",
      "Epoch=998, iteration=0, train_loss=2.3625826835632324\n",
      "Epoch=998, iteration=1, train_loss=2.3875303268432617\n",
      "Epoch=998, iteration=2, train_loss=2.42993426322937\n",
      "Epoch=998, iteration=3, train_loss=2.5783236026763916\n",
      "Epoch=998, val_loss=2.6003458499908447\n",
      "Epoch=999, iteration=0, train_loss=2.3625705242156982\n",
      "Epoch=999, iteration=1, train_loss=2.387519359588623\n",
      "Epoch=999, iteration=2, train_loss=2.429924488067627\n",
      "Epoch=999, iteration=3, train_loss=2.5783164501190186\n",
      "Epoch=999, val_loss=2.6003386974334717\n",
      "Epoch=1000, iteration=0, train_loss=2.362558364868164\n",
      "Epoch=1000, iteration=1, train_loss=2.3875081539154053\n",
      "Epoch=1000, iteration=2, train_loss=2.429915189743042\n",
      "Epoch=1000, iteration=3, train_loss=2.5783090591430664\n",
      "Epoch=1000, val_loss=2.6003317832946777\n"
     ]
    }
   ],
   "source": [
    "def train(parameters: dict, \n",
    "          X_train: torch.tensor, \n",
    "          y_train: torch.tensor, \n",
    "          X_validation: torch.tensor, \n",
    "          y_validation: torch.tensor, \n",
    "          embedding_dim: int, \n",
    "          le: list=None, \n",
    "          batch_size: int=0, \n",
    "          epochs: int=10, \n",
    "          lr: int=0.1) -> dict:\n",
    "    '''\n",
    "    Training the parameters by backpropagation.\n",
    "\n",
    "    Args:\n",
    "        parameters: a dictionary of layers and neurons\n",
    "        X_train: training data set\n",
    "        y_train training label set\n",
    "        X_validation: validation data set\n",
    "        y_validation: validation label set\n",
    "        embedding_dim: dimensions of word emebddings\n",
    "        le: linear learning rate values (only used for research and not train)\n",
    "        batch_size: number of batches which determines the frequency of parameter updates\n",
    "        epochs: number of epochs which determines the number of times we see all the dataset\n",
    "        lr: a nonlinear list of learning rate values from high to low values - 0.1 if we are not in research mode\n",
    "\n",
    "    Returns:\n",
    "       Optimized parameters of the model\n",
    "    '''\n",
    "\n",
    "    lr_iter = []\n",
    "    le_iter = []\n",
    "    loss_iter = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batch_size):\n",
    "            data = X_train[batch*(X_train.size()[0] // batch_size): (X_train.size()[0] // batch_size)*(batch+1)]\n",
    "            label = y_train[batch*(y_train.size()[0] // batch_size): (y_train.size()[0] // batch_size)*(batch+1)]\n",
    "            embeds = parameters['C'][data]\n",
    "            layer1 = embeds.view((-1, data.size()[-1]*embedding_dim))\n",
    "            h = torch.tanh(layer1 @ parameters['W1'])*parameters['b1']\n",
    "            logits = (h @ parameters['W2'])*parameters['b2']\n",
    "            loss = F.cross_entropy(logits, label) \n",
    "            # cross entropy comapres only with relative to the correct label so should our defined loss be like. \n",
    "            # Since the offset can be distinguished with only this value.\n",
    "            print(f\"Epoch={epoch+1}, iteration={batch}, train_loss={loss}\")\n",
    "            \n",
    "            for _, v in parameters.items():\n",
    "                v.grad = None\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if epoch <= 0.8*epochs:\n",
    "                lr = 0.1\n",
    "            else:\n",
    "                lr = 0.01\n",
    "\n",
    "            for _, v in parameters.items():\n",
    "                v.data += -lr * v.grad\n",
    "                # v.data += -lr[epoch]*v.grad\n",
    "            \n",
    "            # le_iter.append(le[epoch])\n",
    "            # lr_iter.append(lr[epoch])\n",
    "            # loss_iter.append(loss.item())\n",
    "        \n",
    "        val_embeds = parameters['C'][X_validation]\n",
    "        val_layer1 = val_embeds.view((-1, X_validation.size()[-1]*embedding_dim))\n",
    "        val_h = torch.tanh(val_layer1 @ parameters['W1'])*parameters['b1']\n",
    "        val_logits = (val_h @ parameters['W2'])*parameters['b2']\n",
    "        val_loss = F.cross_entropy(val_logits, y_validation) \n",
    "        print(f\"Epoch={epoch+1}, val_loss={val_loss}\")\n",
    "    \n",
    "    # plt.plot(lr_iter, loss_iter)\n",
    "    # return parameters, lr_iter, le_iter, loss_iter\n",
    "    return parameters\n",
    "\n",
    "\n",
    "for k,v in parameters.items():\n",
    "    v.requires_grad = True\n",
    "\n",
    "# model, lr_iter, le_iter, loss_iter = train(parameters, X_train, y_train, X_validation, y_validation, 2, le, batch_size=8, epochs=500, lr=lrs)\n",
    "model = train(parameters, X_train, y_train, X_validation, y_validation, 2, le, batch_size=BATCH_SIZE, epochs=EPOCHS, lr=lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the lr hyperparameter, we first identify the optimal learning rate using a diagram that visualizes the loss relative to LR values, generated by the learning_rates() function at each iteration. The lr values start with small values and gradually increase. The \"elbow\" of this plot, where the loss begins to decrease rapidly without significant oscillations, indicates the best learning rate to use.\n",
    "\n",
    "After determining the optimal LR, we apply a learning rate decay method to dynamically adjust the LR during training, starting from the optimal value and gradually decreasing it over time to improve convergence and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a7427010d0>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(lr_iter, loss_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a73e545d90>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(le_iter, loss_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caora', 'hkial', 'ialaalaa', '', 'simassie', 'n', 'enh', 'hscire', 'nlahariln', '']\n"
     ]
    }
   ],
   "source": [
    "def inference(parameters: dict, n_names: int) -> list:\n",
    "    '''\n",
    "    Making names from model.\n",
    "\n",
    "    Args:\n",
    "        parameters: model\n",
    "        n_names: number of names to be produced\n",
    "    \n",
    "    Returns:\n",
    "        names: a list of generated names by the model\n",
    "    '''\n",
    "\n",
    "    sequence_length = parameters['W1'].size()[0] // parameters['C'].size()[1]\n",
    "    tokens = [0] * sequence_length\n",
    "\n",
    "    names = []\n",
    "\n",
    "    for _ in range(n_names):\n",
    "        chars = []\n",
    "        while True:\n",
    "            embed = parameters['C'][tokens]\n",
    "            layer1 = embed.view((-1, sequence_length*parameters['C'].size()[1]))\n",
    "            h = torch.tanh(layer1 @ parameters['W1'])*parameters['b1']\n",
    "            logits = (h @ parameters['W2'])*parameters['b2']\n",
    "            probs = torch.softmax(logits.data[0], dim=0) # Softmax + log likelihood = CrossEntropy\n",
    "            pred = torch.multinomial(probs, 1, replacement=True)\n",
    "            if pred==0:\n",
    "                break\n",
    "            chars.append(id_chars[pred.item()])\n",
    "\n",
    "            tokens = tokens[1:] + [pred.item()]\n",
    "\n",
    "        names.append(chars)\n",
    "\n",
    "    return names\n",
    "    \n",
    "print([''.join(i) for i in inference(model, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def viz_embeddings(embeds: torch.tensor) -> None:\n",
    "    '''\n",
    "    Visualizing the learned word embeddings\n",
    "\n",
    "    Args:\n",
    "        model embedding values\n",
    "    '''\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.scatter(embeds[:, 0].data, embeds[:, 1].data, s=200)\n",
    "    for i in range(len(embeds[:, 0])):\n",
    "        plt.text(embeds[i, 0], embeds[i, 1], id_chars[i], ha='center')\n",
    "\n",
    "\n",
    "viz_embeddings(parameters['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tunes\n",
    "In this section, the model is tested with higher embedding dimensions, and more hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "parameters_finetune = create_model(X.size()[-1], 200, 10, 27)\n",
    "print(f\"Total number of parameters: {sum(v.nelement() for k, v in parameters.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1, iteration=0, train_loss=32.9820442199707\n",
      "Epoch=1, iteration=1, train_loss=15.410675048828125\n",
      "Epoch=1, iteration=2, train_loss=48.46055221557617\n",
      "Epoch=1, iteration=3, train_loss=66.63805389404297\n",
      "Epoch=1, val_loss=62.79025650024414\n",
      "Epoch=2, iteration=0, train_loss=63.362640380859375\n",
      "Epoch=2, iteration=1, train_loss=45.60008239746094\n",
      "Epoch=2, iteration=2, train_loss=23.67373275756836\n",
      "Epoch=2, iteration=3, train_loss=7.9285359382629395\n",
      "Epoch=2, val_loss=16.072547912597656\n",
      "Epoch=3, iteration=0, train_loss=16.416250228881836\n",
      "Epoch=3, iteration=1, train_loss=8.73753833770752\n",
      "Epoch=3, iteration=2, train_loss=7.045695781707764\n",
      "Epoch=3, iteration=3, train_loss=10.09320068359375\n",
      "Epoch=3, val_loss=5.286426067352295\n",
      "Epoch=4, iteration=0, train_loss=5.461031913757324\n",
      "Epoch=4, iteration=1, train_loss=6.890325546264648\n",
      "Epoch=4, iteration=2, train_loss=5.3806376457214355\n",
      "Epoch=4, iteration=3, train_loss=5.523053169250488\n",
      "Epoch=4, val_loss=4.353522777557373\n",
      "Epoch=5, iteration=0, train_loss=4.438297748565674\n",
      "Epoch=5, iteration=1, train_loss=4.64382791519165\n",
      "Epoch=5, iteration=2, train_loss=3.4715476036071777\n",
      "Epoch=5, iteration=3, train_loss=4.1544189453125\n",
      "Epoch=5, val_loss=3.5959038734436035\n",
      "Epoch=6, iteration=0, train_loss=3.6234593391418457\n",
      "Epoch=6, iteration=1, train_loss=4.086386680603027\n",
      "Epoch=6, iteration=2, train_loss=3.3042380809783936\n",
      "Epoch=6, iteration=3, train_loss=3.802947759628296\n",
      "Epoch=6, val_loss=3.5193703174591064\n",
      "Epoch=7, iteration=0, train_loss=3.5320849418640137\n",
      "Epoch=7, iteration=1, train_loss=3.866931915283203\n",
      "Epoch=7, iteration=2, train_loss=3.2222983837127686\n",
      "Epoch=7, iteration=3, train_loss=3.617126941680908\n",
      "Epoch=7, val_loss=3.4418818950653076\n",
      "Epoch=8, iteration=0, train_loss=3.4429771900177\n",
      "Epoch=8, iteration=1, train_loss=3.6943888664245605\n",
      "Epoch=8, iteration=2, train_loss=3.1796603202819824\n",
      "Epoch=8, iteration=3, train_loss=3.507111072540283\n",
      "Epoch=8, val_loss=3.3610689640045166\n",
      "Epoch=9, iteration=0, train_loss=3.351409912109375\n",
      "Epoch=9, iteration=1, train_loss=3.53981351852417\n",
      "Epoch=9, iteration=2, train_loss=3.1470727920532227\n",
      "Epoch=9, iteration=3, train_loss=3.42148494720459\n",
      "Epoch=9, val_loss=3.2926979064941406\n",
      "Epoch=10, iteration=0, train_loss=3.2738099098205566\n",
      "Epoch=10, iteration=1, train_loss=3.41139817237854\n",
      "Epoch=10, iteration=2, train_loss=3.115974187850952\n",
      "Epoch=10, iteration=3, train_loss=3.346710443496704\n",
      "Epoch=10, val_loss=3.2357165813446045\n",
      "Epoch=11, iteration=0, train_loss=3.2089343070983887\n",
      "Epoch=11, iteration=1, train_loss=3.306694746017456\n",
      "Epoch=11, iteration=2, train_loss=3.085620164871216\n",
      "Epoch=11, iteration=3, train_loss=3.2815053462982178\n",
      "Epoch=11, val_loss=3.187720775604248\n",
      "Epoch=12, iteration=0, train_loss=3.1540744304656982\n",
      "Epoch=12, iteration=1, train_loss=3.2216179370880127\n",
      "Epoch=12, iteration=2, train_loss=3.0566840171813965\n",
      "Epoch=12, iteration=3, train_loss=3.2256557941436768\n",
      "Epoch=12, val_loss=3.147089719772339\n",
      "Epoch=13, iteration=0, train_loss=3.1073720455169678\n",
      "Epoch=13, iteration=1, train_loss=3.1525344848632812\n",
      "Epoch=13, iteration=2, train_loss=3.0298330783843994\n",
      "Epoch=13, iteration=3, train_loss=3.178516149520874\n",
      "Epoch=13, val_loss=3.1127045154571533\n",
      "Epoch=14, iteration=0, train_loss=3.0675272941589355\n",
      "Epoch=14, iteration=1, train_loss=3.096388816833496\n",
      "Epoch=14, iteration=2, train_loss=3.005459785461426\n",
      "Epoch=14, iteration=3, train_loss=3.139112710952759\n",
      "Epoch=14, val_loss=3.0836684703826904\n",
      "Epoch=15, iteration=0, train_loss=3.0334973335266113\n",
      "Epoch=15, iteration=1, train_loss=3.05062198638916\n",
      "Epoch=15, iteration=2, train_loss=2.983673334121704\n",
      "Epoch=15, iteration=3, train_loss=3.106339931488037\n",
      "Epoch=15, val_loss=3.0591886043548584\n",
      "Epoch=16, iteration=0, train_loss=3.0043609142303467\n",
      "Epoch=16, iteration=1, train_loss=3.013092279434204\n",
      "Epoch=16, iteration=2, train_loss=2.9643545150756836\n",
      "Epoch=16, iteration=3, train_loss=3.079096555709839\n",
      "Epoch=16, val_loss=3.0385329723358154\n",
      "Epoch=17, iteration=0, train_loss=2.9792685508728027\n",
      "Epoch=17, iteration=1, train_loss=2.982023239135742\n",
      "Epoch=17, iteration=2, train_loss=2.947237730026245\n",
      "Epoch=17, iteration=3, train_loss=3.056368589401245\n",
      "Epoch=17, val_loss=3.0210390090942383\n",
      "Epoch=18, iteration=0, train_loss=2.957440137863159\n",
      "Epoch=18, iteration=1, train_loss=2.955949068069458\n",
      "Epoch=18, iteration=2, train_loss=2.931978225708008\n",
      "Epoch=18, iteration=3, train_loss=3.037266254425049\n",
      "Epoch=18, val_loss=3.006113290786743\n",
      "Epoch=19, iteration=0, train_loss=2.938171148300171\n",
      "Epoch=19, iteration=1, train_loss=2.9336721897125244\n",
      "Epoch=19, iteration=2, train_loss=2.918208122253418\n",
      "Epoch=19, iteration=3, train_loss=3.0210366249084473\n",
      "Epoch=19, val_loss=2.993248701095581\n",
      "Epoch=20, iteration=0, train_loss=2.9208438396453857\n",
      "Epoch=20, iteration=1, train_loss=2.914229393005371\n",
      "Epoch=20, iteration=2, train_loss=2.905574083328247\n",
      "Epoch=20, iteration=3, train_loss=3.007070302963257\n",
      "Epoch=20, val_loss=2.982027292251587\n",
      "Epoch=21, iteration=0, train_loss=2.904933452606201\n",
      "Epoch=21, iteration=1, train_loss=2.8968594074249268\n",
      "Epoch=21, iteration=2, train_loss=2.893770933151245\n",
      "Epoch=21, iteration=3, train_loss=2.9948956966400146\n",
      "Epoch=21, val_loss=2.9721298217773438\n",
      "Epoch=22, iteration=0, train_loss=2.8900251388549805\n",
      "Epoch=22, iteration=1, train_loss=2.880990505218506\n",
      "Epoch=22, iteration=2, train_loss=2.882572889328003\n",
      "Epoch=22, iteration=3, train_loss=2.984179735183716\n",
      "Epoch=22, val_loss=2.963344097137451\n",
      "Epoch=23, iteration=0, train_loss=2.8758327960968018\n",
      "Epoch=23, iteration=1, train_loss=2.8662421703338623\n",
      "Epoch=23, iteration=2, train_loss=2.8718605041503906\n",
      "Epoch=23, iteration=3, train_loss=2.9747374057769775\n",
      "Epoch=23, val_loss=2.955575942993164\n",
      "Epoch=24, iteration=0, train_loss=2.862234115600586\n",
      "Epoch=24, iteration=1, train_loss=2.8524370193481445\n",
      "Epoch=24, iteration=2, train_loss=2.86165452003479\n",
      "Epoch=24, iteration=3, train_loss=2.966526985168457\n",
      "Epoch=24, val_loss=2.948847770690918\n",
      "Epoch=25, iteration=0, train_loss=2.849290370941162\n",
      "Epoch=25, iteration=1, train_loss=2.8396105766296387\n",
      "Epoch=25, iteration=2, train_loss=2.8521320819854736\n",
      "Epoch=25, iteration=3, train_loss=2.9596354961395264\n",
      "Epoch=25, val_loss=2.943279504776001\n",
      "Epoch=26, iteration=0, train_loss=2.837259292602539\n",
      "Epoch=26, iteration=1, train_loss=2.8279976844787598\n",
      "Epoch=26, iteration=2, train_loss=2.8435981273651123\n",
      "Epoch=26, iteration=3, train_loss=2.954214572906494\n",
      "Epoch=26, val_loss=2.939016342163086\n",
      "Epoch=27, iteration=0, train_loss=2.8265347480773926\n",
      "Epoch=27, iteration=1, train_loss=2.8179452419281006\n",
      "Epoch=27, iteration=2, train_loss=2.8363919258117676\n",
      "Epoch=27, iteration=3, train_loss=2.9503681659698486\n",
      "Epoch=27, val_loss=2.936124086380005\n",
      "Epoch=28, iteration=0, train_loss=2.8175160884857178\n",
      "Epoch=28, iteration=1, train_loss=2.809762477874756\n",
      "Epoch=28, iteration=2, train_loss=2.830742120742798\n",
      "Epoch=28, iteration=3, train_loss=2.948035478591919\n",
      "Epoch=28, val_loss=2.934494972229004\n",
      "Epoch=29, iteration=0, train_loss=2.8104381561279297\n",
      "Epoch=29, iteration=1, train_loss=2.8035593032836914\n",
      "Epoch=29, iteration=2, train_loss=2.8266496658325195\n",
      "Epoch=29, iteration=3, train_loss=2.946953535079956\n",
      "Epoch=29, val_loss=2.933854341506958\n",
      "Epoch=30, iteration=0, train_loss=2.8052706718444824\n",
      "Epoch=30, iteration=1, train_loss=2.799187421798706\n",
      "Epoch=30, iteration=2, train_loss=2.8238961696624756\n",
      "Epoch=30, iteration=3, train_loss=2.9467427730560303\n",
      "Epoch=30, val_loss=2.933856725692749\n",
      "Epoch=31, iteration=0, train_loss=2.801746368408203\n",
      "Epoch=31, iteration=1, train_loss=2.7963106632232666\n",
      "Epoch=31, iteration=2, train_loss=2.8221516609191895\n",
      "Epoch=31, iteration=3, train_loss=2.9470372200012207\n",
      "Epoch=31, val_loss=2.934201240539551\n",
      "Epoch=32, iteration=0, train_loss=2.799485683441162\n",
      "Epoch=32, iteration=1, train_loss=2.7945311069488525\n",
      "Epoch=32, iteration=2, train_loss=2.821089267730713\n",
      "Epoch=32, iteration=3, train_loss=2.9475622177124023\n",
      "Epoch=32, val_loss=2.934675455093384\n",
      "Epoch=33, iteration=0, train_loss=2.7981035709381104\n",
      "Epoch=33, iteration=1, train_loss=2.7934815883636475\n",
      "Epoch=33, iteration=2, train_loss=2.820444107055664\n",
      "Epoch=33, iteration=3, train_loss=2.948134660720825\n",
      "Epoch=33, val_loss=2.9351468086242676\n",
      "Epoch=34, iteration=0, train_loss=2.797269105911255\n",
      "Epoch=34, iteration=1, train_loss=2.7928614616394043\n",
      "Epoch=34, iteration=2, train_loss=2.820019483566284\n",
      "Epoch=34, iteration=3, train_loss=2.9486374855041504\n",
      "Epoch=34, val_loss=2.9355313777923584\n",
      "Epoch=35, iteration=0, train_loss=2.796727418899536\n",
      "Epoch=35, iteration=1, train_loss=2.7924437522888184\n",
      "Epoch=35, iteration=2, train_loss=2.8196792602539062\n",
      "Epoch=35, iteration=3, train_loss=2.9489989280700684\n",
      "Epoch=35, val_loss=2.9357798099517822\n",
      "Epoch=36, iteration=0, train_loss=2.7962961196899414\n",
      "Epoch=36, iteration=1, train_loss=2.792069673538208\n",
      "Epoch=36, iteration=2, train_loss=2.8193323612213135\n",
      "Epoch=36, iteration=3, train_loss=2.9491779804229736\n",
      "Epoch=36, val_loss=2.9358675479888916\n",
      "Epoch=37, iteration=0, train_loss=2.7958569526672363\n",
      "Epoch=37, iteration=1, train_loss=2.7916407585144043\n",
      "Epoch=37, iteration=2, train_loss=2.8189280033111572\n",
      "Epoch=37, iteration=3, train_loss=2.949159860610962\n",
      "Epoch=37, val_loss=2.935789108276367\n",
      "Epoch=38, iteration=0, train_loss=2.7953429222106934\n",
      "Epoch=38, iteration=1, train_loss=2.7911057472229004\n",
      "Epoch=38, iteration=2, train_loss=2.8184423446655273\n",
      "Epoch=38, iteration=3, train_loss=2.9489526748657227\n",
      "Epoch=38, val_loss=2.9355552196502686\n",
      "Epoch=39, iteration=0, train_loss=2.794729471206665\n",
      "Epoch=39, iteration=1, train_loss=2.790449619293213\n",
      "Epoch=39, iteration=2, train_loss=2.817873954772949\n",
      "Epoch=39, iteration=3, train_loss=2.948577880859375\n",
      "Epoch=39, val_loss=2.935187816619873\n",
      "Epoch=40, iteration=0, train_loss=2.7940142154693604\n",
      "Epoch=40, iteration=1, train_loss=2.7896811962127686\n",
      "Epoch=40, iteration=2, train_loss=2.817232131958008\n",
      "Epoch=40, iteration=3, train_loss=2.9480648040771484\n",
      "Epoch=40, val_loss=2.9347128868103027\n",
      "Epoch=41, iteration=0, train_loss=2.7932116985321045\n",
      "Epoch=41, iteration=1, train_loss=2.788818597793579\n",
      "Epoch=41, iteration=2, train_loss=2.8165338039398193\n",
      "Epoch=41, iteration=3, train_loss=2.9474456310272217\n",
      "Epoch=41, val_loss=2.9341564178466797\n",
      "Epoch=42, iteration=0, train_loss=2.792341709136963\n",
      "Epoch=42, iteration=1, train_loss=2.78788685798645\n",
      "Epoch=42, iteration=2, train_loss=2.815795660018921\n",
      "Epoch=42, iteration=3, train_loss=2.9467501640319824\n",
      "Epoch=42, val_loss=2.9335436820983887\n",
      "Epoch=43, iteration=0, train_loss=2.791426181793213\n",
      "Epoch=43, iteration=1, train_loss=2.786910057067871\n",
      "Epoch=43, iteration=2, train_loss=2.8150341510772705\n",
      "Epoch=43, iteration=3, train_loss=2.94600510597229\n",
      "Epoch=43, val_loss=2.932896137237549\n",
      "Epoch=44, iteration=0, train_loss=2.7904837131500244\n",
      "Epoch=44, iteration=1, train_loss=2.785909414291382\n",
      "Epoch=44, iteration=2, train_loss=2.814262866973877\n",
      "Epoch=44, iteration=3, train_loss=2.945230722427368\n",
      "Epoch=44, val_loss=2.93222975730896\n",
      "Epoch=45, iteration=0, train_loss=2.789530038833618\n",
      "Epoch=45, iteration=1, train_loss=2.7849013805389404\n",
      "Epoch=45, iteration=2, train_loss=2.813493490219116\n",
      "Epoch=45, iteration=3, train_loss=2.9444448947906494\n",
      "Epoch=45, val_loss=2.9315571784973145\n",
      "Epoch=46, iteration=0, train_loss=2.788578987121582\n",
      "Epoch=46, iteration=1, train_loss=2.7838988304138184\n",
      "Epoch=46, iteration=2, train_loss=2.8127334117889404\n",
      "Epoch=46, iteration=3, train_loss=2.9436585903167725\n",
      "Epoch=46, val_loss=2.930889129638672\n",
      "Epoch=47, iteration=0, train_loss=2.7876389026641846\n",
      "Epoch=47, iteration=1, train_loss=2.782911777496338\n",
      "Epoch=47, iteration=2, train_loss=2.811988115310669\n",
      "Epoch=47, iteration=3, train_loss=2.942882776260376\n",
      "Epoch=47, val_loss=2.930232286453247\n",
      "Epoch=48, iteration=0, train_loss=2.7867162227630615\n",
      "Epoch=48, iteration=1, train_loss=2.7819464206695557\n",
      "Epoch=48, iteration=2, train_loss=2.811262607574463\n",
      "Epoch=48, iteration=3, train_loss=2.942122459411621\n",
      "Epoch=48, val_loss=2.9295907020568848\n",
      "Epoch=49, iteration=0, train_loss=2.7858171463012695\n",
      "Epoch=49, iteration=1, train_loss=2.7810072898864746\n",
      "Epoch=49, iteration=2, train_loss=2.8105595111846924\n",
      "Epoch=49, iteration=3, train_loss=2.941382646560669\n",
      "Epoch=49, val_loss=2.9289679527282715\n",
      "Epoch=50, iteration=0, train_loss=2.784942865371704\n",
      "Epoch=50, iteration=1, train_loss=2.780097246170044\n",
      "Epoch=50, iteration=2, train_loss=2.809879779815674\n",
      "Epoch=50, iteration=3, train_loss=2.9406652450561523\n",
      "Epoch=50, val_loss=2.9283664226531982\n",
      "Epoch=51, iteration=0, train_loss=2.7840962409973145\n",
      "Epoch=51, iteration=1, train_loss=2.7792177200317383\n",
      "Epoch=51, iteration=2, train_loss=2.8092241287231445\n",
      "Epoch=51, iteration=3, train_loss=2.9399731159210205\n",
      "Epoch=51, val_loss=2.927785873413086\n",
      "Epoch=52, iteration=0, train_loss=2.7832772731781006\n",
      "Epoch=52, iteration=1, train_loss=2.778369665145874\n",
      "Epoch=52, iteration=2, train_loss=2.8085930347442627\n",
      "Epoch=52, iteration=3, train_loss=2.939305067062378\n",
      "Epoch=52, val_loss=2.9272279739379883\n",
      "Epoch=53, iteration=0, train_loss=2.782487154006958\n",
      "Epoch=53, iteration=1, train_loss=2.777553081512451\n",
      "Epoch=53, iteration=2, train_loss=2.8079864978790283\n",
      "Epoch=53, iteration=3, train_loss=2.938662528991699\n",
      "Epoch=53, val_loss=2.926692485809326\n",
      "Epoch=54, iteration=0, train_loss=2.781724691390991\n",
      "Epoch=54, iteration=1, train_loss=2.776766538619995\n",
      "Epoch=54, iteration=2, train_loss=2.807403087615967\n",
      "Epoch=54, iteration=3, train_loss=2.9380455017089844\n",
      "Epoch=54, val_loss=2.926178455352783\n",
      "Epoch=55, iteration=0, train_loss=2.7809898853302\n",
      "Epoch=55, iteration=1, train_loss=2.776010036468506\n",
      "Epoch=55, iteration=2, train_loss=2.8068435192108154\n",
      "Epoch=55, iteration=3, train_loss=2.9374523162841797\n",
      "Epoch=55, val_loss=2.925685405731201\n",
      "Epoch=56, iteration=0, train_loss=2.7802817821502686\n",
      "Epoch=56, iteration=1, train_loss=2.775282859802246\n",
      "Epoch=56, iteration=2, train_loss=2.806305408477783\n",
      "Epoch=56, iteration=3, train_loss=2.9368836879730225\n",
      "Epoch=56, val_loss=2.925212860107422\n",
      "Epoch=57, iteration=0, train_loss=2.779599189758301\n",
      "Epoch=57, iteration=1, train_loss=2.774583578109741\n",
      "Epoch=57, iteration=2, train_loss=2.80578875541687\n",
      "Epoch=57, iteration=3, train_loss=2.9363369941711426\n",
      "Epoch=57, val_loss=2.924760341644287\n",
      "Epoch=58, iteration=0, train_loss=2.778942108154297\n",
      "Epoch=58, iteration=1, train_loss=2.7739109992980957\n",
      "Epoch=58, iteration=2, train_loss=2.805293083190918\n",
      "Epoch=58, iteration=3, train_loss=2.9358131885528564\n",
      "Epoch=58, val_loss=2.9243266582489014\n",
      "Epoch=59, iteration=0, train_loss=2.778308868408203\n",
      "Epoch=59, iteration=1, train_loss=2.773264169692993\n",
      "Epoch=59, iteration=2, train_loss=2.804816484451294\n",
      "Epoch=59, iteration=3, train_loss=2.935310125350952\n",
      "Epoch=59, val_loss=2.9239108562469482\n",
      "Epoch=60, iteration=0, train_loss=2.777698516845703\n",
      "Epoch=60, iteration=1, train_loss=2.7726423740386963\n",
      "Epoch=60, iteration=2, train_loss=2.804358959197998\n",
      "Epoch=60, iteration=3, train_loss=2.934826374053955\n",
      "Epoch=60, val_loss=2.9235124588012695\n",
      "Epoch=61, iteration=0, train_loss=2.7771100997924805\n",
      "Epoch=61, iteration=1, train_loss=2.7720437049865723\n",
      "Epoch=61, iteration=2, train_loss=2.8039190769195557\n",
      "Epoch=61, iteration=3, train_loss=2.9343626499176025\n",
      "Epoch=61, val_loss=2.923130512237549\n",
      "Epoch=62, iteration=0, train_loss=2.776543140411377\n",
      "Epoch=62, iteration=1, train_loss=2.771467685699463\n",
      "Epoch=62, iteration=2, train_loss=2.8034958839416504\n",
      "Epoch=62, iteration=3, train_loss=2.93391752243042\n",
      "Epoch=62, val_loss=2.9227640628814697\n",
      "Epoch=63, iteration=0, train_loss=2.7759957313537598\n",
      "Epoch=63, iteration=1, train_loss=2.7709131240844727\n",
      "Epoch=63, iteration=2, train_loss=2.8030898571014404\n",
      "Epoch=63, iteration=3, train_loss=2.9334893226623535\n",
      "Epoch=63, val_loss=2.922412395477295\n",
      "Epoch=64, iteration=0, train_loss=2.775467872619629\n",
      "Epoch=64, iteration=1, train_loss=2.770378828048706\n",
      "Epoch=64, iteration=2, train_loss=2.802699089050293\n",
      "Epoch=64, iteration=3, train_loss=2.933077812194824\n",
      "Epoch=64, val_loss=2.922075033187866\n",
      "Epoch=65, iteration=0, train_loss=2.774958610534668\n",
      "Epoch=65, iteration=1, train_loss=2.769864082336426\n",
      "Epoch=65, iteration=2, train_loss=2.8023223876953125\n",
      "Epoch=65, iteration=3, train_loss=2.932682514190674\n",
      "Epoch=65, val_loss=2.921751022338867\n",
      "Epoch=66, iteration=0, train_loss=2.7744662761688232\n",
      "Epoch=66, iteration=1, train_loss=2.7693679332733154\n",
      "Epoch=66, iteration=2, train_loss=2.8019604682922363\n",
      "Epoch=66, iteration=3, train_loss=2.9323017597198486\n",
      "Epoch=66, val_loss=2.9214396476745605\n",
      "Epoch=67, iteration=0, train_loss=2.773991107940674\n",
      "Epoch=67, iteration=1, train_loss=2.7688894271850586\n",
      "Epoch=67, iteration=2, train_loss=2.80161190032959\n",
      "Epoch=67, iteration=3, train_loss=2.931936025619507\n",
      "Epoch=67, val_loss=2.9211409091949463\n",
      "Epoch=68, iteration=0, train_loss=2.773531913757324\n",
      "Epoch=68, iteration=1, train_loss=2.768427848815918\n",
      "Epoch=68, iteration=2, train_loss=2.8012757301330566\n",
      "Epoch=68, iteration=3, train_loss=2.9315834045410156\n",
      "Epoch=68, val_loss=2.92085337638855\n",
      "Epoch=69, iteration=0, train_loss=2.7730886936187744\n",
      "Epoch=69, iteration=1, train_loss=2.7679824829101562\n",
      "Epoch=69, iteration=2, train_loss=2.8009517192840576\n",
      "Epoch=69, iteration=3, train_loss=2.931243658065796\n",
      "Epoch=69, val_loss=2.920577049255371\n",
      "Epoch=70, iteration=0, train_loss=2.7726593017578125\n",
      "Epoch=70, iteration=1, train_loss=2.767552375793457\n",
      "Epoch=70, iteration=2, train_loss=2.8006391525268555\n",
      "Epoch=70, iteration=3, train_loss=2.9309167861938477\n",
      "Epoch=70, val_loss=2.920311450958252\n",
      "Epoch=71, iteration=0, train_loss=2.772244691848755\n",
      "Epoch=71, iteration=1, train_loss=2.767137050628662\n",
      "Epoch=71, iteration=2, train_loss=2.8003387451171875\n",
      "Epoch=71, iteration=3, train_loss=2.9306013584136963\n",
      "Epoch=71, val_loss=2.920055627822876\n",
      "Epoch=72, iteration=0, train_loss=2.7718427181243896\n",
      "Epoch=72, iteration=1, train_loss=2.7667360305786133\n",
      "Epoch=72, iteration=2, train_loss=2.8000481128692627\n",
      "Epoch=72, iteration=3, train_loss=2.9302971363067627\n",
      "Epoch=72, val_loss=2.919809103012085\n",
      "Epoch=73, iteration=0, train_loss=2.771454095840454\n",
      "Epoch=73, iteration=1, train_loss=2.766348123550415\n",
      "Epoch=73, iteration=2, train_loss=2.79976749420166\n",
      "Epoch=73, iteration=3, train_loss=2.930004835128784\n",
      "Epoch=73, val_loss=2.919572114944458\n",
      "Epoch=74, iteration=0, train_loss=2.771077871322632\n",
      "Epoch=74, iteration=1, train_loss=2.7659730911254883\n",
      "Epoch=74, iteration=2, train_loss=2.7994964122772217\n",
      "Epoch=74, iteration=3, train_loss=2.9297215938568115\n",
      "Epoch=74, val_loss=2.9193432331085205\n",
      "Epoch=75, iteration=0, train_loss=2.7707130908966064\n",
      "Epoch=75, iteration=1, train_loss=2.765610694885254\n",
      "Epoch=75, iteration=2, train_loss=2.7992353439331055\n",
      "Epoch=75, iteration=3, train_loss=2.929447889328003\n",
      "Epoch=75, val_loss=2.9191231727600098\n",
      "Epoch=76, iteration=0, train_loss=2.770359754562378\n",
      "Epoch=76, iteration=1, train_loss=2.7652597427368164\n",
      "Epoch=76, iteration=2, train_loss=2.7989823818206787\n",
      "Epoch=76, iteration=3, train_loss=2.9291839599609375\n",
      "Epoch=76, val_loss=2.918910026550293\n",
      "Epoch=77, iteration=0, train_loss=2.770017385482788\n",
      "Epoch=77, iteration=1, train_loss=2.764920473098755\n",
      "Epoch=77, iteration=2, train_loss=2.798738479614258\n",
      "Epoch=77, iteration=3, train_loss=2.9289283752441406\n",
      "Epoch=77, val_loss=2.9187049865722656\n",
      "Epoch=78, iteration=0, train_loss=2.7696852684020996\n",
      "Epoch=78, iteration=1, train_loss=2.7645909786224365\n",
      "Epoch=78, iteration=2, train_loss=2.798502206802368\n",
      "Epoch=78, iteration=3, train_loss=2.9286818504333496\n",
      "Epoch=78, val_loss=2.9185070991516113\n",
      "Epoch=79, iteration=0, train_loss=2.7693636417388916\n",
      "Epoch=79, iteration=1, train_loss=2.764272451400757\n",
      "Epoch=79, iteration=2, train_loss=2.798273801803589\n",
      "Epoch=79, iteration=3, train_loss=2.928443670272827\n",
      "Epoch=79, val_loss=2.9183156490325928\n",
      "Epoch=80, iteration=0, train_loss=2.7690508365631104\n",
      "Epoch=80, iteration=1, train_loss=2.7639641761779785\n",
      "Epoch=80, iteration=2, train_loss=2.7980527877807617\n",
      "Epoch=80, iteration=3, train_loss=2.928213119506836\n",
      "Epoch=80, val_loss=2.918131113052368\n",
      "Epoch=81, iteration=0, train_loss=2.768747091293335\n",
      "Epoch=81, iteration=1, train_loss=2.763664722442627\n",
      "Epoch=81, iteration=2, train_loss=2.7978389263153076\n",
      "Epoch=81, iteration=3, train_loss=2.9279894828796387\n",
      "Epoch=81, val_loss=2.917952537536621\n",
      "Epoch=82, iteration=0, train_loss=2.7684526443481445\n",
      "Epoch=82, iteration=1, train_loss=2.7633748054504395\n",
      "Epoch=82, iteration=2, train_loss=2.7976317405700684\n",
      "Epoch=82, iteration=3, train_loss=2.9277737140655518\n",
      "Epoch=82, val_loss=2.9177801609039307\n",
      "Epoch=83, iteration=0, train_loss=2.7681665420532227\n",
      "Epoch=83, iteration=1, train_loss=2.7630937099456787\n",
      "Epoch=83, iteration=2, train_loss=2.797431468963623\n",
      "Epoch=83, iteration=3, train_loss=2.9275646209716797\n",
      "Epoch=83, val_loss=2.9176132678985596\n",
      "Epoch=84, iteration=0, train_loss=2.7678887844085693\n",
      "Epoch=84, iteration=1, train_loss=2.7628207206726074\n",
      "Epoch=84, iteration=2, train_loss=2.7972371578216553\n",
      "Epoch=84, iteration=3, train_loss=2.927361488342285\n",
      "Epoch=84, val_loss=2.917452573776245\n",
      "Epoch=85, iteration=0, train_loss=2.7676188945770264\n",
      "Epoch=85, iteration=1, train_loss=2.762556314468384\n",
      "Epoch=85, iteration=2, train_loss=2.797048807144165\n",
      "Epoch=85, iteration=3, train_loss=2.9271645545959473\n",
      "Epoch=85, val_loss=2.9172961711883545\n",
      "Epoch=86, iteration=0, train_loss=2.7673566341400146\n",
      "Epoch=86, iteration=1, train_loss=2.762299060821533\n",
      "Epoch=86, iteration=2, train_loss=2.7968664169311523\n",
      "Epoch=86, iteration=3, train_loss=2.926973819732666\n",
      "Epoch=86, val_loss=2.917144775390625\n",
      "Epoch=87, iteration=0, train_loss=2.767101287841797\n",
      "Epoch=87, iteration=1, train_loss=2.7620491981506348\n",
      "Epoch=87, iteration=2, train_loss=2.796689510345459\n",
      "Epoch=87, iteration=3, train_loss=2.9267892837524414\n",
      "Epoch=87, val_loss=2.9169986248016357\n",
      "Epoch=88, iteration=0, train_loss=2.766853094100952\n",
      "Epoch=88, iteration=1, train_loss=2.7618067264556885\n",
      "Epoch=88, iteration=2, train_loss=2.7965176105499268\n",
      "Epoch=88, iteration=3, train_loss=2.926610231399536\n",
      "Epoch=88, val_loss=2.9168567657470703\n",
      "Epoch=89, iteration=0, train_loss=2.7666115760803223\n",
      "Epoch=89, iteration=1, train_loss=2.7615714073181152\n",
      "Epoch=89, iteration=2, train_loss=2.796351194381714\n",
      "Epoch=89, iteration=3, train_loss=2.926436185836792\n",
      "Epoch=89, val_loss=2.916719436645508\n",
      "Epoch=90, iteration=0, train_loss=2.7663769721984863\n",
      "Epoch=90, iteration=1, train_loss=2.761342763900757\n",
      "Epoch=90, iteration=2, train_loss=2.796189785003662\n",
      "Epoch=90, iteration=3, train_loss=2.926267385482788\n",
      "Epoch=90, val_loss=2.9165866374969482\n",
      "Epoch=91, iteration=0, train_loss=2.766148090362549\n",
      "Epoch=91, iteration=1, train_loss=2.761120319366455\n",
      "Epoch=91, iteration=2, train_loss=2.7960333824157715\n",
      "Epoch=91, iteration=3, train_loss=2.926103353500366\n",
      "Epoch=91, val_loss=2.916457176208496\n",
      "Epoch=92, iteration=0, train_loss=2.765925645828247\n",
      "Epoch=92, iteration=1, train_loss=2.76090407371521\n",
      "Epoch=92, iteration=2, train_loss=2.7958810329437256\n",
      "Epoch=92, iteration=3, train_loss=2.9259445667266846\n",
      "Epoch=92, val_loss=2.916332483291626\n",
      "Epoch=93, iteration=0, train_loss=2.7657089233398438\n",
      "Epoch=93, iteration=1, train_loss=2.7606942653656006\n",
      "Epoch=93, iteration=2, train_loss=2.7957332134246826\n",
      "Epoch=93, iteration=3, train_loss=2.9257898330688477\n",
      "Epoch=93, val_loss=2.9162113666534424\n",
      "Epoch=94, iteration=0, train_loss=2.765497922897339\n",
      "Epoch=94, iteration=1, train_loss=2.7604892253875732\n",
      "Epoch=94, iteration=2, train_loss=2.7955899238586426\n",
      "Epoch=94, iteration=3, train_loss=2.9256391525268555\n",
      "Epoch=94, val_loss=2.916093111038208\n",
      "Epoch=95, iteration=0, train_loss=2.7652924060821533\n",
      "Epoch=95, iteration=1, train_loss=2.7602906227111816\n",
      "Epoch=95, iteration=2, train_loss=2.7954509258270264\n",
      "Epoch=95, iteration=3, train_loss=2.9254932403564453\n",
      "Epoch=95, val_loss=2.9159791469573975\n",
      "Epoch=96, iteration=0, train_loss=2.765091896057129\n",
      "Epoch=96, iteration=1, train_loss=2.7600972652435303\n",
      "Epoch=96, iteration=2, train_loss=2.7953147888183594\n",
      "Epoch=96, iteration=3, train_loss=2.925351142883301\n",
      "Epoch=96, val_loss=2.9158682823181152\n",
      "Epoch=97, iteration=0, train_loss=2.764896869659424\n",
      "Epoch=97, iteration=1, train_loss=2.759908437728882\n",
      "Epoch=97, iteration=2, train_loss=2.795182943344116\n",
      "Epoch=97, iteration=3, train_loss=2.9252123832702637\n",
      "Epoch=97, val_loss=2.9157607555389404\n",
      "Epoch=98, iteration=0, train_loss=2.7647063732147217\n",
      "Epoch=98, iteration=1, train_loss=2.7597250938415527\n",
      "Epoch=98, iteration=2, train_loss=2.7950551509857178\n",
      "Epoch=98, iteration=3, train_loss=2.9250781536102295\n",
      "Epoch=98, val_loss=2.9156558513641357\n",
      "Epoch=99, iteration=0, train_loss=2.7645208835601807\n",
      "Epoch=99, iteration=1, train_loss=2.7595462799072266\n",
      "Epoch=99, iteration=2, train_loss=2.7949306964874268\n",
      "Epoch=99, iteration=3, train_loss=2.9249472618103027\n",
      "Epoch=99, val_loss=2.9155545234680176\n",
      "Epoch=100, iteration=0, train_loss=2.7643399238586426\n",
      "Epoch=100, iteration=1, train_loss=2.7593719959259033\n",
      "Epoch=100, iteration=2, train_loss=2.794809341430664\n",
      "Epoch=100, iteration=3, train_loss=2.9248199462890625\n",
      "Epoch=100, val_loss=2.9154551029205322\n",
      "Epoch=101, iteration=0, train_loss=2.7641634941101074\n",
      "Epoch=101, iteration=1, train_loss=2.759202241897583\n",
      "Epoch=101, iteration=2, train_loss=2.794691562652588\n",
      "Epoch=101, iteration=3, train_loss=2.924696445465088\n",
      "Epoch=101, val_loss=2.9153592586517334\n",
      "Epoch=102, iteration=0, train_loss=2.763990640640259\n",
      "Epoch=102, iteration=1, train_loss=2.7590370178222656\n",
      "Epoch=102, iteration=2, train_loss=2.794577121734619\n",
      "Epoch=102, iteration=3, train_loss=2.924575090408325\n",
      "Epoch=102, val_loss=2.9152657985687256\n",
      "Epoch=103, iteration=0, train_loss=2.763822317123413\n",
      "Epoch=103, iteration=1, train_loss=2.758876323699951\n",
      "Epoch=103, iteration=2, train_loss=2.7944655418395996\n",
      "Epoch=103, iteration=3, train_loss=2.924457550048828\n",
      "Epoch=103, val_loss=2.915174722671509\n",
      "Epoch=104, iteration=0, train_loss=2.7636587619781494\n",
      "Epoch=104, iteration=1, train_loss=2.7587192058563232\n",
      "Epoch=104, iteration=2, train_loss=2.7943570613861084\n",
      "Epoch=104, iteration=3, train_loss=2.9243428707122803\n",
      "Epoch=104, val_loss=2.915086507797241\n",
      "Epoch=105, iteration=0, train_loss=2.763498306274414\n",
      "Epoch=105, iteration=1, train_loss=2.758566379547119\n",
      "Epoch=105, iteration=2, train_loss=2.7942512035369873\n",
      "Epoch=105, iteration=3, train_loss=2.92423152923584\n",
      "Epoch=105, val_loss=2.9150006771087646\n",
      "Epoch=106, iteration=0, train_loss=2.7633419036865234\n",
      "Epoch=106, iteration=1, train_loss=2.7584168910980225\n",
      "Epoch=106, iteration=2, train_loss=2.7941482067108154\n",
      "Epoch=106, iteration=3, train_loss=2.9241228103637695\n",
      "Epoch=106, val_loss=2.914917230606079\n",
      "Epoch=107, iteration=0, train_loss=2.7631893157958984\n",
      "Epoch=107, iteration=1, train_loss=2.7582714557647705\n",
      "Epoch=107, iteration=2, train_loss=2.7940475940704346\n",
      "Epoch=107, iteration=3, train_loss=2.9240164756774902\n",
      "Epoch=107, val_loss=2.9148356914520264\n",
      "Epoch=108, iteration=0, train_loss=2.763040065765381\n",
      "Epoch=108, iteration=1, train_loss=2.758129119873047\n",
      "Epoch=108, iteration=2, train_loss=2.793950080871582\n",
      "Epoch=108, iteration=3, train_loss=2.923912525177002\n",
      "Epoch=108, val_loss=2.9147562980651855\n",
      "Epoch=109, iteration=0, train_loss=2.7628941535949707\n",
      "Epoch=109, iteration=1, train_loss=2.757990837097168\n",
      "Epoch=109, iteration=2, train_loss=2.7938547134399414\n",
      "Epoch=109, iteration=3, train_loss=2.923811435699463\n",
      "Epoch=109, val_loss=2.9146788120269775\n",
      "Epoch=110, iteration=0, train_loss=2.762751817703247\n",
      "Epoch=110, iteration=1, train_loss=2.7578554153442383\n",
      "Epoch=110, iteration=2, train_loss=2.793761730194092\n",
      "Epoch=110, iteration=3, train_loss=2.923713207244873\n",
      "Epoch=110, val_loss=2.9146034717559814\n",
      "Epoch=111, iteration=0, train_loss=2.7626125812530518\n",
      "Epoch=111, iteration=1, train_loss=2.757723569869995\n",
      "Epoch=111, iteration=2, train_loss=2.7936716079711914\n",
      "Epoch=111, iteration=3, train_loss=2.923617362976074\n",
      "Epoch=111, val_loss=2.914530038833618\n",
      "Epoch=112, iteration=0, train_loss=2.7624764442443848\n",
      "Epoch=112, iteration=1, train_loss=2.757594347000122\n",
      "Epoch=112, iteration=2, train_loss=2.7935831546783447\n",
      "Epoch=112, iteration=3, train_loss=2.923523187637329\n",
      "Epoch=112, val_loss=2.914458751678467\n",
      "Epoch=113, iteration=0, train_loss=2.762343406677246\n",
      "Epoch=113, iteration=1, train_loss=2.7574684619903564\n",
      "Epoch=113, iteration=2, train_loss=2.793497323989868\n",
      "Epoch=113, iteration=3, train_loss=2.923431396484375\n",
      "Epoch=113, val_loss=2.914389133453369\n",
      "Epoch=114, iteration=0, train_loss=2.7622134685516357\n",
      "Epoch=114, iteration=1, train_loss=2.757345676422119\n",
      "Epoch=114, iteration=2, train_loss=2.7934134006500244\n",
      "Epoch=114, iteration=3, train_loss=2.923341989517212\n",
      "Epoch=114, val_loss=2.9143214225769043\n",
      "Epoch=115, iteration=0, train_loss=2.7620861530303955\n",
      "Epoch=115, iteration=1, train_loss=2.75722599029541\n",
      "Epoch=115, iteration=2, train_loss=2.793332099914551\n",
      "Epoch=115, iteration=3, train_loss=2.9232547283172607\n",
      "Epoch=115, val_loss=2.914254903793335\n",
      "Epoch=116, iteration=0, train_loss=2.7619614601135254\n",
      "Epoch=116, iteration=1, train_loss=2.757108688354492\n",
      "Epoch=116, iteration=2, train_loss=2.7932522296905518\n",
      "Epoch=116, iteration=3, train_loss=2.9231691360473633\n",
      "Epoch=116, val_loss=2.9141905307769775\n",
      "Epoch=117, iteration=0, train_loss=2.7618396282196045\n",
      "Epoch=117, iteration=1, train_loss=2.7569942474365234\n",
      "Epoch=117, iteration=2, train_loss=2.7931745052337646\n",
      "Epoch=117, iteration=3, train_loss=2.9230854511260986\n",
      "Epoch=117, val_loss=2.9141271114349365\n",
      "Epoch=118, iteration=0, train_loss=2.761720657348633\n",
      "Epoch=118, iteration=1, train_loss=2.7568821907043457\n",
      "Epoch=118, iteration=2, train_loss=2.7930986881256104\n",
      "Epoch=118, iteration=3, train_loss=2.923004388809204\n",
      "Epoch=118, val_loss=2.9140658378601074\n",
      "Epoch=119, iteration=0, train_loss=2.7616045475006104\n",
      "Epoch=119, iteration=1, train_loss=2.756772518157959\n",
      "Epoch=119, iteration=2, train_loss=2.7930243015289307\n",
      "Epoch=119, iteration=3, train_loss=2.922924518585205\n",
      "Epoch=119, val_loss=2.914005756378174\n",
      "Epoch=120, iteration=0, train_loss=2.7614903450012207\n",
      "Epoch=120, iteration=1, train_loss=2.7566654682159424\n",
      "Epoch=120, iteration=2, train_loss=2.7929515838623047\n",
      "Epoch=120, iteration=3, train_loss=2.922846555709839\n",
      "Epoch=120, val_loss=2.913947343826294\n",
      "Epoch=121, iteration=0, train_loss=2.761378526687622\n",
      "Epoch=121, iteration=1, train_loss=2.756560802459717\n",
      "Epoch=121, iteration=2, train_loss=2.7928812503814697\n",
      "Epoch=121, iteration=3, train_loss=2.9227707386016846\n",
      "Epoch=121, val_loss=2.9138898849487305\n",
      "Epoch=122, iteration=0, train_loss=2.7612693309783936\n",
      "Epoch=122, iteration=1, train_loss=2.7564587593078613\n",
      "Epoch=122, iteration=2, train_loss=2.792811870574951\n",
      "Epoch=122, iteration=3, train_loss=2.922696113586426\n",
      "Epoch=122, val_loss=2.9138343334198\n",
      "Epoch=123, iteration=0, train_loss=2.7611618041992188\n",
      "Epoch=123, iteration=1, train_loss=2.7563586235046387\n",
      "Epoch=123, iteration=2, train_loss=2.7927446365356445\n",
      "Epoch=123, iteration=3, train_loss=2.9226231575012207\n",
      "Epoch=123, val_loss=2.9137799739837646\n",
      "Epoch=124, iteration=0, train_loss=2.761057138442993\n",
      "Epoch=124, iteration=1, train_loss=2.756260633468628\n",
      "Epoch=124, iteration=2, train_loss=2.7926788330078125\n",
      "Epoch=124, iteration=3, train_loss=2.9225516319274902\n",
      "Epoch=124, val_loss=2.913726568222046\n",
      "Epoch=125, iteration=0, train_loss=2.7609546184539795\n",
      "Epoch=125, iteration=1, train_loss=2.756164789199829\n",
      "Epoch=125, iteration=2, train_loss=2.792614698410034\n",
      "Epoch=125, iteration=3, train_loss=2.9224822521209717\n",
      "Epoch=125, val_loss=2.913674831390381\n",
      "Epoch=126, iteration=0, train_loss=2.7608537673950195\n",
      "Epoch=126, iteration=1, train_loss=2.7560715675354004\n",
      "Epoch=126, iteration=2, train_loss=2.7925515174865723\n",
      "Epoch=126, iteration=3, train_loss=2.9224135875701904\n",
      "Epoch=126, val_loss=2.913623809814453\n",
      "Epoch=127, iteration=0, train_loss=2.7607550621032715\n",
      "Epoch=127, iteration=1, train_loss=2.755979537963867\n",
      "Epoch=127, iteration=2, train_loss=2.792490243911743\n",
      "Epoch=127, iteration=3, train_loss=2.922346830368042\n",
      "Epoch=127, val_loss=2.91357421875\n",
      "Epoch=128, iteration=0, train_loss=2.7606587409973145\n",
      "Epoch=128, iteration=1, train_loss=2.755890130996704\n",
      "Epoch=128, iteration=2, train_loss=2.7924304008483887\n",
      "Epoch=128, iteration=3, train_loss=2.92228102684021\n",
      "Epoch=128, val_loss=2.9135255813598633\n",
      "Epoch=129, iteration=0, train_loss=2.760564088821411\n",
      "Epoch=129, iteration=1, train_loss=2.7558021545410156\n",
      "Epoch=129, iteration=2, train_loss=2.7923717498779297\n",
      "Epoch=129, iteration=3, train_loss=2.92221736907959\n",
      "Epoch=129, val_loss=2.913478374481201\n",
      "Epoch=130, iteration=0, train_loss=2.7604713439941406\n",
      "Epoch=130, iteration=1, train_loss=2.755715847015381\n",
      "Epoch=130, iteration=2, train_loss=2.7923145294189453\n",
      "Epoch=130, iteration=3, train_loss=2.922154426574707\n",
      "Epoch=130, val_loss=2.9134316444396973\n",
      "Epoch=131, iteration=0, train_loss=2.760380506515503\n",
      "Epoch=131, iteration=1, train_loss=2.755632162094116\n",
      "Epoch=131, iteration=2, train_loss=2.7922582626342773\n",
      "Epoch=131, iteration=3, train_loss=2.922093152999878\n",
      "Epoch=131, val_loss=2.913386106491089\n",
      "Epoch=132, iteration=0, train_loss=2.76029109954834\n",
      "Epoch=132, iteration=1, train_loss=2.7555501461029053\n",
      "Epoch=132, iteration=2, train_loss=2.792203664779663\n",
      "Epoch=132, iteration=3, train_loss=2.9220328330993652\n",
      "Epoch=132, val_loss=2.913341999053955\n",
      "Epoch=133, iteration=0, train_loss=2.7602038383483887\n",
      "Epoch=133, iteration=1, train_loss=2.75546932220459\n",
      "Epoch=133, iteration=2, train_loss=2.792149782180786\n",
      "Epoch=133, iteration=3, train_loss=2.9219744205474854\n",
      "Epoch=133, val_loss=2.9132986068725586\n",
      "Epoch=134, iteration=0, train_loss=2.760117530822754\n",
      "Epoch=134, iteration=1, train_loss=2.755390167236328\n",
      "Epoch=134, iteration=2, train_loss=2.792097330093384\n",
      "Epoch=134, iteration=3, train_loss=2.9219164848327637\n",
      "Epoch=134, val_loss=2.9132556915283203\n",
      "Epoch=135, iteration=0, train_loss=2.76003360748291\n",
      "Epoch=135, iteration=1, train_loss=2.75531268119812\n",
      "Epoch=135, iteration=2, train_loss=2.792046070098877\n",
      "Epoch=135, iteration=3, train_loss=2.9218599796295166\n",
      "Epoch=135, val_loss=2.913214683532715\n",
      "Epoch=136, iteration=0, train_loss=2.759951114654541\n",
      "Epoch=136, iteration=1, train_loss=2.755236864089966\n",
      "Epoch=136, iteration=2, train_loss=2.7919955253601074\n",
      "Epoch=136, iteration=3, train_loss=2.921804189682007\n",
      "Epoch=136, val_loss=2.9131741523742676\n",
      "Epoch=137, iteration=0, train_loss=2.7598698139190674\n",
      "Epoch=137, iteration=1, train_loss=2.755162477493286\n",
      "Epoch=137, iteration=2, train_loss=2.7919464111328125\n",
      "Epoch=137, iteration=3, train_loss=2.921750068664551\n",
      "Epoch=137, val_loss=2.9131340980529785\n",
      "Epoch=138, iteration=0, train_loss=2.7597906589508057\n",
      "Epoch=138, iteration=1, train_loss=2.755089521408081\n",
      "Epoch=138, iteration=2, train_loss=2.791898488998413\n",
      "Epoch=138, iteration=3, train_loss=2.921696662902832\n",
      "Epoch=138, val_loss=2.913095474243164\n",
      "Epoch=139, iteration=0, train_loss=2.7597126960754395\n",
      "Epoch=139, iteration=1, train_loss=2.7550182342529297\n",
      "Epoch=139, iteration=2, train_loss=2.79185152053833\n",
      "Epoch=139, iteration=3, train_loss=2.921644449234009\n",
      "Epoch=139, val_loss=2.913057327270508\n",
      "Epoch=140, iteration=0, train_loss=2.759636640548706\n",
      "Epoch=140, iteration=1, train_loss=2.754948139190674\n",
      "Epoch=140, iteration=2, train_loss=2.7918052673339844\n",
      "Epoch=140, iteration=3, train_loss=2.921593189239502\n",
      "Epoch=140, val_loss=2.9130194187164307\n",
      "Epoch=141, iteration=0, train_loss=2.759561061859131\n",
      "Epoch=141, iteration=1, train_loss=2.7548794746398926\n",
      "Epoch=141, iteration=2, train_loss=2.791760206222534\n",
      "Epoch=141, iteration=3, train_loss=2.9215424060821533\n",
      "Epoch=141, val_loss=2.9129831790924072\n",
      "Epoch=142, iteration=0, train_loss=2.7594876289367676\n",
      "Epoch=142, iteration=1, train_loss=2.754812002182007\n",
      "Epoch=142, iteration=2, train_loss=2.7917160987854004\n",
      "Epoch=142, iteration=3, train_loss=2.9214932918548584\n",
      "Epoch=142, val_loss=2.912947416305542\n",
      "Epoch=143, iteration=0, train_loss=2.7594151496887207\n",
      "Epoch=143, iteration=1, train_loss=2.754746437072754\n",
      "Epoch=143, iteration=2, train_loss=2.791672945022583\n",
      "Epoch=143, iteration=3, train_loss=2.921444892883301\n",
      "Epoch=143, val_loss=2.912912130355835\n",
      "Epoch=144, iteration=0, train_loss=2.7593441009521484\n",
      "Epoch=144, iteration=1, train_loss=2.7546815872192383\n",
      "Epoch=144, iteration=2, train_loss=2.791630506515503\n",
      "Epoch=144, iteration=3, train_loss=2.9213972091674805\n",
      "Epoch=144, val_loss=2.9128775596618652\n",
      "Epoch=145, iteration=0, train_loss=2.7592740058898926\n",
      "Epoch=145, iteration=1, train_loss=2.7546181678771973\n",
      "Epoch=145, iteration=2, train_loss=2.79158878326416\n",
      "Epoch=145, iteration=3, train_loss=2.9213509559631348\n",
      "Epoch=145, val_loss=2.912844181060791\n",
      "Epoch=146, iteration=0, train_loss=2.7592060565948486\n",
      "Epoch=146, iteration=1, train_loss=2.7545557022094727\n",
      "Epoch=146, iteration=2, train_loss=2.791548252105713\n",
      "Epoch=146, iteration=3, train_loss=2.9213051795959473\n",
      "Epoch=146, val_loss=2.912810802459717\n",
      "Epoch=147, iteration=0, train_loss=2.7591381072998047\n",
      "Epoch=147, iteration=1, train_loss=2.7544949054718018\n",
      "Epoch=147, iteration=2, train_loss=2.791508436203003\n",
      "Epoch=147, iteration=3, train_loss=2.921259880065918\n",
      "Epoch=147, val_loss=2.912778615951538\n",
      "Epoch=148, iteration=0, train_loss=2.7590720653533936\n",
      "Epoch=148, iteration=1, train_loss=2.754434585571289\n",
      "Epoch=148, iteration=2, train_loss=2.7914695739746094\n",
      "Epoch=148, iteration=3, train_loss=2.9212162494659424\n",
      "Epoch=148, val_loss=2.9127471446990967\n",
      "Epoch=149, iteration=0, train_loss=2.759006977081299\n",
      "Epoch=149, iteration=1, train_loss=2.754375696182251\n",
      "Epoch=149, iteration=2, train_loss=2.791431188583374\n",
      "Epoch=149, iteration=3, train_loss=2.9211723804473877\n",
      "Epoch=149, val_loss=2.9127159118652344\n",
      "Epoch=150, iteration=0, train_loss=2.7589433193206787\n",
      "Epoch=150, iteration=1, train_loss=2.7543182373046875\n",
      "Epoch=150, iteration=2, train_loss=2.791393518447876\n",
      "Epoch=150, iteration=3, train_loss=2.9211299419403076\n",
      "Epoch=150, val_loss=2.9126856327056885\n",
      "Epoch=151, iteration=0, train_loss=2.758880376815796\n",
      "Epoch=151, iteration=1, train_loss=2.7542612552642822\n",
      "Epoch=151, iteration=2, train_loss=2.7913568019866943\n",
      "Epoch=151, iteration=3, train_loss=2.9210877418518066\n",
      "Epoch=151, val_loss=2.91265606880188\n",
      "Epoch=152, iteration=0, train_loss=2.7588188648223877\n",
      "Epoch=152, iteration=1, train_loss=2.7542061805725098\n",
      "Epoch=152, iteration=2, train_loss=2.791321039199829\n",
      "Epoch=152, iteration=3, train_loss=2.921046733856201\n",
      "Epoch=152, val_loss=2.9126267433166504\n",
      "Epoch=153, iteration=0, train_loss=2.758758068084717\n",
      "Epoch=153, iteration=1, train_loss=2.7541513442993164\n",
      "Epoch=153, iteration=2, train_loss=2.791285514831543\n",
      "Epoch=153, iteration=3, train_loss=2.921006202697754\n",
      "Epoch=153, val_loss=2.912597894668579\n",
      "Epoch=154, iteration=0, train_loss=2.7586984634399414\n",
      "Epoch=154, iteration=1, train_loss=2.7540977001190186\n",
      "Epoch=154, iteration=2, train_loss=2.7912509441375732\n",
      "Epoch=154, iteration=3, train_loss=2.920966863632202\n",
      "Epoch=154, val_loss=2.912569761276245\n",
      "Epoch=155, iteration=0, train_loss=2.7586398124694824\n",
      "Epoch=155, iteration=1, train_loss=2.754045009613037\n",
      "Epoch=155, iteration=2, train_loss=2.791217088699341\n",
      "Epoch=155, iteration=3, train_loss=2.9209277629852295\n",
      "Epoch=155, val_loss=2.9125421047210693\n",
      "Epoch=156, iteration=0, train_loss=2.7585818767547607\n",
      "Epoch=156, iteration=1, train_loss=2.753993511199951\n",
      "Epoch=156, iteration=2, train_loss=2.7911834716796875\n",
      "Epoch=156, iteration=3, train_loss=2.920889139175415\n",
      "Epoch=156, val_loss=2.9125146865844727\n",
      "Epoch=157, iteration=0, train_loss=2.7585256099700928\n",
      "Epoch=157, iteration=1, train_loss=2.7539424896240234\n",
      "Epoch=157, iteration=2, train_loss=2.7911510467529297\n",
      "Epoch=157, iteration=3, train_loss=2.920851707458496\n",
      "Epoch=157, val_loss=2.9124882221221924\n",
      "Epoch=158, iteration=0, train_loss=2.758469820022583\n",
      "Epoch=158, iteration=1, train_loss=2.753892660140991\n",
      "Epoch=158, iteration=2, train_loss=2.791118860244751\n",
      "Epoch=158, iteration=3, train_loss=2.9208152294158936\n",
      "Epoch=158, val_loss=2.912461996078491\n",
      "Epoch=159, iteration=0, train_loss=2.7584147453308105\n",
      "Epoch=159, iteration=1, train_loss=2.7538437843322754\n",
      "Epoch=159, iteration=2, train_loss=2.7910873889923096\n",
      "Epoch=159, iteration=3, train_loss=2.920778751373291\n",
      "Epoch=159, val_loss=2.9124364852905273\n",
      "Epoch=160, iteration=0, train_loss=2.758361339569092\n",
      "Epoch=160, iteration=1, train_loss=2.753795623779297\n",
      "Epoch=160, iteration=2, train_loss=2.7910568714141846\n",
      "Epoch=160, iteration=3, train_loss=2.9207425117492676\n",
      "Epoch=160, val_loss=2.912411689758301\n",
      "Epoch=161, iteration=0, train_loss=2.758307933807373\n",
      "Epoch=161, iteration=1, train_loss=2.7537481784820557\n",
      "Epoch=161, iteration=2, train_loss=2.7910263538360596\n",
      "Epoch=161, iteration=3, train_loss=2.9207072257995605\n",
      "Epoch=161, val_loss=2.912386655807495\n",
      "Epoch=162, iteration=0, train_loss=2.75825572013855\n",
      "Epoch=162, iteration=1, train_loss=2.75370192527771\n",
      "Epoch=162, iteration=2, train_loss=2.790996789932251\n",
      "Epoch=162, iteration=3, train_loss=2.920673131942749\n",
      "Epoch=162, val_loss=2.9123623371124268\n",
      "Epoch=163, iteration=0, train_loss=2.758204698562622\n",
      "Epoch=163, iteration=1, train_loss=2.7536559104919434\n",
      "Epoch=163, iteration=2, train_loss=2.7909674644470215\n",
      "Epoch=163, iteration=3, train_loss=2.9206387996673584\n",
      "Epoch=163, val_loss=2.9123387336730957\n",
      "Epoch=164, iteration=0, train_loss=2.7581536769866943\n",
      "Epoch=164, iteration=1, train_loss=2.753610849380493\n",
      "Epoch=164, iteration=2, train_loss=2.79093861579895\n",
      "Epoch=164, iteration=3, train_loss=2.9206056594848633\n",
      "Epoch=164, val_loss=2.9123151302337646\n",
      "Epoch=165, iteration=0, train_loss=2.758104085922241\n",
      "Epoch=165, iteration=1, train_loss=2.7535669803619385\n",
      "Epoch=165, iteration=2, train_loss=2.7909107208251953\n",
      "Epoch=165, iteration=3, train_loss=2.920572280883789\n",
      "Epoch=165, val_loss=2.912292003631592\n",
      "Epoch=166, iteration=0, train_loss=2.758054733276367\n",
      "Epoch=166, iteration=1, train_loss=2.753523588180542\n",
      "Epoch=166, iteration=2, train_loss=2.7908830642700195\n",
      "Epoch=166, iteration=3, train_loss=2.9205400943756104\n",
      "Epoch=166, val_loss=2.912269353866577\n",
      "Epoch=167, iteration=0, train_loss=2.7580065727233887\n",
      "Epoch=167, iteration=1, train_loss=2.7534806728363037\n",
      "Epoch=167, iteration=2, train_loss=2.790856122970581\n",
      "Epoch=167, iteration=3, train_loss=2.92050838470459\n",
      "Epoch=167, val_loss=2.9122471809387207\n",
      "Epoch=168, iteration=0, train_loss=2.7579591274261475\n",
      "Epoch=168, iteration=1, train_loss=2.753438949584961\n",
      "Epoch=168, iteration=2, train_loss=2.7908291816711426\n",
      "Epoch=168, iteration=3, train_loss=2.9204769134521484\n",
      "Epoch=168, val_loss=2.912226438522339\n",
      "Epoch=169, iteration=0, train_loss=2.7579123973846436\n",
      "Epoch=169, iteration=1, train_loss=2.7533974647521973\n",
      "Epoch=169, iteration=2, train_loss=2.7908031940460205\n",
      "Epoch=169, iteration=3, train_loss=2.920445680618286\n",
      "Epoch=169, val_loss=2.9122045040130615\n",
      "Epoch=170, iteration=0, train_loss=2.757866144180298\n",
      "Epoch=170, iteration=1, train_loss=2.753356695175171\n",
      "Epoch=170, iteration=2, train_loss=2.7907776832580566\n",
      "Epoch=170, iteration=3, train_loss=2.920415163040161\n",
      "Epoch=170, val_loss=2.9121835231781006\n",
      "Epoch=171, iteration=0, train_loss=2.7578210830688477\n",
      "Epoch=171, iteration=1, train_loss=2.753316879272461\n",
      "Epoch=171, iteration=2, train_loss=2.7907521724700928\n",
      "Epoch=171, iteration=3, train_loss=2.9203853607177734\n",
      "Epoch=171, val_loss=2.9121625423431396\n",
      "Epoch=172, iteration=0, train_loss=2.7577762603759766\n",
      "Epoch=172, iteration=1, train_loss=2.753277540206909\n",
      "Epoch=172, iteration=2, train_loss=2.790727376937866\n",
      "Epoch=172, iteration=3, train_loss=2.9203555583953857\n",
      "Epoch=172, val_loss=2.912142515182495\n",
      "Epoch=173, iteration=0, train_loss=2.757732629776001\n",
      "Epoch=173, iteration=1, train_loss=2.753239154815674\n",
      "Epoch=173, iteration=2, train_loss=2.790703535079956\n",
      "Epoch=173, iteration=3, train_loss=2.9203269481658936\n",
      "Epoch=173, val_loss=2.9121224880218506\n",
      "Epoch=174, iteration=0, train_loss=2.7576889991760254\n",
      "Epoch=174, iteration=1, train_loss=2.7532010078430176\n",
      "Epoch=174, iteration=2, train_loss=2.7906792163848877\n",
      "Epoch=174, iteration=3, train_loss=2.9202983379364014\n",
      "Epoch=174, val_loss=2.9121029376983643\n",
      "Epoch=175, iteration=0, train_loss=2.757645845413208\n",
      "Epoch=175, iteration=1, train_loss=2.7531633377075195\n",
      "Epoch=175, iteration=2, train_loss=2.7906553745269775\n",
      "Epoch=175, iteration=3, train_loss=2.9202699661254883\n",
      "Epoch=175, val_loss=2.912083625793457\n",
      "Epoch=176, iteration=0, train_loss=2.7576041221618652\n",
      "Epoch=176, iteration=1, train_loss=2.753126621246338\n",
      "Epoch=176, iteration=2, train_loss=2.790632486343384\n",
      "Epoch=176, iteration=3, train_loss=2.9202418327331543\n",
      "Epoch=176, val_loss=2.912065029144287\n",
      "Epoch=177, iteration=0, train_loss=2.7575628757476807\n",
      "Epoch=177, iteration=1, train_loss=2.7530903816223145\n",
      "Epoch=177, iteration=2, train_loss=2.790609359741211\n",
      "Epoch=177, iteration=3, train_loss=2.920214891433716\n",
      "Epoch=177, val_loss=2.912046194076538\n",
      "Epoch=178, iteration=0, train_loss=2.757521867752075\n",
      "Epoch=178, iteration=1, train_loss=2.753054618835449\n",
      "Epoch=178, iteration=2, train_loss=2.7905874252319336\n",
      "Epoch=178, iteration=3, train_loss=2.9201881885528564\n",
      "Epoch=178, val_loss=2.912027597427368\n",
      "Epoch=179, iteration=0, train_loss=2.757481813430786\n",
      "Epoch=179, iteration=1, train_loss=2.753019332885742\n",
      "Epoch=179, iteration=2, train_loss=2.790565013885498\n",
      "Epoch=179, iteration=3, train_loss=2.920161485671997\n",
      "Epoch=179, val_loss=2.9120099544525146\n",
      "Epoch=180, iteration=0, train_loss=2.757441759109497\n",
      "Epoch=180, iteration=1, train_loss=2.7529847621917725\n",
      "Epoch=180, iteration=2, train_loss=2.790543794631958\n",
      "Epoch=180, iteration=3, train_loss=2.920135021209717\n",
      "Epoch=180, val_loss=2.911992073059082\n",
      "Epoch=181, iteration=0, train_loss=2.7574026584625244\n",
      "Epoch=181, iteration=1, train_loss=2.75295090675354\n",
      "Epoch=181, iteration=2, train_loss=2.790522336959839\n",
      "Epoch=181, iteration=3, train_loss=2.920109748840332\n",
      "Epoch=181, val_loss=2.9119741916656494\n",
      "Epoch=182, iteration=0, train_loss=2.75736403465271\n",
      "Epoch=182, iteration=1, train_loss=2.7529172897338867\n",
      "Epoch=182, iteration=2, train_loss=2.790501594543457\n",
      "Epoch=182, iteration=3, train_loss=2.9200847148895264\n",
      "Epoch=182, val_loss=2.911957263946533\n",
      "Epoch=183, iteration=0, train_loss=2.7573258876800537\n",
      "Epoch=183, iteration=1, train_loss=2.7528841495513916\n",
      "Epoch=183, iteration=2, train_loss=2.790480613708496\n",
      "Epoch=183, iteration=3, train_loss=2.9200594425201416\n",
      "Epoch=183, val_loss=2.911940336227417\n",
      "Epoch=184, iteration=0, train_loss=2.757288694381714\n",
      "Epoch=184, iteration=1, train_loss=2.752851724624634\n",
      "Epoch=184, iteration=2, train_loss=2.7904605865478516\n",
      "Epoch=184, iteration=3, train_loss=2.920034646987915\n",
      "Epoch=184, val_loss=2.911923885345459\n",
      "Epoch=185, iteration=0, train_loss=2.757251501083374\n",
      "Epoch=185, iteration=1, train_loss=2.752819776535034\n",
      "Epoch=185, iteration=2, train_loss=2.790440559387207\n",
      "Epoch=185, iteration=3, train_loss=2.9200098514556885\n",
      "Epoch=185, val_loss=2.911907434463501\n",
      "Epoch=186, iteration=0, train_loss=2.7572150230407715\n",
      "Epoch=186, iteration=1, train_loss=2.7527878284454346\n",
      "Epoch=186, iteration=2, train_loss=2.7904210090637207\n",
      "Epoch=186, iteration=3, train_loss=2.9199864864349365\n",
      "Epoch=186, val_loss=2.911891460418701\n",
      "Epoch=187, iteration=0, train_loss=2.7571792602539062\n",
      "Epoch=187, iteration=1, train_loss=2.7527568340301514\n",
      "Epoch=187, iteration=2, train_loss=2.7904019355773926\n",
      "Epoch=187, iteration=3, train_loss=2.9199624061584473\n",
      "Epoch=187, val_loss=2.9118757247924805\n",
      "Epoch=188, iteration=0, train_loss=2.757143974304199\n",
      "Epoch=188, iteration=1, train_loss=2.7527265548706055\n",
      "Epoch=188, iteration=2, train_loss=2.7903828620910645\n",
      "Epoch=188, iteration=3, train_loss=2.919938802719116\n",
      "Epoch=188, val_loss=2.9118599891662598\n",
      "Epoch=189, iteration=0, train_loss=2.757108688354492\n",
      "Epoch=189, iteration=1, train_loss=2.7526962757110596\n",
      "Epoch=189, iteration=2, train_loss=2.7903645038604736\n",
      "Epoch=189, iteration=3, train_loss=2.9199161529541016\n",
      "Epoch=189, val_loss=2.9118452072143555\n",
      "Epoch=190, iteration=0, train_loss=2.7570738792419434\n",
      "Epoch=190, iteration=1, train_loss=2.7526662349700928\n",
      "Epoch=190, iteration=2, train_loss=2.790346145629883\n",
      "Epoch=190, iteration=3, train_loss=2.919893264770508\n",
      "Epoch=190, val_loss=2.911829710006714\n",
      "Epoch=191, iteration=0, train_loss=2.757039785385132\n",
      "Epoch=191, iteration=1, train_loss=2.7526369094848633\n",
      "Epoch=191, iteration=2, train_loss=2.790327548980713\n",
      "Epoch=191, iteration=3, train_loss=2.9198710918426514\n",
      "Epoch=191, val_loss=2.9118154048919678\n",
      "Epoch=192, iteration=0, train_loss=2.7570061683654785\n",
      "Epoch=192, iteration=1, train_loss=2.752608060836792\n",
      "Epoch=192, iteration=2, train_loss=2.7903103828430176\n",
      "Epoch=192, iteration=3, train_loss=2.919848918914795\n",
      "Epoch=192, val_loss=2.9118006229400635\n",
      "Epoch=193, iteration=0, train_loss=2.7569730281829834\n",
      "Epoch=193, iteration=1, train_loss=2.752579689025879\n",
      "Epoch=193, iteration=2, train_loss=2.790292978286743\n",
      "Epoch=193, iteration=3, train_loss=2.9198269844055176\n",
      "Epoch=193, val_loss=2.9117863178253174\n",
      "Epoch=194, iteration=0, train_loss=2.7569403648376465\n",
      "Epoch=194, iteration=1, train_loss=2.752551555633545\n",
      "Epoch=194, iteration=2, train_loss=2.7902755737304688\n",
      "Epoch=194, iteration=3, train_loss=2.9198060035705566\n",
      "Epoch=194, val_loss=2.9117722511291504\n",
      "Epoch=195, iteration=0, train_loss=2.7569077014923096\n",
      "Epoch=195, iteration=1, train_loss=2.75252366065979\n",
      "Epoch=195, iteration=2, train_loss=2.7902584075927734\n",
      "Epoch=195, iteration=3, train_loss=2.9197845458984375\n",
      "Epoch=195, val_loss=2.9117579460144043\n",
      "Epoch=196, iteration=0, train_loss=2.756875991821289\n",
      "Epoch=196, iteration=1, train_loss=2.7524962425231934\n",
      "Epoch=196, iteration=2, train_loss=2.7902419567108154\n",
      "Epoch=196, iteration=3, train_loss=2.9197635650634766\n",
      "Epoch=196, val_loss=2.9117441177368164\n",
      "Epoch=197, iteration=0, train_loss=2.7568440437316895\n",
      "Epoch=197, iteration=1, train_loss=2.752469301223755\n",
      "Epoch=197, iteration=2, train_loss=2.7902252674102783\n",
      "Epoch=197, iteration=3, train_loss=2.9197425842285156\n",
      "Epoch=197, val_loss=2.9117307662963867\n",
      "Epoch=198, iteration=0, train_loss=2.7568132877349854\n",
      "Epoch=198, iteration=1, train_loss=2.7524428367614746\n",
      "Epoch=198, iteration=2, train_loss=2.7902092933654785\n",
      "Epoch=198, iteration=3, train_loss=2.919722557067871\n",
      "Epoch=198, val_loss=2.911717414855957\n",
      "Epoch=199, iteration=0, train_loss=2.7567827701568604\n",
      "Epoch=199, iteration=1, train_loss=2.7524166107177734\n",
      "Epoch=199, iteration=2, train_loss=2.7901933193206787\n",
      "Epoch=199, iteration=3, train_loss=2.9197025299072266\n",
      "Epoch=199, val_loss=2.9117045402526855\n",
      "Epoch=200, iteration=0, train_loss=2.7567520141601562\n",
      "Epoch=200, iteration=1, train_loss=2.7523913383483887\n",
      "Epoch=200, iteration=2, train_loss=2.790177583694458\n",
      "Epoch=200, iteration=3, train_loss=2.919682502746582\n",
      "Epoch=200, val_loss=2.911691427230835\n",
      "Epoch=201, iteration=0, train_loss=2.7567224502563477\n",
      "Epoch=201, iteration=1, train_loss=2.7523655891418457\n",
      "Epoch=201, iteration=2, train_loss=2.7901618480682373\n",
      "Epoch=201, iteration=3, train_loss=2.9196629524230957\n",
      "Epoch=201, val_loss=2.9116787910461426\n",
      "Epoch=202, iteration=0, train_loss=2.75669264793396\n",
      "Epoch=202, iteration=1, train_loss=2.752340078353882\n",
      "Epoch=202, iteration=2, train_loss=2.790146589279175\n",
      "Epoch=202, iteration=3, train_loss=2.9196441173553467\n",
      "Epoch=202, val_loss=2.91166615486145\n",
      "Epoch=203, iteration=0, train_loss=2.7566635608673096\n",
      "Epoch=203, iteration=1, train_loss=2.7523157596588135\n",
      "Epoch=203, iteration=2, train_loss=2.7901315689086914\n",
      "Epoch=203, iteration=3, train_loss=2.9196245670318604\n",
      "Epoch=203, val_loss=2.911653995513916\n",
      "Epoch=204, iteration=0, train_loss=2.7566347122192383\n",
      "Epoch=204, iteration=1, train_loss=2.752290964126587\n",
      "Epoch=204, iteration=2, train_loss=2.790116786956787\n",
      "Epoch=204, iteration=3, train_loss=2.9196057319641113\n",
      "Epoch=204, val_loss=2.911641836166382\n",
      "Epoch=205, iteration=0, train_loss=2.756606101989746\n",
      "Epoch=205, iteration=1, train_loss=2.7522668838500977\n",
      "Epoch=205, iteration=2, train_loss=2.790102243423462\n",
      "Epoch=205, iteration=3, train_loss=2.9195868968963623\n",
      "Epoch=205, val_loss=2.9116296768188477\n",
      "Epoch=206, iteration=0, train_loss=2.756577730178833\n",
      "Epoch=206, iteration=1, train_loss=2.7522430419921875\n",
      "Epoch=206, iteration=2, train_loss=2.7900876998901367\n",
      "Epoch=206, iteration=3, train_loss=2.9195687770843506\n",
      "Epoch=206, val_loss=2.9116177558898926\n",
      "Epoch=207, iteration=0, train_loss=2.756549835205078\n",
      "Epoch=207, iteration=1, train_loss=2.7522194385528564\n",
      "Epoch=207, iteration=2, train_loss=2.7900733947753906\n",
      "Epoch=207, iteration=3, train_loss=2.919550657272339\n",
      "Epoch=207, val_loss=2.9116060733795166\n",
      "Epoch=208, iteration=0, train_loss=2.7565226554870605\n",
      "Epoch=208, iteration=1, train_loss=2.7521963119506836\n",
      "Epoch=208, iteration=2, train_loss=2.7900595664978027\n",
      "Epoch=208, iteration=3, train_loss=2.919532299041748\n",
      "Epoch=208, val_loss=2.911594867706299\n",
      "Epoch=209, iteration=0, train_loss=2.756495714187622\n",
      "Epoch=209, iteration=1, train_loss=2.75217342376709\n",
      "Epoch=209, iteration=2, train_loss=2.790045738220215\n",
      "Epoch=209, iteration=3, train_loss=2.9195148944854736\n",
      "Epoch=209, val_loss=2.911583662033081\n",
      "Epoch=210, iteration=0, train_loss=2.7564687728881836\n",
      "Epoch=210, iteration=1, train_loss=2.752150774002075\n",
      "Epoch=210, iteration=2, train_loss=2.790032148361206\n",
      "Epoch=210, iteration=3, train_loss=2.919497489929199\n",
      "Epoch=210, val_loss=2.911572217941284\n",
      "Epoch=211, iteration=0, train_loss=2.756442070007324\n",
      "Epoch=211, iteration=1, train_loss=2.7521286010742188\n",
      "Epoch=211, iteration=2, train_loss=2.7900185585021973\n",
      "Epoch=211, iteration=3, train_loss=2.919480085372925\n",
      "Epoch=211, val_loss=2.9115612506866455\n",
      "Epoch=212, iteration=0, train_loss=2.756415843963623\n",
      "Epoch=212, iteration=1, train_loss=2.7521066665649414\n",
      "Epoch=212, iteration=2, train_loss=2.7900054454803467\n",
      "Epoch=212, iteration=3, train_loss=2.9194629192352295\n",
      "Epoch=212, val_loss=2.911550760269165\n",
      "Epoch=213, iteration=0, train_loss=2.75639009475708\n",
      "Epoch=213, iteration=1, train_loss=2.752084732055664\n",
      "Epoch=213, iteration=2, train_loss=2.789992570877075\n",
      "Epoch=213, iteration=3, train_loss=2.919445753097534\n",
      "Epoch=213, val_loss=2.9115400314331055\n",
      "Epoch=214, iteration=0, train_loss=2.756364583969116\n",
      "Epoch=214, iteration=1, train_loss=2.752063274383545\n",
      "Epoch=214, iteration=2, train_loss=2.7899796962738037\n",
      "Epoch=214, iteration=3, train_loss=2.919429063796997\n",
      "Epoch=214, val_loss=2.911529541015625\n",
      "Epoch=215, iteration=0, train_loss=2.7563390731811523\n",
      "Epoch=215, iteration=1, train_loss=2.752042293548584\n",
      "Epoch=215, iteration=2, train_loss=2.7899670600891113\n",
      "Epoch=215, iteration=3, train_loss=2.919412851333618\n",
      "Epoch=215, val_loss=2.9115192890167236\n",
      "Epoch=216, iteration=0, train_loss=2.756314277648926\n",
      "Epoch=216, iteration=1, train_loss=2.752021312713623\n",
      "Epoch=216, iteration=2, train_loss=2.789954423904419\n",
      "Epoch=216, iteration=3, train_loss=2.91939640045166\n",
      "Epoch=216, val_loss=2.911508560180664\n",
      "Epoch=217, iteration=0, train_loss=2.75628924369812\n",
      "Epoch=217, iteration=1, train_loss=2.752000331878662\n",
      "Epoch=217, iteration=2, train_loss=2.7899417877197266\n",
      "Epoch=217, iteration=3, train_loss=2.9193804264068604\n",
      "Epoch=217, val_loss=2.9114990234375\n",
      "Epoch=218, iteration=0, train_loss=2.756265163421631\n",
      "Epoch=218, iteration=1, train_loss=2.7519800662994385\n",
      "Epoch=218, iteration=2, train_loss=2.7899298667907715\n",
      "Epoch=218, iteration=3, train_loss=2.9193646907806396\n",
      "Epoch=218, val_loss=2.9114890098571777\n",
      "Epoch=219, iteration=0, train_loss=2.7562406063079834\n",
      "Epoch=219, iteration=1, train_loss=2.751960039138794\n",
      "Epoch=219, iteration=2, train_loss=2.7899179458618164\n",
      "Epoch=219, iteration=3, train_loss=2.919348955154419\n",
      "Epoch=219, val_loss=2.9114789962768555\n",
      "Epoch=220, iteration=0, train_loss=2.7562170028686523\n",
      "Epoch=220, iteration=1, train_loss=2.7519400119781494\n",
      "Epoch=220, iteration=2, train_loss=2.7899060249328613\n",
      "Epoch=220, iteration=3, train_loss=2.919332981109619\n",
      "Epoch=220, val_loss=2.9114694595336914\n",
      "Epoch=221, iteration=0, train_loss=2.756193161010742\n",
      "Epoch=221, iteration=1, train_loss=2.751920461654663\n",
      "Epoch=221, iteration=2, train_loss=2.7898943424224854\n",
      "Epoch=221, iteration=3, train_loss=2.9193177223205566\n",
      "Epoch=221, val_loss=2.9114599227905273\n",
      "Epoch=222, iteration=0, train_loss=2.756169557571411\n",
      "Epoch=222, iteration=1, train_loss=2.751901149749756\n",
      "Epoch=222, iteration=2, train_loss=2.7898831367492676\n",
      "Epoch=222, iteration=3, train_loss=2.9193027019500732\n",
      "Epoch=222, val_loss=2.9114506244659424\n",
      "Epoch=223, iteration=0, train_loss=2.7561466693878174\n",
      "Epoch=223, iteration=1, train_loss=2.7518818378448486\n",
      "Epoch=223, iteration=2, train_loss=2.7898712158203125\n",
      "Epoch=223, iteration=3, train_loss=2.9192874431610107\n",
      "Epoch=223, val_loss=2.9114413261413574\n",
      "Epoch=224, iteration=0, train_loss=2.7561235427856445\n",
      "Epoch=224, iteration=1, train_loss=2.7518625259399414\n",
      "Epoch=224, iteration=2, train_loss=2.789860248565674\n",
      "Epoch=224, iteration=3, train_loss=2.9192728996276855\n",
      "Epoch=224, val_loss=2.9114322662353516\n",
      "Epoch=225, iteration=0, train_loss=2.756100654602051\n",
      "Epoch=225, iteration=1, train_loss=2.7518436908721924\n",
      "Epoch=225, iteration=2, train_loss=2.7898495197296143\n",
      "Epoch=225, iteration=3, train_loss=2.9192588329315186\n",
      "Epoch=225, val_loss=2.9114229679107666\n",
      "Epoch=226, iteration=0, train_loss=2.7560784816741943\n",
      "Epoch=226, iteration=1, train_loss=2.7518255710601807\n",
      "Epoch=226, iteration=2, train_loss=2.7898380756378174\n",
      "Epoch=226, iteration=3, train_loss=2.9192440509796143\n",
      "Epoch=226, val_loss=2.911414623260498\n",
      "Epoch=227, iteration=0, train_loss=2.756056547164917\n",
      "Epoch=227, iteration=1, train_loss=2.751807451248169\n",
      "Epoch=227, iteration=2, train_loss=2.789827585220337\n",
      "Epoch=227, iteration=3, train_loss=2.91922926902771\n",
      "Epoch=227, val_loss=2.9114058017730713\n",
      "Epoch=228, iteration=0, train_loss=2.7560343742370605\n",
      "Epoch=228, iteration=1, train_loss=2.7517893314361572\n",
      "Epoch=228, iteration=2, train_loss=2.7898173332214355\n",
      "Epoch=228, iteration=3, train_loss=2.919214963912964\n",
      "Epoch=228, val_loss=2.9113969802856445\n",
      "Epoch=229, iteration=0, train_loss=2.7560129165649414\n",
      "Epoch=229, iteration=1, train_loss=2.7517714500427246\n",
      "Epoch=229, iteration=2, train_loss=2.7898058891296387\n",
      "Epoch=229, iteration=3, train_loss=2.919200897216797\n",
      "Epoch=229, val_loss=2.9113879203796387\n",
      "Epoch=230, iteration=0, train_loss=2.755990982055664\n",
      "Epoch=230, iteration=1, train_loss=2.751753568649292\n",
      "Epoch=230, iteration=2, train_loss=2.7897958755493164\n",
      "Epoch=230, iteration=3, train_loss=2.919187307357788\n",
      "Epoch=230, val_loss=2.9113800525665283\n",
      "Epoch=231, iteration=0, train_loss=2.755969762802124\n",
      "Epoch=231, iteration=1, train_loss=2.7517359256744385\n",
      "Epoch=231, iteration=2, train_loss=2.789785861968994\n",
      "Epoch=231, iteration=3, train_loss=2.9191734790802\n",
      "Epoch=231, val_loss=2.9113714694976807\n",
      "Epoch=232, iteration=0, train_loss=2.755949020385742\n",
      "Epoch=232, iteration=1, train_loss=2.7517189979553223\n",
      "Epoch=232, iteration=2, train_loss=2.7897753715515137\n",
      "Epoch=232, iteration=3, train_loss=2.9191598892211914\n",
      "Epoch=232, val_loss=2.911363363265991\n",
      "Epoch=233, iteration=0, train_loss=2.755927801132202\n",
      "Epoch=233, iteration=1, train_loss=2.751701831817627\n",
      "Epoch=233, iteration=2, train_loss=2.7897655963897705\n",
      "Epoch=233, iteration=3, train_loss=2.9191465377807617\n",
      "Epoch=233, val_loss=2.911355495452881\n",
      "Epoch=234, iteration=0, train_loss=2.7559077739715576\n",
      "Epoch=234, iteration=1, train_loss=2.7516849040985107\n",
      "Epoch=234, iteration=2, train_loss=2.7897555828094482\n",
      "Epoch=234, iteration=3, train_loss=2.919133186340332\n",
      "Epoch=234, val_loss=2.9113471508026123\n",
      "Epoch=235, iteration=0, train_loss=2.7558867931365967\n",
      "Epoch=235, iteration=1, train_loss=2.7516682147979736\n",
      "Epoch=235, iteration=2, train_loss=2.789745807647705\n",
      "Epoch=235, iteration=3, train_loss=2.9191198348999023\n",
      "Epoch=235, val_loss=2.91133975982666\n",
      "Epoch=236, iteration=0, train_loss=2.755866765975952\n",
      "Epoch=236, iteration=1, train_loss=2.7516515254974365\n",
      "Epoch=236, iteration=2, train_loss=2.789736270904541\n",
      "Epoch=236, iteration=3, train_loss=2.9191067218780518\n",
      "Epoch=236, val_loss=2.91133189201355\n",
      "Epoch=237, iteration=0, train_loss=2.7558469772338867\n",
      "Epoch=237, iteration=1, train_loss=2.7516350746154785\n",
      "Epoch=237, iteration=2, train_loss=2.789726734161377\n",
      "Epoch=237, iteration=3, train_loss=2.9190940856933594\n",
      "Epoch=237, val_loss=2.9113240242004395\n",
      "Epoch=238, iteration=0, train_loss=2.755826950073242\n",
      "Epoch=238, iteration=1, train_loss=2.7516191005706787\n",
      "Epoch=238, iteration=2, train_loss=2.789717197418213\n",
      "Epoch=238, iteration=3, train_loss=2.919081449508667\n",
      "Epoch=238, val_loss=2.911316394805908\n",
      "Epoch=239, iteration=0, train_loss=2.755807399749756\n",
      "Epoch=239, iteration=1, train_loss=2.7516028881073\n",
      "Epoch=239, iteration=2, train_loss=2.789707899093628\n",
      "Epoch=239, iteration=3, train_loss=2.9190688133239746\n",
      "Epoch=239, val_loss=2.911309003829956\n",
      "Epoch=240, iteration=0, train_loss=2.7557878494262695\n",
      "Epoch=240, iteration=1, train_loss=2.751587152481079\n",
      "Epoch=240, iteration=2, train_loss=2.789698839187622\n",
      "Epoch=240, iteration=3, train_loss=2.9190564155578613\n",
      "Epoch=240, val_loss=2.911301612854004\n",
      "Epoch=241, iteration=0, train_loss=2.7557690143585205\n",
      "Epoch=241, iteration=1, train_loss=2.7515716552734375\n",
      "Epoch=241, iteration=2, train_loss=2.789689779281616\n",
      "Epoch=241, iteration=3, train_loss=2.919043779373169\n",
      "Epoch=241, val_loss=2.9112942218780518\n",
      "Epoch=242, iteration=0, train_loss=2.7557499408721924\n",
      "Epoch=242, iteration=1, train_loss=2.751555919647217\n",
      "Epoch=242, iteration=2, train_loss=2.7896804809570312\n",
      "Epoch=242, iteration=3, train_loss=2.9190313816070557\n",
      "Epoch=242, val_loss=2.9112870693206787\n",
      "Epoch=243, iteration=0, train_loss=2.7557311058044434\n",
      "Epoch=243, iteration=1, train_loss=2.7515408992767334\n",
      "Epoch=243, iteration=2, train_loss=2.7896721363067627\n",
      "Epoch=243, iteration=3, train_loss=2.9190189838409424\n",
      "Epoch=243, val_loss=2.9112799167633057\n",
      "Epoch=244, iteration=0, train_loss=2.7557122707366943\n",
      "Epoch=244, iteration=1, train_loss=2.751525640487671\n",
      "Epoch=244, iteration=2, train_loss=2.789663076400757\n",
      "Epoch=244, iteration=3, train_loss=2.9190070629119873\n",
      "Epoch=244, val_loss=2.9112725257873535\n",
      "Epoch=245, iteration=0, train_loss=2.7556939125061035\n",
      "Epoch=245, iteration=1, train_loss=2.7515108585357666\n",
      "Epoch=245, iteration=2, train_loss=2.78965425491333\n",
      "Epoch=245, iteration=3, train_loss=2.9189953804016113\n",
      "Epoch=245, val_loss=2.9112660884857178\n",
      "Epoch=246, iteration=0, train_loss=2.7556755542755127\n",
      "Epoch=246, iteration=1, train_loss=2.751495838165283\n",
      "Epoch=246, iteration=2, train_loss=2.7896459102630615\n",
      "Epoch=246, iteration=3, train_loss=2.9189841747283936\n",
      "Epoch=246, val_loss=2.911259174346924\n",
      "Epoch=247, iteration=0, train_loss=2.755657196044922\n",
      "Epoch=247, iteration=1, train_loss=2.751481294631958\n",
      "Epoch=247, iteration=2, train_loss=2.789637804031372\n",
      "Epoch=247, iteration=3, train_loss=2.9189724922180176\n",
      "Epoch=247, val_loss=2.911252498626709\n",
      "Epoch=248, iteration=0, train_loss=2.75563907623291\n",
      "Epoch=248, iteration=1, train_loss=2.751466751098633\n",
      "Epoch=248, iteration=2, train_loss=2.7896289825439453\n",
      "Epoch=248, iteration=3, train_loss=2.9189608097076416\n",
      "Epoch=248, val_loss=2.911245822906494\n",
      "Epoch=249, iteration=0, train_loss=2.7556216716766357\n",
      "Epoch=249, iteration=1, train_loss=2.751452684402466\n",
      "Epoch=249, iteration=2, train_loss=2.789620876312256\n",
      "Epoch=249, iteration=3, train_loss=2.9189491271972656\n",
      "Epoch=249, val_loss=2.911238670349121\n",
      "Epoch=250, iteration=0, train_loss=2.7556040287017822\n",
      "Epoch=250, iteration=1, train_loss=2.7514381408691406\n",
      "Epoch=250, iteration=2, train_loss=2.7896125316619873\n",
      "Epoch=250, iteration=3, train_loss=2.918937921524048\n",
      "Epoch=250, val_loss=2.9112324714660645\n",
      "Epoch=251, iteration=0, train_loss=2.755586624145508\n",
      "Epoch=251, iteration=1, train_loss=2.7514240741729736\n",
      "Epoch=251, iteration=2, train_loss=2.789604425430298\n",
      "Epoch=251, iteration=3, train_loss=2.918926239013672\n",
      "Epoch=251, val_loss=2.9112260341644287\n",
      "Epoch=252, iteration=0, train_loss=2.7555689811706543\n",
      "Epoch=252, iteration=1, train_loss=2.7514100074768066\n",
      "Epoch=252, iteration=2, train_loss=2.7895965576171875\n",
      "Epoch=252, iteration=3, train_loss=2.9189157485961914\n",
      "Epoch=252, val_loss=2.911219835281372\n",
      "Epoch=253, iteration=0, train_loss=2.755552291870117\n",
      "Epoch=253, iteration=1, train_loss=2.751396417617798\n",
      "Epoch=253, iteration=2, train_loss=2.789588689804077\n",
      "Epoch=253, iteration=3, train_loss=2.9189043045043945\n",
      "Epoch=253, val_loss=2.9112136363983154\n",
      "Epoch=254, iteration=0, train_loss=2.7555348873138428\n",
      "Epoch=254, iteration=1, train_loss=2.751382827758789\n",
      "Epoch=254, iteration=2, train_loss=2.789581060409546\n",
      "Epoch=254, iteration=3, train_loss=2.918893814086914\n",
      "Epoch=254, val_loss=2.911207914352417\n",
      "Epoch=255, iteration=0, train_loss=2.755518674850464\n",
      "Epoch=255, iteration=1, train_loss=2.7513694763183594\n",
      "Epoch=255, iteration=2, train_loss=2.7895731925964355\n",
      "Epoch=255, iteration=3, train_loss=2.9188828468322754\n",
      "Epoch=255, val_loss=2.9112017154693604\n",
      "Epoch=256, iteration=0, train_loss=2.7555015087127686\n",
      "Epoch=256, iteration=1, train_loss=2.7513558864593506\n",
      "Epoch=256, iteration=2, train_loss=2.7895655632019043\n",
      "Epoch=256, iteration=3, train_loss=2.918872594833374\n",
      "Epoch=256, val_loss=2.9111952781677246\n",
      "Epoch=257, iteration=0, train_loss=2.7554850578308105\n",
      "Epoch=257, iteration=1, train_loss=2.7513427734375\n",
      "Epoch=257, iteration=2, train_loss=2.789557933807373\n",
      "Epoch=257, iteration=3, train_loss=2.9188616275787354\n",
      "Epoch=257, val_loss=2.9111897945404053\n",
      "Epoch=258, iteration=0, train_loss=2.7554686069488525\n",
      "Epoch=258, iteration=1, train_loss=2.7513294219970703\n",
      "Epoch=258, iteration=2, train_loss=2.789550542831421\n",
      "Epoch=258, iteration=3, train_loss=2.918850898742676\n",
      "Epoch=258, val_loss=2.9111838340759277\n",
      "Epoch=259, iteration=0, train_loss=2.7554526329040527\n",
      "Epoch=259, iteration=1, train_loss=2.751316547393799\n",
      "Epoch=259, iteration=2, train_loss=2.7895429134368896\n",
      "Epoch=259, iteration=3, train_loss=2.9188406467437744\n",
      "Epoch=259, val_loss=2.9111781120300293\n",
      "Epoch=260, iteration=0, train_loss=2.755436420440674\n",
      "Epoch=260, iteration=1, train_loss=2.7513036727905273\n",
      "Epoch=260, iteration=2, train_loss=2.7895359992980957\n",
      "Epoch=260, iteration=3, train_loss=2.918830633163452\n",
      "Epoch=260, val_loss=2.91117262840271\n",
      "Epoch=261, iteration=0, train_loss=2.755420446395874\n",
      "Epoch=261, iteration=1, train_loss=2.751291036605835\n",
      "Epoch=261, iteration=2, train_loss=2.7895286083221436\n",
      "Epoch=261, iteration=3, train_loss=2.9188201427459717\n",
      "Epoch=261, val_loss=2.9111669063568115\n",
      "Epoch=262, iteration=0, train_loss=2.755404472351074\n",
      "Epoch=262, iteration=1, train_loss=2.7512784004211426\n",
      "Epoch=262, iteration=2, train_loss=2.7895214557647705\n",
      "Epoch=262, iteration=3, train_loss=2.9188101291656494\n",
      "Epoch=262, val_loss=2.911161422729492\n",
      "Epoch=263, iteration=0, train_loss=2.7553892135620117\n",
      "Epoch=263, iteration=1, train_loss=2.7512660026550293\n",
      "Epoch=263, iteration=2, train_loss=2.7895143032073975\n",
      "Epoch=263, iteration=3, train_loss=2.918799877166748\n",
      "Epoch=263, val_loss=2.9111554622650146\n",
      "Epoch=264, iteration=0, train_loss=2.755373477935791\n",
      "Epoch=264, iteration=1, train_loss=2.751253604888916\n",
      "Epoch=264, iteration=2, train_loss=2.7895071506500244\n",
      "Epoch=264, iteration=3, train_loss=2.918790102005005\n",
      "Epoch=264, val_loss=2.9111499786376953\n",
      "Epoch=265, iteration=0, train_loss=2.7553579807281494\n",
      "Epoch=265, iteration=1, train_loss=2.751241445541382\n",
      "Epoch=265, iteration=2, train_loss=2.7895004749298096\n",
      "Epoch=265, iteration=3, train_loss=2.91878080368042\n",
      "Epoch=265, val_loss=2.911144971847534\n",
      "Epoch=266, iteration=0, train_loss=2.755342960357666\n",
      "Epoch=266, iteration=1, train_loss=2.7512292861938477\n",
      "Epoch=266, iteration=2, train_loss=2.7894935607910156\n",
      "Epoch=266, iteration=3, train_loss=2.9187707901000977\n",
      "Epoch=266, val_loss=2.9111390113830566\n",
      "Epoch=267, iteration=0, train_loss=2.7553277015686035\n",
      "Epoch=267, iteration=1, train_loss=2.7512173652648926\n",
      "Epoch=267, iteration=2, train_loss=2.789486885070801\n",
      "Epoch=267, iteration=3, train_loss=2.9187607765197754\n",
      "Epoch=267, val_loss=2.9111335277557373\n",
      "Epoch=268, iteration=0, train_loss=2.755312204360962\n",
      "Epoch=268, iteration=1, train_loss=2.7512049674987793\n",
      "Epoch=268, iteration=2, train_loss=2.789479970932007\n",
      "Epoch=268, iteration=3, train_loss=2.9187512397766113\n",
      "Epoch=268, val_loss=2.9111287593841553\n",
      "Epoch=269, iteration=0, train_loss=2.7552974224090576\n",
      "Epoch=269, iteration=1, train_loss=2.751193046569824\n",
      "Epoch=269, iteration=2, train_loss=2.789473533630371\n",
      "Epoch=269, iteration=3, train_loss=2.9187417030334473\n",
      "Epoch=269, val_loss=2.9111239910125732\n",
      "Epoch=270, iteration=0, train_loss=2.7552828788757324\n",
      "Epoch=270, iteration=1, train_loss=2.7511818408966064\n",
      "Epoch=270, iteration=2, train_loss=2.7894668579101562\n",
      "Epoch=270, iteration=3, train_loss=2.9187328815460205\n",
      "Epoch=270, val_loss=2.911118507385254\n",
      "Epoch=271, iteration=0, train_loss=2.755268096923828\n",
      "Epoch=271, iteration=1, train_loss=2.7511701583862305\n",
      "Epoch=271, iteration=2, train_loss=2.7894604206085205\n",
      "Epoch=271, iteration=3, train_loss=2.9187231063842773\n",
      "Epoch=271, val_loss=2.911113739013672\n",
      "Epoch=272, iteration=0, train_loss=2.755253553390503\n",
      "Epoch=272, iteration=1, train_loss=2.7511587142944336\n",
      "Epoch=272, iteration=2, train_loss=2.7894537448883057\n",
      "Epoch=272, iteration=3, train_loss=2.9187142848968506\n",
      "Epoch=272, val_loss=2.91110897064209\n",
      "Epoch=273, iteration=0, train_loss=2.755239248275757\n",
      "Epoch=273, iteration=1, train_loss=2.7511472702026367\n",
      "Epoch=273, iteration=2, train_loss=2.789447546005249\n",
      "Epoch=273, iteration=3, train_loss=2.918705463409424\n",
      "Epoch=273, val_loss=2.9111037254333496\n",
      "Epoch=274, iteration=0, train_loss=2.7552249431610107\n",
      "Epoch=274, iteration=1, train_loss=2.751136302947998\n",
      "Epoch=274, iteration=2, train_loss=2.7894413471221924\n",
      "Epoch=274, iteration=3, train_loss=2.918696165084839\n",
      "Epoch=274, val_loss=2.9110989570617676\n",
      "Epoch=275, iteration=0, train_loss=2.7552106380462646\n",
      "Epoch=275, iteration=1, train_loss=2.7511250972747803\n",
      "Epoch=275, iteration=2, train_loss=2.7894351482391357\n",
      "Epoch=275, iteration=3, train_loss=2.918686866760254\n",
      "Epoch=275, val_loss=2.9110941886901855\n",
      "Epoch=276, iteration=0, train_loss=2.7551965713500977\n",
      "Epoch=276, iteration=1, train_loss=2.7511136531829834\n",
      "Epoch=276, iteration=2, train_loss=2.7894287109375\n",
      "Epoch=276, iteration=3, train_loss=2.918678045272827\n",
      "Epoch=276, val_loss=2.9110894203186035\n",
      "Epoch=277, iteration=0, train_loss=2.7551825046539307\n",
      "Epoch=277, iteration=1, train_loss=2.751102924346924\n",
      "Epoch=277, iteration=2, train_loss=2.7894225120544434\n",
      "Epoch=277, iteration=3, train_loss=2.9186692237854004\n",
      "Epoch=277, val_loss=2.9110846519470215\n",
      "Epoch=278, iteration=0, train_loss=2.7551686763763428\n",
      "Epoch=278, iteration=1, train_loss=2.751091957092285\n",
      "Epoch=278, iteration=2, train_loss=2.789417028427124\n",
      "Epoch=278, iteration=3, train_loss=2.9186606407165527\n",
      "Epoch=278, val_loss=2.9110798835754395\n",
      "Epoch=279, iteration=0, train_loss=2.755154848098755\n",
      "Epoch=279, iteration=1, train_loss=2.7510812282562256\n",
      "Epoch=279, iteration=2, train_loss=2.7894108295440674\n",
      "Epoch=279, iteration=3, train_loss=2.918652057647705\n",
      "Epoch=279, val_loss=2.9110755920410156\n",
      "Epoch=280, iteration=0, train_loss=2.7551417350769043\n",
      "Epoch=280, iteration=1, train_loss=2.751070737838745\n",
      "Epoch=280, iteration=2, train_loss=2.78940486907959\n",
      "Epoch=280, iteration=3, train_loss=2.9186434745788574\n",
      "Epoch=280, val_loss=2.9110708236694336\n",
      "Epoch=281, iteration=0, train_loss=2.7551281452178955\n",
      "Epoch=281, iteration=1, train_loss=2.7510597705841064\n",
      "Epoch=281, iteration=2, train_loss=2.7893989086151123\n",
      "Epoch=281, iteration=3, train_loss=2.9186344146728516\n",
      "Epoch=281, val_loss=2.9110665321350098\n",
      "Epoch=282, iteration=0, train_loss=2.7551145553588867\n",
      "Epoch=282, iteration=1, train_loss=2.751049518585205\n",
      "Epoch=282, iteration=2, train_loss=2.7893929481506348\n",
      "Epoch=282, iteration=3, train_loss=2.918626308441162\n",
      "Epoch=282, val_loss=2.911062002182007\n",
      "Epoch=283, iteration=0, train_loss=2.755101203918457\n",
      "Epoch=283, iteration=1, train_loss=2.7510390281677246\n",
      "Epoch=283, iteration=2, train_loss=2.7893874645233154\n",
      "Epoch=283, iteration=3, train_loss=2.9186182022094727\n",
      "Epoch=283, val_loss=2.911057949066162\n",
      "Epoch=284, iteration=0, train_loss=2.7550880908966064\n",
      "Epoch=284, iteration=1, train_loss=2.7510287761688232\n",
      "Epoch=284, iteration=2, train_loss=2.789381742477417\n",
      "Epoch=284, iteration=3, train_loss=2.918609619140625\n",
      "Epoch=284, val_loss=2.911053419113159\n",
      "Epoch=285, iteration=0, train_loss=2.7550747394561768\n",
      "Epoch=285, iteration=1, train_loss=2.751018762588501\n",
      "Epoch=285, iteration=2, train_loss=2.7893757820129395\n",
      "Epoch=285, iteration=3, train_loss=2.9186010360717773\n",
      "Epoch=285, val_loss=2.9110493659973145\n",
      "Epoch=286, iteration=0, train_loss=2.755061626434326\n",
      "Epoch=286, iteration=1, train_loss=2.7510085105895996\n",
      "Epoch=286, iteration=2, train_loss=2.78937029838562\n",
      "Epoch=286, iteration=3, train_loss=2.918593168258667\n",
      "Epoch=286, val_loss=2.9110448360443115\n",
      "Epoch=287, iteration=0, train_loss=2.755048990249634\n",
      "Epoch=287, iteration=1, train_loss=2.7509982585906982\n",
      "Epoch=287, iteration=2, train_loss=2.789364814758301\n",
      "Epoch=287, iteration=3, train_loss=2.9185853004455566\n",
      "Epoch=287, val_loss=2.911040782928467\n",
      "Epoch=288, iteration=0, train_loss=2.7550361156463623\n",
      "Epoch=288, iteration=1, train_loss=2.750988483428955\n",
      "Epoch=288, iteration=2, train_loss=2.7893593311309814\n",
      "Epoch=288, iteration=3, train_loss=2.918576955795288\n",
      "Epoch=288, val_loss=2.911036729812622\n",
      "Epoch=289, iteration=0, train_loss=2.75502347946167\n",
      "Epoch=289, iteration=1, train_loss=2.750978708267212\n",
      "Epoch=289, iteration=2, train_loss=2.789353847503662\n",
      "Epoch=289, iteration=3, train_loss=2.9185690879821777\n",
      "Epoch=289, val_loss=2.9110329151153564\n",
      "Epoch=290, iteration=0, train_loss=2.7550108432769775\n",
      "Epoch=290, iteration=1, train_loss=2.7509686946868896\n",
      "Epoch=290, iteration=2, train_loss=2.789348602294922\n",
      "Epoch=290, iteration=3, train_loss=2.9185612201690674\n",
      "Epoch=290, val_loss=2.9110283851623535\n",
      "Epoch=291, iteration=0, train_loss=2.7549984455108643\n",
      "Epoch=291, iteration=1, train_loss=2.7509593963623047\n",
      "Epoch=291, iteration=2, train_loss=2.7893428802490234\n",
      "Epoch=291, iteration=3, train_loss=2.918553113937378\n",
      "Epoch=291, val_loss=2.911024570465088\n",
      "Epoch=292, iteration=0, train_loss=2.754986047744751\n",
      "Epoch=292, iteration=1, train_loss=2.7509496212005615\n",
      "Epoch=292, iteration=2, train_loss=2.7893378734588623\n",
      "Epoch=292, iteration=3, train_loss=2.9185454845428467\n",
      "Epoch=292, val_loss=2.9110209941864014\n",
      "Epoch=293, iteration=0, train_loss=2.7549734115600586\n",
      "Epoch=293, iteration=1, train_loss=2.7509400844573975\n",
      "Epoch=293, iteration=2, train_loss=2.789332628250122\n",
      "Epoch=293, iteration=3, train_loss=2.9185376167297363\n",
      "Epoch=293, val_loss=2.9110171794891357\n",
      "Epoch=294, iteration=0, train_loss=2.7549614906311035\n",
      "Epoch=294, iteration=1, train_loss=2.7509305477142334\n",
      "Epoch=294, iteration=2, train_loss=2.789327621459961\n",
      "Epoch=294, iteration=3, train_loss=2.918529987335205\n",
      "Epoch=294, val_loss=2.911013126373291\n",
      "Epoch=295, iteration=0, train_loss=2.7549493312835693\n",
      "Epoch=295, iteration=1, train_loss=2.7509212493896484\n",
      "Epoch=295, iteration=2, train_loss=2.7893221378326416\n",
      "Epoch=295, iteration=3, train_loss=2.9185221195220947\n",
      "Epoch=295, val_loss=2.9110093116760254\n",
      "Epoch=296, iteration=0, train_loss=2.754937171936035\n",
      "Epoch=296, iteration=1, train_loss=2.7509119510650635\n",
      "Epoch=296, iteration=2, train_loss=2.7893171310424805\n",
      "Epoch=296, iteration=3, train_loss=2.9185147285461426\n",
      "Epoch=296, val_loss=2.911005735397339\n",
      "Epoch=297, iteration=0, train_loss=2.754925489425659\n",
      "Epoch=297, iteration=1, train_loss=2.7509026527404785\n",
      "Epoch=297, iteration=2, train_loss=2.7893121242523193\n",
      "Epoch=297, iteration=3, train_loss=2.9185075759887695\n",
      "Epoch=297, val_loss=2.911001682281494\n",
      "Epoch=298, iteration=0, train_loss=2.754913091659546\n",
      "Epoch=298, iteration=1, train_loss=2.7508935928344727\n",
      "Epoch=298, iteration=2, train_loss=2.789307117462158\n",
      "Epoch=298, iteration=3, train_loss=2.9184999465942383\n",
      "Epoch=298, val_loss=2.9109983444213867\n",
      "Epoch=299, iteration=0, train_loss=2.754901647567749\n",
      "Epoch=299, iteration=1, train_loss=2.750884771347046\n",
      "Epoch=299, iteration=2, train_loss=2.789302110671997\n",
      "Epoch=299, iteration=3, train_loss=2.918492317199707\n",
      "Epoch=299, val_loss=2.9109950065612793\n",
      "Epoch=300, iteration=0, train_loss=2.754889726638794\n",
      "Epoch=300, iteration=1, train_loss=2.750875473022461\n",
      "Epoch=300, iteration=2, train_loss=2.789297580718994\n",
      "Epoch=300, iteration=3, train_loss=2.918485164642334\n",
      "Epoch=300, val_loss=2.9109907150268555\n",
      "Epoch=301, iteration=0, train_loss=2.754878044128418\n",
      "Epoch=301, iteration=1, train_loss=2.7508668899536133\n",
      "Epoch=301, iteration=2, train_loss=2.789292335510254\n",
      "Epoch=301, iteration=3, train_loss=2.918478012084961\n",
      "Epoch=301, val_loss=2.910987615585327\n",
      "Epoch=302, iteration=0, train_loss=2.754866600036621\n",
      "Epoch=302, iteration=1, train_loss=2.750857353210449\n",
      "Epoch=302, iteration=2, train_loss=2.7892873287200928\n",
      "Epoch=302, iteration=3, train_loss=2.918470859527588\n",
      "Epoch=302, val_loss=2.9109840393066406\n",
      "Epoch=303, iteration=0, train_loss=2.754855155944824\n",
      "Epoch=303, iteration=1, train_loss=2.7508487701416016\n",
      "Epoch=303, iteration=2, train_loss=2.78928279876709\n",
      "Epoch=303, iteration=3, train_loss=2.918463706970215\n",
      "Epoch=303, val_loss=2.910980463027954\n",
      "Epoch=304, iteration=0, train_loss=2.7548437118530273\n",
      "Epoch=304, iteration=1, train_loss=2.750840187072754\n",
      "Epoch=304, iteration=2, train_loss=2.7892777919769287\n",
      "Epoch=304, iteration=3, train_loss=2.918456554412842\n",
      "Epoch=304, val_loss=2.9109766483306885\n",
      "Epoch=305, iteration=0, train_loss=2.7548325061798096\n",
      "Epoch=305, iteration=1, train_loss=2.7508316040039062\n",
      "Epoch=305, iteration=2, train_loss=2.7892730236053467\n",
      "Epoch=305, iteration=3, train_loss=2.9184491634368896\n",
      "Epoch=305, val_loss=2.91097354888916\n",
      "Epoch=306, iteration=0, train_loss=2.7548210620880127\n",
      "Epoch=306, iteration=1, train_loss=2.7508227825164795\n",
      "Epoch=306, iteration=2, train_loss=2.7892684936523438\n",
      "Epoch=306, iteration=3, train_loss=2.918442487716675\n",
      "Epoch=306, val_loss=2.910970687866211\n",
      "Epoch=307, iteration=0, train_loss=2.754810094833374\n",
      "Epoch=307, iteration=1, train_loss=2.750814199447632\n",
      "Epoch=307, iteration=2, train_loss=2.78926420211792\n",
      "Epoch=307, iteration=3, train_loss=2.9184353351593018\n",
      "Epoch=307, val_loss=2.9109675884246826\n",
      "Epoch=308, iteration=0, train_loss=2.7547991275787354\n",
      "Epoch=308, iteration=1, train_loss=2.7508063316345215\n",
      "Epoch=308, iteration=2, train_loss=2.789259433746338\n",
      "Epoch=308, iteration=3, train_loss=2.918428421020508\n",
      "Epoch=308, val_loss=2.910963773727417\n",
      "Epoch=309, iteration=0, train_loss=2.7547881603240967\n",
      "Epoch=309, iteration=1, train_loss=2.7507975101470947\n",
      "Epoch=309, iteration=2, train_loss=2.789254903793335\n",
      "Epoch=309, iteration=3, train_loss=2.918421983718872\n",
      "Epoch=309, val_loss=2.9109609127044678\n",
      "Epoch=310, iteration=0, train_loss=2.754776954650879\n",
      "Epoch=310, iteration=1, train_loss=2.7507896423339844\n",
      "Epoch=310, iteration=2, train_loss=2.789250612258911\n",
      "Epoch=310, iteration=3, train_loss=2.9184153079986572\n",
      "Epoch=310, val_loss=2.9109575748443604\n",
      "Epoch=311, iteration=0, train_loss=2.7547664642333984\n",
      "Epoch=311, iteration=1, train_loss=2.7507810592651367\n",
      "Epoch=311, iteration=2, train_loss=2.789245843887329\n",
      "Epoch=311, iteration=3, train_loss=2.918408155441284\n",
      "Epoch=311, val_loss=2.910954475402832\n",
      "Epoch=312, iteration=0, train_loss=2.7547554969787598\n",
      "Epoch=312, iteration=1, train_loss=2.7507729530334473\n",
      "Epoch=312, iteration=2, train_loss=2.789241313934326\n",
      "Epoch=312, iteration=3, train_loss=2.9184012413024902\n",
      "Epoch=312, val_loss=2.9109511375427246\n",
      "Epoch=313, iteration=0, train_loss=2.7547447681427\n",
      "Epoch=313, iteration=1, train_loss=2.750764846801758\n",
      "Epoch=313, iteration=2, train_loss=2.7892367839813232\n",
      "Epoch=313, iteration=3, train_loss=2.9183948040008545\n",
      "Epoch=313, val_loss=2.9109482765197754\n",
      "Epoch=314, iteration=0, train_loss=2.7547340393066406\n",
      "Epoch=314, iteration=1, train_loss=2.75075626373291\n",
      "Epoch=314, iteration=2, train_loss=2.7892324924468994\n",
      "Epoch=314, iteration=3, train_loss=2.9183878898620605\n",
      "Epoch=314, val_loss=2.9109456539154053\n",
      "Epoch=315, iteration=0, train_loss=2.7547237873077393\n",
      "Epoch=315, iteration=1, train_loss=2.750748634338379\n",
      "Epoch=315, iteration=2, train_loss=2.7892284393310547\n",
      "Epoch=315, iteration=3, train_loss=2.918381929397583\n",
      "Epoch=315, val_loss=2.9109420776367188\n",
      "Epoch=316, iteration=0, train_loss=2.7547130584716797\n",
      "Epoch=316, iteration=1, train_loss=2.7507407665252686\n",
      "Epoch=316, iteration=2, train_loss=2.7892239093780518\n",
      "Epoch=316, iteration=3, train_loss=2.918375253677368\n",
      "Epoch=316, val_loss=2.9109389781951904\n",
      "Epoch=317, iteration=0, train_loss=2.7547028064727783\n",
      "Epoch=317, iteration=1, train_loss=2.750732898712158\n",
      "Epoch=317, iteration=2, train_loss=2.789219617843628\n",
      "Epoch=317, iteration=3, train_loss=2.9183688163757324\n",
      "Epoch=317, val_loss=2.910935878753662\n",
      "Epoch=318, iteration=0, train_loss=2.754692316055298\n",
      "Epoch=318, iteration=1, train_loss=2.7507247924804688\n",
      "Epoch=318, iteration=2, train_loss=2.789215564727783\n",
      "Epoch=318, iteration=3, train_loss=2.918362617492676\n",
      "Epoch=318, val_loss=2.910933256149292\n",
      "Epoch=319, iteration=0, train_loss=2.7546818256378174\n",
      "Epoch=319, iteration=1, train_loss=2.7507171630859375\n",
      "Epoch=319, iteration=2, train_loss=2.7892115116119385\n",
      "Epoch=319, iteration=3, train_loss=2.918355703353882\n",
      "Epoch=319, val_loss=2.9109301567077637\n",
      "Epoch=320, iteration=0, train_loss=2.754671573638916\n",
      "Epoch=320, iteration=1, train_loss=2.750709295272827\n",
      "Epoch=320, iteration=2, train_loss=2.7892074584960938\n",
      "Epoch=320, iteration=3, train_loss=2.9183497428894043\n",
      "Epoch=320, val_loss=2.9109275341033936\n",
      "Epoch=321, iteration=0, train_loss=2.754661798477173\n",
      "Epoch=321, iteration=1, train_loss=2.750701904296875\n",
      "Epoch=321, iteration=2, train_loss=2.789202928543091\n",
      "Epoch=321, iteration=3, train_loss=2.9183435440063477\n",
      "Epoch=321, val_loss=2.9109246730804443\n",
      "Epoch=322, iteration=0, train_loss=2.7546517848968506\n",
      "Epoch=322, iteration=1, train_loss=2.7506940364837646\n",
      "Epoch=322, iteration=2, train_loss=2.789199113845825\n",
      "Epoch=322, iteration=3, train_loss=2.918337345123291\n",
      "Epoch=322, val_loss=2.910922050476074\n",
      "Epoch=323, iteration=0, train_loss=2.754641532897949\n",
      "Epoch=323, iteration=1, train_loss=2.7506864070892334\n",
      "Epoch=323, iteration=2, train_loss=2.7891948223114014\n",
      "Epoch=323, iteration=3, train_loss=2.9183311462402344\n",
      "Epoch=323, val_loss=2.910918951034546\n",
      "Epoch=324, iteration=0, train_loss=2.754631519317627\n",
      "Epoch=324, iteration=1, train_loss=2.7506790161132812\n",
      "Epoch=324, iteration=2, train_loss=2.7891910076141357\n",
      "Epoch=324, iteration=3, train_loss=2.9183249473571777\n",
      "Epoch=324, val_loss=2.910916566848755\n",
      "Epoch=325, iteration=0, train_loss=2.754621744155884\n",
      "Epoch=325, iteration=1, train_loss=2.750671625137329\n",
      "Epoch=325, iteration=2, train_loss=2.78918719291687\n",
      "Epoch=325, iteration=3, train_loss=2.918318510055542\n",
      "Epoch=325, val_loss=2.9109134674072266\n",
      "Epoch=326, iteration=0, train_loss=2.7546117305755615\n",
      "Epoch=326, iteration=1, train_loss=2.750663995742798\n",
      "Epoch=326, iteration=2, train_loss=2.7891829013824463\n",
      "Epoch=326, iteration=3, train_loss=2.9183125495910645\n",
      "Epoch=326, val_loss=2.9109106063842773\n",
      "Epoch=327, iteration=0, train_loss=2.7546024322509766\n",
      "Epoch=327, iteration=1, train_loss=2.7506566047668457\n",
      "Epoch=327, iteration=2, train_loss=2.7891788482666016\n",
      "Epoch=327, iteration=3, train_loss=2.918306827545166\n",
      "Epoch=327, val_loss=2.9109082221984863\n",
      "Epoch=328, iteration=0, train_loss=2.7545924186706543\n",
      "Epoch=328, iteration=1, train_loss=2.7506494522094727\n",
      "Epoch=328, iteration=2, train_loss=2.789175510406494\n",
      "Epoch=328, iteration=3, train_loss=2.9183006286621094\n",
      "Epoch=328, val_loss=2.910905599594116\n",
      "Epoch=329, iteration=0, train_loss=2.7545828819274902\n",
      "Epoch=329, iteration=1, train_loss=2.7506418228149414\n",
      "Epoch=329, iteration=2, train_loss=2.7891714572906494\n",
      "Epoch=329, iteration=3, train_loss=2.9182944297790527\n",
      "Epoch=329, val_loss=2.910902738571167\n",
      "Epoch=330, iteration=0, train_loss=2.754573345184326\n",
      "Epoch=330, iteration=1, train_loss=2.7506349086761475\n",
      "Epoch=330, iteration=2, train_loss=2.789167881011963\n",
      "Epoch=330, iteration=3, train_loss=2.9182887077331543\n",
      "Epoch=330, val_loss=2.910900831222534\n",
      "Epoch=331, iteration=0, train_loss=2.754563570022583\n",
      "Epoch=331, iteration=1, train_loss=2.7506279945373535\n",
      "Epoch=331, iteration=2, train_loss=2.789163827896118\n",
      "Epoch=331, iteration=3, train_loss=2.9182827472686768\n",
      "Epoch=331, val_loss=2.910897970199585\n",
      "Epoch=332, iteration=0, train_loss=2.754554033279419\n",
      "Epoch=332, iteration=1, train_loss=2.7506208419799805\n",
      "Epoch=332, iteration=2, train_loss=2.7891600131988525\n",
      "Epoch=332, iteration=3, train_loss=2.9182770252227783\n",
      "Epoch=332, val_loss=2.910895347595215\n",
      "Epoch=333, iteration=0, train_loss=2.754544734954834\n",
      "Epoch=333, iteration=1, train_loss=2.7506139278411865\n",
      "Epoch=333, iteration=2, train_loss=2.789156436920166\n",
      "Epoch=333, iteration=3, train_loss=2.91827130317688\n",
      "Epoch=333, val_loss=2.910892963409424\n",
      "Epoch=334, iteration=0, train_loss=2.754535675048828\n",
      "Epoch=334, iteration=1, train_loss=2.7506072521209717\n",
      "Epoch=334, iteration=2, train_loss=2.7891526222229004\n",
      "Epoch=334, iteration=3, train_loss=2.9182653427124023\n",
      "Epoch=334, val_loss=2.9108901023864746\n",
      "Epoch=335, iteration=0, train_loss=2.754526138305664\n",
      "Epoch=335, iteration=1, train_loss=2.7505998611450195\n",
      "Epoch=335, iteration=2, train_loss=2.7891488075256348\n",
      "Epoch=335, iteration=3, train_loss=2.918259859085083\n",
      "Epoch=335, val_loss=2.9108879566192627\n",
      "Epoch=336, iteration=0, train_loss=2.754516839981079\n",
      "Epoch=336, iteration=1, train_loss=2.7505931854248047\n",
      "Epoch=336, iteration=2, train_loss=2.7891452312469482\n",
      "Epoch=336, iteration=3, train_loss=2.9182541370391846\n",
      "Epoch=336, val_loss=2.9108855724334717\n",
      "Epoch=337, iteration=0, train_loss=2.754507541656494\n",
      "Epoch=337, iteration=1, train_loss=2.7505860328674316\n",
      "Epoch=337, iteration=2, train_loss=2.7891416549682617\n",
      "Epoch=337, iteration=3, train_loss=2.9182486534118652\n",
      "Epoch=337, val_loss=2.9108831882476807\n",
      "Epoch=338, iteration=0, train_loss=2.7544984817504883\n",
      "Epoch=338, iteration=1, train_loss=2.750579357147217\n",
      "Epoch=338, iteration=2, train_loss=2.7891385555267334\n",
      "Epoch=338, iteration=3, train_loss=2.9182426929473877\n",
      "Epoch=338, val_loss=2.9108808040618896\n",
      "Epoch=339, iteration=0, train_loss=2.7544894218444824\n",
      "Epoch=339, iteration=1, train_loss=2.750572443008423\n",
      "Epoch=339, iteration=2, train_loss=2.7891342639923096\n",
      "Epoch=339, iteration=3, train_loss=2.9182372093200684\n",
      "Epoch=339, val_loss=2.9108784198760986\n",
      "Epoch=340, iteration=0, train_loss=2.7544803619384766\n",
      "Epoch=340, iteration=1, train_loss=2.750565767288208\n",
      "Epoch=340, iteration=2, train_loss=2.789130926132202\n",
      "Epoch=340, iteration=3, train_loss=2.918231725692749\n",
      "Epoch=340, val_loss=2.9108760356903076\n",
      "Epoch=341, iteration=0, train_loss=2.75447154045105\n",
      "Epoch=341, iteration=1, train_loss=2.7505593299865723\n",
      "Epoch=341, iteration=2, train_loss=2.7891275882720947\n",
      "Epoch=341, iteration=3, train_loss=2.9182262420654297\n",
      "Epoch=341, val_loss=2.910874128341675\n",
      "Epoch=342, iteration=0, train_loss=2.754462718963623\n",
      "Epoch=342, iteration=1, train_loss=2.7505524158477783\n",
      "Epoch=342, iteration=2, train_loss=2.7891242504119873\n",
      "Epoch=342, iteration=3, train_loss=2.9182207584381104\n",
      "Epoch=342, val_loss=2.9108715057373047\n",
      "Epoch=343, iteration=0, train_loss=2.7544538974761963\n",
      "Epoch=343, iteration=1, train_loss=2.7505462169647217\n",
      "Epoch=343, iteration=2, train_loss=2.789120674133301\n",
      "Epoch=343, iteration=3, train_loss=2.918215274810791\n",
      "Epoch=343, val_loss=2.9108691215515137\n",
      "Epoch=344, iteration=0, train_loss=2.7544450759887695\n",
      "Epoch=344, iteration=1, train_loss=2.750539541244507\n",
      "Epoch=344, iteration=2, train_loss=2.7891173362731934\n",
      "Epoch=344, iteration=3, train_loss=2.9182097911834717\n",
      "Epoch=344, val_loss=2.910867214202881\n",
      "Epoch=345, iteration=0, train_loss=2.7544362545013428\n",
      "Epoch=345, iteration=1, train_loss=2.750533103942871\n",
      "Epoch=345, iteration=2, train_loss=2.789113759994507\n",
      "Epoch=345, iteration=3, train_loss=2.9182047843933105\n",
      "Epoch=345, val_loss=2.910865068435669\n",
      "Epoch=346, iteration=0, train_loss=2.754427433013916\n",
      "Epoch=346, iteration=1, train_loss=2.7505266666412354\n",
      "Epoch=346, iteration=2, train_loss=2.7891106605529785\n",
      "Epoch=346, iteration=3, train_loss=2.918199300765991\n",
      "Epoch=346, val_loss=2.910862445831299\n",
      "Epoch=347, iteration=0, train_loss=2.7544188499450684\n",
      "Epoch=347, iteration=1, train_loss=2.7505199909210205\n",
      "Epoch=347, iteration=2, train_loss=2.789106607437134\n",
      "Epoch=347, iteration=3, train_loss=2.918193817138672\n",
      "Epoch=347, val_loss=2.910861015319824\n",
      "Epoch=348, iteration=0, train_loss=2.7544105052948\n",
      "Epoch=348, iteration=1, train_loss=2.750514030456543\n",
      "Epoch=348, iteration=2, train_loss=2.7891035079956055\n",
      "Epoch=348, iteration=3, train_loss=2.9181885719299316\n",
      "Epoch=348, val_loss=2.910858631134033\n",
      "Epoch=349, iteration=0, train_loss=2.754401683807373\n",
      "Epoch=349, iteration=1, train_loss=2.7505075931549072\n",
      "Epoch=349, iteration=2, train_loss=2.789100408554077\n",
      "Epoch=349, iteration=3, train_loss=2.9181830883026123\n",
      "Epoch=349, val_loss=2.910856246948242\n",
      "Epoch=350, iteration=0, train_loss=2.7543928623199463\n",
      "Epoch=350, iteration=1, train_loss=2.7505013942718506\n",
      "Epoch=350, iteration=2, train_loss=2.7890968322753906\n",
      "Epoch=350, iteration=3, train_loss=2.918178081512451\n",
      "Epoch=350, val_loss=2.9108545780181885\n",
      "Epoch=351, iteration=0, train_loss=2.7543842792510986\n",
      "Epoch=351, iteration=1, train_loss=2.7504947185516357\n",
      "Epoch=351, iteration=2, train_loss=2.7890937328338623\n",
      "Epoch=351, iteration=3, train_loss=2.918173313140869\n",
      "Epoch=351, val_loss=2.9108524322509766\n",
      "Epoch=352, iteration=0, train_loss=2.7543764114379883\n",
      "Epoch=352, iteration=1, train_loss=2.750488758087158\n",
      "Epoch=352, iteration=2, train_loss=2.789090633392334\n",
      "Epoch=352, iteration=3, train_loss=2.91816782951355\n",
      "Epoch=352, val_loss=2.9108502864837646\n",
      "Epoch=353, iteration=0, train_loss=2.7543678283691406\n",
      "Epoch=353, iteration=1, train_loss=2.7504825592041016\n",
      "Epoch=353, iteration=2, train_loss=2.7890872955322266\n",
      "Epoch=353, iteration=3, train_loss=2.9181630611419678\n",
      "Epoch=353, val_loss=2.910848379135132\n",
      "Epoch=354, iteration=0, train_loss=2.754359483718872\n",
      "Epoch=354, iteration=1, train_loss=2.750476360321045\n",
      "Epoch=354, iteration=2, train_loss=2.789083957672119\n",
      "Epoch=354, iteration=3, train_loss=2.9181578159332275\n",
      "Epoch=354, val_loss=2.91084623336792\n",
      "Epoch=355, iteration=0, train_loss=2.7543513774871826\n",
      "Epoch=355, iteration=1, train_loss=2.7504703998565674\n",
      "Epoch=355, iteration=2, train_loss=2.78908109664917\n",
      "Epoch=355, iteration=3, train_loss=2.9181528091430664\n",
      "Epoch=355, val_loss=2.910844564437866\n",
      "Epoch=356, iteration=0, train_loss=2.754343032836914\n",
      "Epoch=356, iteration=1, train_loss=2.75046443939209\n",
      "Epoch=356, iteration=2, train_loss=2.7890779972076416\n",
      "Epoch=356, iteration=3, train_loss=2.9181480407714844\n",
      "Epoch=356, val_loss=2.910842180252075\n",
      "Epoch=357, iteration=0, train_loss=2.7543351650238037\n",
      "Epoch=357, iteration=1, train_loss=2.7504584789276123\n",
      "Epoch=357, iteration=2, train_loss=2.7890748977661133\n",
      "Epoch=357, iteration=3, train_loss=2.918142795562744\n",
      "Epoch=357, val_loss=2.9108405113220215\n",
      "Epoch=358, iteration=0, train_loss=2.7543270587921143\n",
      "Epoch=358, iteration=1, train_loss=2.7504522800445557\n",
      "Epoch=358, iteration=2, train_loss=2.789071798324585\n",
      "Epoch=358, iteration=3, train_loss=2.918137550354004\n",
      "Epoch=358, val_loss=2.9108383655548096\n",
      "Epoch=359, iteration=0, train_loss=2.7543187141418457\n",
      "Epoch=359, iteration=1, train_loss=2.750446319580078\n",
      "Epoch=359, iteration=2, train_loss=2.7890686988830566\n",
      "Epoch=359, iteration=3, train_loss=2.91813325881958\n",
      "Epoch=359, val_loss=2.910836696624756\n",
      "Epoch=360, iteration=0, train_loss=2.7543106079101562\n",
      "Epoch=360, iteration=1, train_loss=2.7504405975341797\n",
      "Epoch=360, iteration=2, train_loss=2.7890658378601074\n",
      "Epoch=360, iteration=3, train_loss=2.9181277751922607\n",
      "Epoch=360, val_loss=2.910834550857544\n",
      "Epoch=361, iteration=0, train_loss=2.754302501678467\n",
      "Epoch=361, iteration=1, train_loss=2.750434637069702\n",
      "Epoch=361, iteration=2, train_loss=2.789062261581421\n",
      "Epoch=361, iteration=3, train_loss=2.918123245239258\n",
      "Epoch=361, val_loss=2.9108328819274902\n",
      "Epoch=362, iteration=0, train_loss=2.7542948722839355\n",
      "Epoch=362, iteration=1, train_loss=2.7504286766052246\n",
      "Epoch=362, iteration=2, train_loss=2.78905987739563\n",
      "Epoch=362, iteration=3, train_loss=2.9181182384490967\n",
      "Epoch=362, val_loss=2.9108312129974365\n",
      "Epoch=363, iteration=0, train_loss=2.7542872428894043\n",
      "Epoch=363, iteration=1, train_loss=2.7504231929779053\n",
      "Epoch=363, iteration=2, train_loss=2.7890565395355225\n",
      "Epoch=363, iteration=3, train_loss=2.9181137084960938\n",
      "Epoch=363, val_loss=2.910829544067383\n",
      "Epoch=364, iteration=0, train_loss=2.754279136657715\n",
      "Epoch=364, iteration=1, train_loss=2.750417709350586\n",
      "Epoch=364, iteration=2, train_loss=2.789053440093994\n",
      "Epoch=364, iteration=3, train_loss=2.9181084632873535\n",
      "Epoch=364, val_loss=2.910827398300171\n",
      "Epoch=365, iteration=0, train_loss=2.7542712688446045\n",
      "Epoch=365, iteration=1, train_loss=2.7504117488861084\n",
      "Epoch=365, iteration=2, train_loss=2.789050579071045\n",
      "Epoch=365, iteration=3, train_loss=2.9181041717529297\n",
      "Epoch=365, val_loss=2.910825490951538\n",
      "Epoch=366, iteration=0, train_loss=2.754263401031494\n",
      "Epoch=366, iteration=1, train_loss=2.75040602684021\n",
      "Epoch=366, iteration=2, train_loss=2.7890477180480957\n",
      "Epoch=366, iteration=3, train_loss=2.9180994033813477\n",
      "Epoch=366, val_loss=2.9108240604400635\n",
      "Epoch=367, iteration=0, train_loss=2.754255533218384\n",
      "Epoch=367, iteration=1, train_loss=2.7504000663757324\n",
      "Epoch=367, iteration=2, train_loss=2.7890450954437256\n",
      "Epoch=367, iteration=3, train_loss=2.9180943965911865\n",
      "Epoch=367, val_loss=2.9108223915100098\n",
      "Epoch=368, iteration=0, train_loss=2.7542479038238525\n",
      "Epoch=368, iteration=1, train_loss=2.750394821166992\n",
      "Epoch=368, iteration=2, train_loss=2.789041757583618\n",
      "Epoch=368, iteration=3, train_loss=2.9180901050567627\n",
      "Epoch=368, val_loss=2.910820484161377\n",
      "Epoch=369, iteration=0, train_loss=2.7542402744293213\n",
      "Epoch=369, iteration=1, train_loss=2.7503890991210938\n",
      "Epoch=369, iteration=2, train_loss=2.789039134979248\n",
      "Epoch=369, iteration=3, train_loss=2.9180848598480225\n",
      "Epoch=369, val_loss=2.9108188152313232\n",
      "Epoch=370, iteration=0, train_loss=2.754232883453369\n",
      "Epoch=370, iteration=1, train_loss=2.7503836154937744\n",
      "Epoch=370, iteration=2, train_loss=2.789036512374878\n",
      "Epoch=370, iteration=3, train_loss=2.9180805683135986\n",
      "Epoch=370, val_loss=2.9108171463012695\n",
      "Epoch=371, iteration=0, train_loss=2.7542247772216797\n",
      "Epoch=371, iteration=1, train_loss=2.750378131866455\n",
      "Epoch=371, iteration=2, train_loss=2.7890329360961914\n",
      "Epoch=371, iteration=3, train_loss=2.9180760383605957\n",
      "Epoch=371, val_loss=2.910815477371216\n",
      "Epoch=372, iteration=0, train_loss=2.7542173862457275\n",
      "Epoch=372, iteration=1, train_loss=2.7503724098205566\n",
      "Epoch=372, iteration=2, train_loss=2.7890305519104004\n",
      "Epoch=372, iteration=3, train_loss=2.9180715084075928\n",
      "Epoch=372, val_loss=2.910813331604004\n",
      "Epoch=373, iteration=0, train_loss=2.7542102336883545\n",
      "Epoch=373, iteration=1, train_loss=2.7503671646118164\n",
      "Epoch=373, iteration=2, train_loss=2.789027690887451\n",
      "Epoch=373, iteration=3, train_loss=2.9180667400360107\n",
      "Epoch=373, val_loss=2.9108123779296875\n",
      "Epoch=374, iteration=0, train_loss=2.7542026042938232\n",
      "Epoch=374, iteration=1, train_loss=2.750361919403076\n",
      "Epoch=374, iteration=2, train_loss=2.789025068283081\n",
      "Epoch=374, iteration=3, train_loss=2.918062686920166\n",
      "Epoch=374, val_loss=2.910810708999634\n",
      "Epoch=375, iteration=0, train_loss=2.754195213317871\n",
      "Epoch=375, iteration=1, train_loss=2.7503561973571777\n",
      "Epoch=375, iteration=2, train_loss=2.789022207260132\n",
      "Epoch=375, iteration=3, train_loss=2.918057918548584\n",
      "Epoch=375, val_loss=2.91080904006958\n",
      "Epoch=376, iteration=0, train_loss=2.754187822341919\n",
      "Epoch=376, iteration=1, train_loss=2.7503509521484375\n",
      "Epoch=376, iteration=2, train_loss=2.7890195846557617\n",
      "Epoch=376, iteration=3, train_loss=2.918053388595581\n",
      "Epoch=376, val_loss=2.9108071327209473\n",
      "Epoch=377, iteration=0, train_loss=2.754180669784546\n",
      "Epoch=377, iteration=1, train_loss=2.750345468521118\n",
      "Epoch=377, iteration=2, train_loss=2.7890167236328125\n",
      "Epoch=377, iteration=3, train_loss=2.918048858642578\n",
      "Epoch=377, val_loss=2.9108059406280518\n",
      "Epoch=378, iteration=0, train_loss=2.7541732788085938\n",
      "Epoch=378, iteration=1, train_loss=2.750340461730957\n",
      "Epoch=378, iteration=2, train_loss=2.7890141010284424\n",
      "Epoch=378, iteration=3, train_loss=2.9180445671081543\n",
      "Epoch=378, val_loss=2.910804271697998\n",
      "Epoch=379, iteration=0, train_loss=2.7541658878326416\n",
      "Epoch=379, iteration=1, train_loss=2.7503349781036377\n",
      "Epoch=379, iteration=2, train_loss=2.7890114784240723\n",
      "Epoch=379, iteration=3, train_loss=2.9180405139923096\n",
      "Epoch=379, val_loss=2.9108028411865234\n",
      "Epoch=380, iteration=0, train_loss=2.7541587352752686\n",
      "Epoch=380, iteration=1, train_loss=2.7503302097320557\n",
      "Epoch=380, iteration=2, train_loss=2.789008855819702\n",
      "Epoch=380, iteration=3, train_loss=2.9180355072021484\n",
      "Epoch=380, val_loss=2.9108011722564697\n",
      "Epoch=381, iteration=0, train_loss=2.7541515827178955\n",
      "Epoch=381, iteration=1, train_loss=2.7503247261047363\n",
      "Epoch=381, iteration=2, train_loss=2.789006233215332\n",
      "Epoch=381, iteration=3, train_loss=2.9180314540863037\n",
      "Epoch=381, val_loss=2.910799741744995\n",
      "Epoch=382, iteration=0, train_loss=2.7541441917419434\n",
      "Epoch=382, iteration=1, train_loss=2.750319480895996\n",
      "Epoch=382, iteration=2, train_loss=2.789003372192383\n",
      "Epoch=382, iteration=3, train_loss=2.91802716255188\n",
      "Epoch=382, val_loss=2.9107983112335205\n",
      "Epoch=383, iteration=0, train_loss=2.7541375160217285\n",
      "Epoch=383, iteration=1, train_loss=2.750314474105835\n",
      "Epoch=383, iteration=2, train_loss=2.7890007495880127\n",
      "Epoch=383, iteration=3, train_loss=2.918022632598877\n",
      "Epoch=383, val_loss=2.910796880722046\n",
      "Epoch=384, iteration=0, train_loss=2.7541301250457764\n",
      "Epoch=384, iteration=1, train_loss=2.7503092288970947\n",
      "Epoch=384, iteration=2, train_loss=2.7889981269836426\n",
      "Epoch=384, iteration=3, train_loss=2.9180188179016113\n",
      "Epoch=384, val_loss=2.910795211791992\n",
      "Epoch=385, iteration=0, train_loss=2.7541229724884033\n",
      "Epoch=385, iteration=1, train_loss=2.7503039836883545\n",
      "Epoch=385, iteration=2, train_loss=2.7889955043792725\n",
      "Epoch=385, iteration=3, train_loss=2.9180142879486084\n",
      "Epoch=385, val_loss=2.910794258117676\n",
      "Epoch=386, iteration=0, train_loss=2.7541160583496094\n",
      "Epoch=386, iteration=1, train_loss=2.7502992153167725\n",
      "Epoch=386, iteration=2, train_loss=2.7889931201934814\n",
      "Epoch=386, iteration=3, train_loss=2.9180102348327637\n",
      "Epoch=386, val_loss=2.910792589187622\n",
      "Epoch=387, iteration=0, train_loss=2.7541091442108154\n",
      "Epoch=387, iteration=1, train_loss=2.7502942085266113\n",
      "Epoch=387, iteration=2, train_loss=2.7889904975891113\n",
      "Epoch=387, iteration=3, train_loss=2.918006181716919\n",
      "Epoch=387, val_loss=2.9107913970947266\n",
      "Epoch=388, iteration=0, train_loss=2.7541019916534424\n",
      "Epoch=388, iteration=1, train_loss=2.75028920173645\n",
      "Epoch=388, iteration=2, train_loss=2.788987874984741\n",
      "Epoch=388, iteration=3, train_loss=2.918001413345337\n",
      "Epoch=388, val_loss=2.910790205001831\n",
      "Epoch=389, iteration=0, train_loss=2.7540950775146484\n",
      "Epoch=389, iteration=1, train_loss=2.750284194946289\n",
      "Epoch=389, iteration=2, train_loss=2.7889857292175293\n",
      "Epoch=389, iteration=3, train_loss=2.917997360229492\n",
      "Epoch=389, val_loss=2.9107882976531982\n",
      "Epoch=390, iteration=0, train_loss=2.7540884017944336\n",
      "Epoch=390, iteration=1, train_loss=2.750279188156128\n",
      "Epoch=390, iteration=2, train_loss=2.78898286819458\n",
      "Epoch=390, iteration=3, train_loss=2.9179935455322266\n",
      "Epoch=390, val_loss=2.910787343978882\n",
      "Epoch=391, iteration=0, train_loss=2.7540817260742188\n",
      "Epoch=391, iteration=1, train_loss=2.750274181365967\n",
      "Epoch=391, iteration=2, train_loss=2.788980722427368\n",
      "Epoch=391, iteration=3, train_loss=2.917989492416382\n",
      "Epoch=391, val_loss=2.9107859134674072\n",
      "Epoch=392, iteration=0, train_loss=2.7540745735168457\n",
      "Epoch=392, iteration=1, train_loss=2.7502694129943848\n",
      "Epoch=392, iteration=2, train_loss=2.788977861404419\n",
      "Epoch=392, iteration=3, train_loss=2.917984962463379\n",
      "Epoch=392, val_loss=2.9107847213745117\n",
      "Epoch=393, iteration=0, train_loss=2.75406813621521\n",
      "Epoch=393, iteration=1, train_loss=2.7502646446228027\n",
      "Epoch=393, iteration=2, train_loss=2.788975715637207\n",
      "Epoch=393, iteration=3, train_loss=2.9179811477661133\n",
      "Epoch=393, val_loss=2.910783290863037\n",
      "Epoch=394, iteration=0, train_loss=2.754061222076416\n",
      "Epoch=394, iteration=1, train_loss=2.7502598762512207\n",
      "Epoch=394, iteration=2, train_loss=2.788973093032837\n",
      "Epoch=394, iteration=3, train_loss=2.9179770946502686\n",
      "Epoch=394, val_loss=2.9107818603515625\n",
      "Epoch=395, iteration=0, train_loss=2.7540547847747803\n",
      "Epoch=395, iteration=1, train_loss=2.7502548694610596\n",
      "Epoch=395, iteration=2, train_loss=2.788970947265625\n",
      "Epoch=395, iteration=3, train_loss=2.9179725646972656\n",
      "Epoch=395, val_loss=2.910780906677246\n",
      "Epoch=396, iteration=0, train_loss=2.7540478706359863\n",
      "Epoch=396, iteration=1, train_loss=2.7502501010894775\n",
      "Epoch=396, iteration=2, train_loss=2.788968086242676\n",
      "Epoch=396, iteration=3, train_loss=2.91796875\n",
      "Epoch=396, val_loss=2.9107794761657715\n",
      "Epoch=397, iteration=0, train_loss=2.7540414333343506\n",
      "Epoch=397, iteration=1, train_loss=2.7502450942993164\n",
      "Epoch=397, iteration=2, train_loss=2.788965940475464\n",
      "Epoch=397, iteration=3, train_loss=2.9179646968841553\n",
      "Epoch=397, val_loss=2.910778045654297\n",
      "Epoch=398, iteration=0, train_loss=2.7540347576141357\n",
      "Epoch=398, iteration=1, train_loss=2.7502408027648926\n",
      "Epoch=398, iteration=2, train_loss=2.788963794708252\n",
      "Epoch=398, iteration=3, train_loss=2.9179608821868896\n",
      "Epoch=398, val_loss=2.9107770919799805\n",
      "Epoch=399, iteration=0, train_loss=2.754027843475342\n",
      "Epoch=399, iteration=1, train_loss=2.7502360343933105\n",
      "Epoch=399, iteration=2, train_loss=2.7889609336853027\n",
      "Epoch=399, iteration=3, train_loss=2.917957305908203\n",
      "Epoch=399, val_loss=2.9107754230499268\n",
      "Epoch=400, iteration=0, train_loss=2.754021644592285\n",
      "Epoch=400, iteration=1, train_loss=2.7502315044403076\n",
      "Epoch=400, iteration=2, train_loss=2.788958787918091\n",
      "Epoch=400, iteration=3, train_loss=2.9179530143737793\n",
      "Epoch=400, val_loss=2.9107744693756104\n",
      "Epoch=401, iteration=0, train_loss=2.7540149688720703\n",
      "Epoch=401, iteration=1, train_loss=2.7502267360687256\n",
      "Epoch=401, iteration=2, train_loss=2.7889564037323\n",
      "Epoch=401, iteration=3, train_loss=2.9179489612579346\n",
      "Epoch=401, val_loss=2.910773277282715\n",
      "Epoch=402, iteration=0, train_loss=2.7540087699890137\n",
      "Epoch=402, iteration=1, train_loss=2.7502222061157227\n",
      "Epoch=402, iteration=2, train_loss=2.788954019546509\n",
      "Epoch=402, iteration=3, train_loss=2.917945623397827\n",
      "Epoch=402, val_loss=2.910771608352661\n",
      "Epoch=403, iteration=0, train_loss=2.754002332687378\n",
      "Epoch=403, iteration=1, train_loss=2.7502174377441406\n",
      "Epoch=403, iteration=2, train_loss=2.7889516353607178\n",
      "Epoch=403, iteration=3, train_loss=2.9179413318634033\n",
      "Epoch=403, val_loss=2.910770893096924\n",
      "Epoch=404, iteration=0, train_loss=2.753995656967163\n",
      "Epoch=404, iteration=1, train_loss=2.7502129077911377\n",
      "Epoch=404, iteration=2, train_loss=2.7889492511749268\n",
      "Epoch=404, iteration=3, train_loss=2.9179372787475586\n",
      "Epoch=404, val_loss=2.910769462585449\n",
      "Epoch=405, iteration=0, train_loss=2.7539896965026855\n",
      "Epoch=405, iteration=1, train_loss=2.7502083778381348\n",
      "Epoch=405, iteration=2, train_loss=2.788947343826294\n",
      "Epoch=405, iteration=3, train_loss=2.917933464050293\n",
      "Epoch=405, val_loss=2.910768508911133\n",
      "Epoch=406, iteration=0, train_loss=2.7539827823638916\n",
      "Epoch=406, iteration=1, train_loss=2.750203847885132\n",
      "Epoch=406, iteration=2, train_loss=2.788944959640503\n",
      "Epoch=406, iteration=3, train_loss=2.9179298877716064\n",
      "Epoch=406, val_loss=2.9107675552368164\n",
      "Epoch=407, iteration=0, train_loss=2.753976821899414\n",
      "Epoch=407, iteration=1, train_loss=2.750199317932129\n",
      "Epoch=407, iteration=2, train_loss=2.788942575454712\n",
      "Epoch=407, iteration=3, train_loss=2.9179255962371826\n",
      "Epoch=407, val_loss=2.910766124725342\n",
      "Epoch=408, iteration=0, train_loss=2.753970146179199\n",
      "Epoch=408, iteration=1, train_loss=2.750195026397705\n",
      "Epoch=408, iteration=2, train_loss=2.788940191268921\n",
      "Epoch=408, iteration=3, train_loss=2.917922258377075\n",
      "Epoch=408, val_loss=2.9107651710510254\n",
      "Epoch=409, iteration=0, train_loss=2.753964424133301\n",
      "Epoch=409, iteration=1, train_loss=2.750190496444702\n",
      "Epoch=409, iteration=2, train_loss=2.788938283920288\n",
      "Epoch=409, iteration=3, train_loss=2.9179182052612305\n",
      "Epoch=409, val_loss=2.91076397895813\n",
      "Epoch=410, iteration=0, train_loss=2.753957986831665\n",
      "Epoch=410, iteration=1, train_loss=2.750185489654541\n",
      "Epoch=410, iteration=2, train_loss=2.788936138153076\n",
      "Epoch=410, iteration=3, train_loss=2.917914628982544\n",
      "Epoch=410, val_loss=2.9107627868652344\n",
      "Epoch=411, iteration=0, train_loss=2.7539517879486084\n",
      "Epoch=411, iteration=1, train_loss=2.7501819133758545\n",
      "Epoch=411, iteration=2, train_loss=2.788933753967285\n",
      "Epoch=411, iteration=3, train_loss=2.9179110527038574\n",
      "Epoch=411, val_loss=2.910761594772339\n",
      "Epoch=412, iteration=0, train_loss=2.7539453506469727\n",
      "Epoch=412, iteration=1, train_loss=2.7501771450042725\n",
      "Epoch=412, iteration=2, train_loss=2.7889316082000732\n",
      "Epoch=412, iteration=3, train_loss=2.917907238006592\n",
      "Epoch=412, val_loss=2.9107606410980225\n",
      "Epoch=413, iteration=0, train_loss=2.753939151763916\n",
      "Epoch=413, iteration=1, train_loss=2.7501723766326904\n",
      "Epoch=413, iteration=2, train_loss=2.7889297008514404\n",
      "Epoch=413, iteration=3, train_loss=2.917903423309326\n",
      "Epoch=413, val_loss=2.910759687423706\n",
      "Epoch=414, iteration=0, train_loss=2.7539334297180176\n",
      "Epoch=414, iteration=1, train_loss=2.7501683235168457\n",
      "Epoch=414, iteration=2, train_loss=2.7889273166656494\n",
      "Epoch=414, iteration=3, train_loss=2.9178996086120605\n",
      "Epoch=414, val_loss=2.9107584953308105\n",
      "Epoch=415, iteration=0, train_loss=2.75392746925354\n",
      "Epoch=415, iteration=1, train_loss=2.750164270401001\n",
      "Epoch=415, iteration=2, train_loss=2.7889251708984375\n",
      "Epoch=415, iteration=3, train_loss=2.917895793914795\n",
      "Epoch=415, val_loss=2.910757541656494\n",
      "Epoch=416, iteration=0, train_loss=2.753920793533325\n",
      "Epoch=416, iteration=1, train_loss=2.750159978866577\n",
      "Epoch=416, iteration=2, train_loss=2.7889230251312256\n",
      "Epoch=416, iteration=3, train_loss=2.9178924560546875\n",
      "Epoch=416, val_loss=2.9107565879821777\n",
      "Epoch=417, iteration=0, train_loss=2.7539150714874268\n",
      "Epoch=417, iteration=1, train_loss=2.7501559257507324\n",
      "Epoch=417, iteration=2, train_loss=2.7889206409454346\n",
      "Epoch=417, iteration=3, train_loss=2.91788911819458\n",
      "Epoch=417, val_loss=2.9107556343078613\n",
      "Epoch=418, iteration=0, train_loss=2.753909111022949\n",
      "Epoch=418, iteration=1, train_loss=2.7501516342163086\n",
      "Epoch=418, iteration=2, train_loss=2.7889187335968018\n",
      "Epoch=418, iteration=3, train_loss=2.9178853034973145\n",
      "Epoch=418, val_loss=2.910754919052124\n",
      "Epoch=419, iteration=0, train_loss=2.7539031505584717\n",
      "Epoch=419, iteration=1, train_loss=2.7501471042633057\n",
      "Epoch=419, iteration=2, train_loss=2.78891658782959\n",
      "Epoch=419, iteration=3, train_loss=2.917881488800049\n",
      "Epoch=419, val_loss=2.9107534885406494\n",
      "Epoch=420, iteration=0, train_loss=2.753897190093994\n",
      "Epoch=420, iteration=1, train_loss=2.750143051147461\n",
      "Epoch=420, iteration=2, train_loss=2.788914442062378\n",
      "Epoch=420, iteration=3, train_loss=2.9178781509399414\n",
      "Epoch=420, val_loss=2.910752534866333\n",
      "Epoch=421, iteration=0, train_loss=2.7538909912109375\n",
      "Epoch=421, iteration=1, train_loss=2.750138759613037\n",
      "Epoch=421, iteration=2, train_loss=2.788912296295166\n",
      "Epoch=421, iteration=3, train_loss=2.917874574661255\n",
      "Epoch=421, val_loss=2.9107513427734375\n",
      "Epoch=422, iteration=0, train_loss=2.75388503074646\n",
      "Epoch=422, iteration=1, train_loss=2.7501347064971924\n",
      "Epoch=422, iteration=2, train_loss=2.7889106273651123\n",
      "Epoch=422, iteration=3, train_loss=2.9178709983825684\n",
      "Epoch=422, val_loss=2.9107506275177\n",
      "Epoch=423, iteration=0, train_loss=2.7538793087005615\n",
      "Epoch=423, iteration=1, train_loss=2.7501304149627686\n",
      "Epoch=423, iteration=2, train_loss=2.7889082431793213\n",
      "Epoch=423, iteration=3, train_loss=2.9178671836853027\n",
      "Epoch=423, val_loss=2.9107494354248047\n",
      "Epoch=424, iteration=0, train_loss=2.753873348236084\n",
      "Epoch=424, iteration=1, train_loss=2.750126600265503\n",
      "Epoch=424, iteration=2, train_loss=2.7889060974121094\n",
      "Epoch=424, iteration=3, train_loss=2.917863368988037\n",
      "Epoch=424, val_loss=2.9107487201690674\n",
      "Epoch=425, iteration=0, train_loss=2.7538678646087646\n",
      "Epoch=425, iteration=1, train_loss=2.750122308731079\n",
      "Epoch=425, iteration=2, train_loss=2.7889044284820557\n",
      "Epoch=425, iteration=3, train_loss=2.9178600311279297\n",
      "Epoch=425, val_loss=2.910747766494751\n",
      "Epoch=426, iteration=0, train_loss=2.753861665725708\n",
      "Epoch=426, iteration=1, train_loss=2.750117778778076\n",
      "Epoch=426, iteration=2, train_loss=2.788902521133423\n",
      "Epoch=426, iteration=3, train_loss=2.917856454849243\n",
      "Epoch=426, val_loss=2.9107465744018555\n",
      "Epoch=427, iteration=0, train_loss=2.7538564205169678\n",
      "Epoch=427, iteration=1, train_loss=2.7501139640808105\n",
      "Epoch=427, iteration=2, train_loss=2.788900375366211\n",
      "Epoch=427, iteration=3, train_loss=2.9178531169891357\n",
      "Epoch=427, val_loss=2.910745620727539\n",
      "Epoch=428, iteration=0, train_loss=2.7538506984710693\n",
      "Epoch=428, iteration=1, train_loss=2.750110149383545\n",
      "Epoch=428, iteration=2, train_loss=2.788898229598999\n",
      "Epoch=428, iteration=3, train_loss=2.91784930229187\n",
      "Epoch=428, val_loss=2.9107449054718018\n",
      "Epoch=429, iteration=0, train_loss=2.7538444995880127\n",
      "Epoch=429, iteration=1, train_loss=2.7501060962677\n",
      "Epoch=429, iteration=2, train_loss=2.788896322250366\n",
      "Epoch=429, iteration=3, train_loss=2.917846202850342\n",
      "Epoch=429, val_loss=2.9107441902160645\n",
      "Epoch=430, iteration=0, train_loss=2.7538390159606934\n",
      "Epoch=430, iteration=1, train_loss=2.7501020431518555\n",
      "Epoch=430, iteration=2, train_loss=2.7888944149017334\n",
      "Epoch=430, iteration=3, train_loss=2.917842388153076\n",
      "Epoch=430, val_loss=2.910743236541748\n",
      "Epoch=431, iteration=0, train_loss=2.753833293914795\n",
      "Epoch=431, iteration=1, train_loss=2.7500979900360107\n",
      "Epoch=431, iteration=2, train_loss=2.7888922691345215\n",
      "Epoch=431, iteration=3, train_loss=2.917839288711548\n",
      "Epoch=431, val_loss=2.9107422828674316\n",
      "Epoch=432, iteration=0, train_loss=2.7538275718688965\n",
      "Epoch=432, iteration=1, train_loss=2.750093936920166\n",
      "Epoch=432, iteration=2, train_loss=2.7888906002044678\n",
      "Epoch=432, iteration=3, train_loss=2.9178359508514404\n",
      "Epoch=432, val_loss=2.910741090774536\n",
      "Epoch=433, iteration=0, train_loss=2.753822088241577\n",
      "Epoch=433, iteration=1, train_loss=2.7500901222229004\n",
      "Epoch=433, iteration=2, train_loss=2.788888454437256\n",
      "Epoch=433, iteration=3, train_loss=2.917832612991333\n",
      "Epoch=433, val_loss=2.910740375518799\n",
      "Epoch=434, iteration=0, train_loss=2.7538163661956787\n",
      "Epoch=434, iteration=1, train_loss=2.7500860691070557\n",
      "Epoch=434, iteration=2, train_loss=2.788886785507202\n",
      "Epoch=434, iteration=3, train_loss=2.9178290367126465\n",
      "Epoch=434, val_loss=2.9107396602630615\n",
      "Epoch=435, iteration=0, train_loss=2.7538108825683594\n",
      "Epoch=435, iteration=1, train_loss=2.750082015991211\n",
      "Epoch=435, iteration=2, train_loss=2.7888846397399902\n",
      "Epoch=435, iteration=3, train_loss=2.91782546043396\n",
      "Epoch=435, val_loss=2.910738706588745\n",
      "Epoch=436, iteration=0, train_loss=2.75380539894104\n",
      "Epoch=436, iteration=1, train_loss=2.7500784397125244\n",
      "Epoch=436, iteration=2, train_loss=2.7888824939727783\n",
      "Epoch=436, iteration=3, train_loss=2.9178223609924316\n",
      "Epoch=436, val_loss=2.9107377529144287\n",
      "Epoch=437, iteration=0, train_loss=2.7537994384765625\n",
      "Epoch=437, iteration=1, train_loss=2.750074625015259\n",
      "Epoch=437, iteration=2, train_loss=2.7888810634613037\n",
      "Epoch=437, iteration=3, train_loss=2.9178192615509033\n",
      "Epoch=437, val_loss=2.9107370376586914\n",
      "Epoch=438, iteration=0, train_loss=2.753793954849243\n",
      "Epoch=438, iteration=1, train_loss=2.750070571899414\n",
      "Epoch=438, iteration=2, train_loss=2.788879156112671\n",
      "Epoch=438, iteration=3, train_loss=2.917816162109375\n",
      "Epoch=438, val_loss=2.910736083984375\n",
      "Epoch=439, iteration=0, train_loss=2.753788948059082\n",
      "Epoch=439, iteration=1, train_loss=2.7500667572021484\n",
      "Epoch=439, iteration=2, train_loss=2.788877010345459\n",
      "Epoch=439, iteration=3, train_loss=2.9178123474121094\n",
      "Epoch=439, val_loss=2.9107353687286377\n",
      "Epoch=440, iteration=0, train_loss=2.7537832260131836\n",
      "Epoch=440, iteration=1, train_loss=2.750062942504883\n",
      "Epoch=440, iteration=2, train_loss=2.788875102996826\n",
      "Epoch=440, iteration=3, train_loss=2.917809009552002\n",
      "Epoch=440, val_loss=2.9107348918914795\n",
      "Epoch=441, iteration=0, train_loss=2.7537779808044434\n",
      "Epoch=441, iteration=1, train_loss=2.7500593662261963\n",
      "Epoch=441, iteration=2, train_loss=2.7888731956481934\n",
      "Epoch=441, iteration=3, train_loss=2.9178059101104736\n",
      "Epoch=441, val_loss=2.910733938217163\n",
      "Epoch=442, iteration=0, train_loss=2.753772497177124\n",
      "Epoch=442, iteration=1, train_loss=2.7500555515289307\n",
      "Epoch=442, iteration=2, train_loss=2.7888715267181396\n",
      "Epoch=442, iteration=3, train_loss=2.917802095413208\n",
      "Epoch=442, val_loss=2.9107329845428467\n",
      "Epoch=443, iteration=0, train_loss=2.753767251968384\n",
      "Epoch=443, iteration=1, train_loss=2.750051736831665\n",
      "Epoch=443, iteration=2, train_loss=2.788869619369507\n",
      "Epoch=443, iteration=3, train_loss=2.917799234390259\n",
      "Epoch=443, val_loss=2.9107322692871094\n",
      "Epoch=444, iteration=0, train_loss=2.7537617683410645\n",
      "Epoch=444, iteration=1, train_loss=2.7500479221343994\n",
      "Epoch=444, iteration=2, train_loss=2.788867950439453\n",
      "Epoch=444, iteration=3, train_loss=2.9177961349487305\n",
      "Epoch=444, val_loss=2.910731554031372\n",
      "Epoch=445, iteration=0, train_loss=2.753756284713745\n",
      "Epoch=445, iteration=1, train_loss=2.750044345855713\n",
      "Epoch=445, iteration=2, train_loss=2.7888660430908203\n",
      "Epoch=445, iteration=3, train_loss=2.9177935123443604\n",
      "Epoch=445, val_loss=2.9107308387756348\n",
      "Epoch=446, iteration=0, train_loss=2.753751039505005\n",
      "Epoch=446, iteration=1, train_loss=2.750040292739868\n",
      "Epoch=446, iteration=2, train_loss=2.7888641357421875\n",
      "Epoch=446, iteration=3, train_loss=2.9177894592285156\n",
      "Epoch=446, val_loss=2.9107301235198975\n",
      "Epoch=447, iteration=0, train_loss=2.7537457942962646\n",
      "Epoch=447, iteration=1, train_loss=2.7500367164611816\n",
      "Epoch=447, iteration=2, train_loss=2.788862466812134\n",
      "Epoch=447, iteration=3, train_loss=2.9177865982055664\n",
      "Epoch=447, val_loss=2.91072940826416\n",
      "Epoch=448, iteration=0, train_loss=2.7537403106689453\n",
      "Epoch=448, iteration=1, train_loss=2.750033140182495\n",
      "Epoch=448, iteration=2, train_loss=2.788860559463501\n",
      "Epoch=448, iteration=3, train_loss=2.9177839756011963\n",
      "Epoch=448, val_loss=2.9107284545898438\n",
      "Epoch=449, iteration=0, train_loss=2.753735303878784\n",
      "Epoch=449, iteration=1, train_loss=2.7500295639038086\n",
      "Epoch=449, iteration=2, train_loss=2.7888591289520264\n",
      "Epoch=449, iteration=3, train_loss=2.9177801609039307\n",
      "Epoch=449, val_loss=2.9107277393341064\n",
      "Epoch=450, iteration=0, train_loss=2.753730058670044\n",
      "Epoch=450, iteration=1, train_loss=2.750025749206543\n",
      "Epoch=450, iteration=2, train_loss=2.7888569831848145\n",
      "Epoch=450, iteration=3, train_loss=2.9177768230438232\n",
      "Epoch=450, val_loss=2.9107275009155273\n",
      "Epoch=451, iteration=0, train_loss=2.7537248134613037\n",
      "Epoch=451, iteration=1, train_loss=2.7500219345092773\n",
      "Epoch=451, iteration=2, train_loss=2.7888550758361816\n",
      "Epoch=451, iteration=3, train_loss=2.917773962020874\n",
      "Epoch=451, val_loss=2.91072678565979\n",
      "Epoch=452, iteration=0, train_loss=2.7537193298339844\n",
      "Epoch=452, iteration=1, train_loss=2.75001859664917\n",
      "Epoch=452, iteration=2, train_loss=2.788853883743286\n",
      "Epoch=452, iteration=3, train_loss=2.9177708625793457\n",
      "Epoch=452, val_loss=2.9107258319854736\n",
      "Epoch=453, iteration=0, train_loss=2.7537143230438232\n",
      "Epoch=453, iteration=1, train_loss=2.7500147819519043\n",
      "Epoch=453, iteration=2, train_loss=2.788851499557495\n",
      "Epoch=453, iteration=3, train_loss=2.9177675247192383\n",
      "Epoch=453, val_loss=2.9107251167297363\n",
      "Epoch=454, iteration=0, train_loss=2.753709077835083\n",
      "Epoch=454, iteration=1, train_loss=2.7500112056732178\n",
      "Epoch=454, iteration=2, train_loss=2.7888498306274414\n",
      "Epoch=454, iteration=3, train_loss=2.91776442527771\n",
      "Epoch=454, val_loss=2.910724639892578\n",
      "Epoch=455, iteration=0, train_loss=2.753704071044922\n",
      "Epoch=455, iteration=1, train_loss=2.7500076293945312\n",
      "Epoch=455, iteration=2, train_loss=2.788848400115967\n",
      "Epoch=455, iteration=3, train_loss=2.9177615642547607\n",
      "Epoch=455, val_loss=2.910723924636841\n",
      "Epoch=456, iteration=0, train_loss=2.7536990642547607\n",
      "Epoch=456, iteration=1, train_loss=2.750004529953003\n",
      "Epoch=456, iteration=2, train_loss=2.788846492767334\n",
      "Epoch=456, iteration=3, train_loss=2.9177584648132324\n",
      "Epoch=456, val_loss=2.9107229709625244\n",
      "Epoch=457, iteration=0, train_loss=2.7536938190460205\n",
      "Epoch=457, iteration=1, train_loss=2.7500009536743164\n",
      "Epoch=457, iteration=2, train_loss=2.788844585418701\n",
      "Epoch=457, iteration=3, train_loss=2.917755603790283\n",
      "Epoch=457, val_loss=2.910722255706787\n",
      "Epoch=458, iteration=0, train_loss=2.7536890506744385\n",
      "Epoch=458, iteration=1, train_loss=2.74999737739563\n",
      "Epoch=458, iteration=2, train_loss=2.7888431549072266\n",
      "Epoch=458, iteration=3, train_loss=2.917752742767334\n",
      "Epoch=458, val_loss=2.91072154045105\n",
      "Epoch=459, iteration=0, train_loss=2.7536838054656982\n",
      "Epoch=459, iteration=1, train_loss=2.7499935626983643\n",
      "Epoch=459, iteration=2, train_loss=2.7888410091400146\n",
      "Epoch=459, iteration=3, train_loss=2.9177494049072266\n",
      "Epoch=459, val_loss=2.9107213020324707\n",
      "Epoch=460, iteration=0, train_loss=2.753678798675537\n",
      "Epoch=460, iteration=1, train_loss=2.749990701675415\n",
      "Epoch=460, iteration=2, train_loss=2.788839817047119\n",
      "Epoch=460, iteration=3, train_loss=2.9177463054656982\n",
      "Epoch=460, val_loss=2.9107205867767334\n",
      "Epoch=461, iteration=0, train_loss=2.753674030303955\n",
      "Epoch=461, iteration=1, train_loss=2.7499868869781494\n",
      "Epoch=461, iteration=2, train_loss=2.7888381481170654\n",
      "Epoch=461, iteration=3, train_loss=2.917743444442749\n",
      "Epoch=461, val_loss=2.910719871520996\n",
      "Epoch=462, iteration=0, train_loss=2.753669261932373\n",
      "Epoch=462, iteration=1, train_loss=2.749983310699463\n",
      "Epoch=462, iteration=2, train_loss=2.7888364791870117\n",
      "Epoch=462, iteration=3, train_loss=2.9177403450012207\n",
      "Epoch=462, val_loss=2.910719156265259\n",
      "Epoch=463, iteration=0, train_loss=2.753664016723633\n",
      "Epoch=463, iteration=1, train_loss=2.7499797344207764\n",
      "Epoch=463, iteration=2, train_loss=2.788835048675537\n",
      "Epoch=463, iteration=3, train_loss=2.9177372455596924\n",
      "Epoch=463, val_loss=2.9107189178466797\n",
      "Epoch=464, iteration=0, train_loss=2.7536590099334717\n",
      "Epoch=464, iteration=1, train_loss=2.74997615814209\n",
      "Epoch=464, iteration=2, train_loss=2.7888331413269043\n",
      "Epoch=464, iteration=3, train_loss=2.9177346229553223\n",
      "Epoch=464, val_loss=2.9107179641723633\n",
      "Epoch=465, iteration=0, train_loss=2.7536540031433105\n",
      "Epoch=465, iteration=1, train_loss=2.7499728202819824\n",
      "Epoch=465, iteration=2, train_loss=2.7888314723968506\n",
      "Epoch=465, iteration=3, train_loss=2.917731285095215\n",
      "Epoch=465, val_loss=2.910717487335205\n",
      "Epoch=466, iteration=0, train_loss=2.7536492347717285\n",
      "Epoch=466, iteration=1, train_loss=2.749969720840454\n",
      "Epoch=466, iteration=2, train_loss=2.788830041885376\n",
      "Epoch=466, iteration=3, train_loss=2.9177284240722656\n",
      "Epoch=466, val_loss=2.9107167720794678\n",
      "Epoch=467, iteration=0, train_loss=2.7536439895629883\n",
      "Epoch=467, iteration=1, train_loss=2.7499663829803467\n",
      "Epoch=467, iteration=2, train_loss=2.7888283729553223\n",
      "Epoch=467, iteration=3, train_loss=2.9177253246307373\n",
      "Epoch=467, val_loss=2.9107160568237305\n",
      "Epoch=468, iteration=0, train_loss=2.7536394596099854\n",
      "Epoch=468, iteration=1, train_loss=2.74996280670166\n",
      "Epoch=468, iteration=2, train_loss=2.7888264656066895\n",
      "Epoch=468, iteration=3, train_loss=2.917722702026367\n",
      "Epoch=468, val_loss=2.910715341567993\n",
      "Epoch=469, iteration=0, train_loss=2.753634452819824\n",
      "Epoch=469, iteration=1, train_loss=2.749959707260132\n",
      "Epoch=469, iteration=2, train_loss=2.788825035095215\n",
      "Epoch=469, iteration=3, train_loss=2.917719602584839\n",
      "Epoch=469, val_loss=2.910714864730835\n",
      "Epoch=470, iteration=0, train_loss=2.753629446029663\n",
      "Epoch=470, iteration=1, train_loss=2.7499561309814453\n",
      "Epoch=470, iteration=2, train_loss=2.7888238430023193\n",
      "Epoch=470, iteration=3, train_loss=2.9177167415618896\n",
      "Epoch=470, val_loss=2.9107141494750977\n",
      "Epoch=471, iteration=0, train_loss=2.75362491607666\n",
      "Epoch=471, iteration=1, train_loss=2.749953031539917\n",
      "Epoch=471, iteration=2, train_loss=2.7888219356536865\n",
      "Epoch=471, iteration=3, train_loss=2.9177141189575195\n",
      "Epoch=471, val_loss=2.9107136726379395\n",
      "Epoch=472, iteration=0, train_loss=2.753620147705078\n",
      "Epoch=472, iteration=1, train_loss=2.7499494552612305\n",
      "Epoch=472, iteration=2, train_loss=2.788820505142212\n",
      "Epoch=472, iteration=3, train_loss=2.917711019515991\n",
      "Epoch=472, val_loss=2.9107134342193604\n",
      "Epoch=473, iteration=0, train_loss=2.753615140914917\n",
      "Epoch=473, iteration=1, train_loss=2.749946355819702\n",
      "Epoch=473, iteration=2, train_loss=2.7888190746307373\n",
      "Epoch=473, iteration=3, train_loss=2.917707920074463\n",
      "Epoch=473, val_loss=2.910712957382202\n",
      "Epoch=474, iteration=0, train_loss=2.753610372543335\n",
      "Epoch=474, iteration=1, train_loss=2.749943256378174\n",
      "Epoch=474, iteration=2, train_loss=2.7888176441192627\n",
      "Epoch=474, iteration=3, train_loss=2.9177050590515137\n",
      "Epoch=474, val_loss=2.910712480545044\n",
      "Epoch=475, iteration=0, train_loss=2.753605842590332\n",
      "Epoch=475, iteration=1, train_loss=2.7499396800994873\n",
      "Epoch=475, iteration=2, train_loss=2.78881573677063\n",
      "Epoch=475, iteration=3, train_loss=2.9177024364471436\n",
      "Epoch=475, val_loss=2.9107117652893066\n",
      "Epoch=476, iteration=0, train_loss=2.753601312637329\n",
      "Epoch=476, iteration=1, train_loss=2.74993634223938\n",
      "Epoch=476, iteration=2, train_loss=2.7888143062591553\n",
      "Epoch=476, iteration=3, train_loss=2.9176993370056152\n",
      "Epoch=476, val_loss=2.9107112884521484\n",
      "Epoch=477, iteration=0, train_loss=2.753596067428589\n",
      "Epoch=477, iteration=1, train_loss=2.7499334812164307\n",
      "Epoch=477, iteration=2, train_loss=2.7888126373291016\n",
      "Epoch=477, iteration=3, train_loss=2.917696714401245\n",
      "Epoch=477, val_loss=2.9107110500335693\n",
      "Epoch=478, iteration=0, train_loss=2.753591775894165\n",
      "Epoch=478, iteration=1, train_loss=2.7499301433563232\n",
      "Epoch=478, iteration=2, train_loss=2.788811206817627\n",
      "Epoch=478, iteration=3, train_loss=2.917693853378296\n",
      "Epoch=478, val_loss=2.910710334777832\n",
      "Epoch=479, iteration=0, train_loss=2.753587245941162\n",
      "Epoch=479, iteration=1, train_loss=2.749926805496216\n",
      "Epoch=479, iteration=2, train_loss=2.7888095378875732\n",
      "Epoch=479, iteration=3, train_loss=2.917691230773926\n",
      "Epoch=479, val_loss=2.9107096195220947\n",
      "Epoch=480, iteration=0, train_loss=2.75358247756958\n",
      "Epoch=480, iteration=1, train_loss=2.7499237060546875\n",
      "Epoch=480, iteration=2, train_loss=2.7888081073760986\n",
      "Epoch=480, iteration=3, train_loss=2.9176881313323975\n",
      "Epoch=480, val_loss=2.9107093811035156\n",
      "Epoch=481, iteration=0, train_loss=2.753577709197998\n",
      "Epoch=481, iteration=1, train_loss=2.74992036819458\n",
      "Epoch=481, iteration=2, train_loss=2.788806438446045\n",
      "Epoch=481, iteration=3, train_loss=2.9176857471466064\n",
      "Epoch=481, val_loss=2.9107086658477783\n",
      "Epoch=482, iteration=0, train_loss=2.753573417663574\n",
      "Epoch=482, iteration=1, train_loss=2.7499170303344727\n",
      "Epoch=482, iteration=2, train_loss=2.7888050079345703\n",
      "Epoch=482, iteration=3, train_loss=2.917682647705078\n",
      "Epoch=482, val_loss=2.910707950592041\n",
      "Epoch=483, iteration=0, train_loss=2.7535688877105713\n",
      "Epoch=483, iteration=1, train_loss=2.7499139308929443\n",
      "Epoch=483, iteration=2, train_loss=2.7888035774230957\n",
      "Epoch=483, iteration=3, train_loss=2.917679786682129\n",
      "Epoch=483, val_loss=2.910707473754883\n",
      "Epoch=484, iteration=0, train_loss=2.75356388092041\n",
      "Epoch=484, iteration=1, train_loss=2.749910831451416\n",
      "Epoch=484, iteration=2, train_loss=2.788801908493042\n",
      "Epoch=484, iteration=3, train_loss=2.917677164077759\n",
      "Epoch=484, val_loss=2.9107072353363037\n",
      "Epoch=485, iteration=0, train_loss=2.7535595893859863\n",
      "Epoch=485, iteration=1, train_loss=2.749907970428467\n",
      "Epoch=485, iteration=2, train_loss=2.7888004779815674\n",
      "Epoch=485, iteration=3, train_loss=2.9176743030548096\n",
      "Epoch=485, val_loss=2.9107065200805664\n",
      "Epoch=486, iteration=0, train_loss=2.7535548210144043\n",
      "Epoch=486, iteration=1, train_loss=2.7499048709869385\n",
      "Epoch=486, iteration=2, train_loss=2.788799285888672\n",
      "Epoch=486, iteration=3, train_loss=2.9176716804504395\n",
      "Epoch=486, val_loss=2.9107062816619873\n",
      "Epoch=487, iteration=0, train_loss=2.7535505294799805\n",
      "Epoch=487, iteration=1, train_loss=2.749901294708252\n",
      "Epoch=487, iteration=2, train_loss=2.788797378540039\n",
      "Epoch=487, iteration=3, train_loss=2.9176688194274902\n",
      "Epoch=487, val_loss=2.91070556640625\n",
      "Epoch=488, iteration=0, train_loss=2.7535457611083984\n",
      "Epoch=488, iteration=1, train_loss=2.749898672103882\n",
      "Epoch=488, iteration=2, train_loss=2.7887961864471436\n",
      "Epoch=488, iteration=3, train_loss=2.91766619682312\n",
      "Epoch=488, val_loss=2.91070556640625\n",
      "Epoch=489, iteration=0, train_loss=2.7535414695739746\n",
      "Epoch=489, iteration=1, train_loss=2.7498953342437744\n",
      "Epoch=489, iteration=2, train_loss=2.788794994354248\n",
      "Epoch=489, iteration=3, train_loss=2.917663335800171\n",
      "Epoch=489, val_loss=2.9107048511505127\n",
      "Epoch=490, iteration=0, train_loss=2.7535367012023926\n",
      "Epoch=490, iteration=1, train_loss=2.749892234802246\n",
      "Epoch=490, iteration=2, train_loss=2.7887933254241943\n",
      "Epoch=490, iteration=3, train_loss=2.917660713195801\n",
      "Epoch=490, val_loss=2.9107046127319336\n",
      "Epoch=491, iteration=0, train_loss=2.7535324096679688\n",
      "Epoch=491, iteration=1, train_loss=2.7498891353607178\n",
      "Epoch=491, iteration=2, train_loss=2.7887918949127197\n",
      "Epoch=491, iteration=3, train_loss=2.9176583290100098\n",
      "Epoch=491, val_loss=2.9107038974761963\n",
      "Epoch=492, iteration=0, train_loss=2.753527879714966\n",
      "Epoch=492, iteration=1, train_loss=2.7498860359191895\n",
      "Epoch=492, iteration=2, train_loss=2.788790225982666\n",
      "Epoch=492, iteration=3, train_loss=2.9176559448242188\n",
      "Epoch=492, val_loss=2.910703420639038\n",
      "Epoch=493, iteration=0, train_loss=2.753523349761963\n",
      "Epoch=493, iteration=1, train_loss=2.7498831748962402\n",
      "Epoch=493, iteration=2, train_loss=2.7887892723083496\n",
      "Epoch=493, iteration=3, train_loss=2.9176528453826904\n",
      "Epoch=493, val_loss=2.910703420639038\n",
      "Epoch=494, iteration=0, train_loss=2.753519296646118\n",
      "Epoch=494, iteration=1, train_loss=2.749879837036133\n",
      "Epoch=494, iteration=2, train_loss=2.7887871265411377\n",
      "Epoch=494, iteration=3, train_loss=2.917649984359741\n",
      "Epoch=494, val_loss=2.910702705383301\n",
      "Epoch=495, iteration=0, train_loss=2.7535150051116943\n",
      "Epoch=495, iteration=1, train_loss=2.7498772144317627\n",
      "Epoch=495, iteration=2, train_loss=2.7887861728668213\n",
      "Epoch=495, iteration=3, train_loss=2.91764760017395\n",
      "Epoch=495, val_loss=2.9107019901275635\n",
      "Epoch=496, iteration=0, train_loss=2.7535104751586914\n",
      "Epoch=496, iteration=1, train_loss=2.7498741149902344\n",
      "Epoch=496, iteration=2, train_loss=2.7887847423553467\n",
      "Epoch=496, iteration=3, train_loss=2.91764497756958\n",
      "Epoch=496, val_loss=2.9107017517089844\n",
      "Epoch=497, iteration=0, train_loss=2.7535059452056885\n",
      "Epoch=497, iteration=1, train_loss=2.749871015548706\n",
      "Epoch=497, iteration=2, train_loss=2.788783311843872\n",
      "Epoch=497, iteration=3, train_loss=2.917642593383789\n",
      "Epoch=497, val_loss=2.9107015132904053\n",
      "Epoch=498, iteration=0, train_loss=2.7535018920898438\n",
      "Epoch=498, iteration=1, train_loss=2.749868154525757\n",
      "Epoch=498, iteration=2, train_loss=2.7887818813323975\n",
      "Epoch=498, iteration=3, train_loss=2.9176390171051025\n",
      "Epoch=498, val_loss=2.9107015132904053\n",
      "Epoch=499, iteration=0, train_loss=2.753497362136841\n",
      "Epoch=499, iteration=1, train_loss=2.7498652935028076\n",
      "Epoch=499, iteration=2, train_loss=2.788780450820923\n",
      "Epoch=499, iteration=3, train_loss=2.9176366329193115\n",
      "Epoch=499, val_loss=2.910700798034668\n",
      "Epoch=500, iteration=0, train_loss=2.753493070602417\n",
      "Epoch=500, iteration=1, train_loss=2.7498624324798584\n",
      "Epoch=500, iteration=2, train_loss=2.7887790203094482\n",
      "Epoch=500, iteration=3, train_loss=2.9176342487335205\n",
      "Epoch=500, val_loss=2.9107003211975098\n",
      "Epoch=501, iteration=0, train_loss=2.7534890174865723\n",
      "Epoch=501, iteration=1, train_loss=2.74985933303833\n",
      "Epoch=501, iteration=2, train_loss=2.7887778282165527\n",
      "Epoch=501, iteration=3, train_loss=2.9176316261291504\n",
      "Epoch=501, val_loss=2.9107000827789307\n",
      "Epoch=502, iteration=0, train_loss=2.7534847259521484\n",
      "Epoch=502, iteration=1, train_loss=2.7498562335968018\n",
      "Epoch=502, iteration=2, train_loss=2.788776159286499\n",
      "Epoch=502, iteration=3, train_loss=2.917628765106201\n",
      "Epoch=502, val_loss=2.9106996059417725\n",
      "Epoch=503, iteration=0, train_loss=2.7534804344177246\n",
      "Epoch=503, iteration=1, train_loss=2.7498536109924316\n",
      "Epoch=503, iteration=2, train_loss=2.7887749671936035\n",
      "Epoch=503, iteration=3, train_loss=2.917626142501831\n",
      "Epoch=503, val_loss=2.9106993675231934\n",
      "Epoch=504, iteration=0, train_loss=2.7534759044647217\n",
      "Epoch=504, iteration=1, train_loss=2.7498505115509033\n",
      "Epoch=504, iteration=2, train_loss=2.788773775100708\n",
      "Epoch=504, iteration=3, train_loss=2.91762375831604\n",
      "Epoch=504, val_loss=2.9106991291046143\n",
      "Epoch=505, iteration=0, train_loss=2.753471851348877\n",
      "Epoch=505, iteration=1, train_loss=2.749847888946533\n",
      "Epoch=505, iteration=2, train_loss=2.7887723445892334\n",
      "Epoch=505, iteration=3, train_loss=2.917621374130249\n",
      "Epoch=505, val_loss=2.910698652267456\n",
      "Epoch=506, iteration=0, train_loss=2.753467321395874\n",
      "Epoch=506, iteration=1, train_loss=2.749845027923584\n",
      "Epoch=506, iteration=2, train_loss=2.788771152496338\n",
      "Epoch=506, iteration=3, train_loss=2.917618751525879\n",
      "Epoch=506, val_loss=2.910698413848877\n",
      "Epoch=507, iteration=0, train_loss=2.75346302986145\n",
      "Epoch=507, iteration=1, train_loss=2.749842405319214\n",
      "Epoch=507, iteration=2, train_loss=2.7887697219848633\n",
      "Epoch=507, iteration=3, train_loss=2.9176158905029297\n",
      "Epoch=507, val_loss=2.9106979370117188\n",
      "Epoch=508, iteration=0, train_loss=2.7534592151641846\n",
      "Epoch=508, iteration=1, train_loss=2.7498390674591064\n",
      "Epoch=508, iteration=2, train_loss=2.7887685298919678\n",
      "Epoch=508, iteration=3, train_loss=2.9176135063171387\n",
      "Epoch=508, val_loss=2.9106972217559814\n",
      "Epoch=509, iteration=0, train_loss=2.7534549236297607\n",
      "Epoch=509, iteration=1, train_loss=2.7498362064361572\n",
      "Epoch=509, iteration=2, train_loss=2.788766860961914\n",
      "Epoch=509, iteration=3, train_loss=2.9176113605499268\n",
      "Epoch=509, val_loss=2.9106969833374023\n",
      "Epoch=510, iteration=0, train_loss=2.753450870513916\n",
      "Epoch=510, iteration=1, train_loss=2.749833583831787\n",
      "Epoch=510, iteration=2, train_loss=2.7887654304504395\n",
      "Epoch=510, iteration=3, train_loss=2.9176089763641357\n",
      "Epoch=510, val_loss=2.910696506500244\n",
      "Epoch=511, iteration=0, train_loss=2.753446578979492\n",
      "Epoch=511, iteration=1, train_loss=2.749830484390259\n",
      "Epoch=511, iteration=2, train_loss=2.788764476776123\n",
      "Epoch=511, iteration=3, train_loss=2.9176058769226074\n",
      "Epoch=511, val_loss=2.910696506500244\n",
      "Epoch=512, iteration=0, train_loss=2.7534427642822266\n",
      "Epoch=512, iteration=1, train_loss=2.7498281002044678\n",
      "Epoch=512, iteration=2, train_loss=2.7887632846832275\n",
      "Epoch=512, iteration=3, train_loss=2.9176034927368164\n",
      "Epoch=512, val_loss=2.910696029663086\n",
      "Epoch=513, iteration=0, train_loss=2.7534384727478027\n",
      "Epoch=513, iteration=1, train_loss=2.7498252391815186\n",
      "Epoch=513, iteration=2, train_loss=2.7887613773345947\n",
      "Epoch=513, iteration=3, train_loss=2.9176011085510254\n",
      "Epoch=513, val_loss=2.9106955528259277\n",
      "Epoch=514, iteration=0, train_loss=2.753434419631958\n",
      "Epoch=514, iteration=1, train_loss=2.7498223781585693\n",
      "Epoch=514, iteration=2, train_loss=2.7887604236602783\n",
      "Epoch=514, iteration=3, train_loss=2.9175987243652344\n",
      "Epoch=514, val_loss=2.9106953144073486\n",
      "Epoch=515, iteration=0, train_loss=2.7534306049346924\n",
      "Epoch=515, iteration=1, train_loss=2.749819278717041\n",
      "Epoch=515, iteration=2, train_loss=2.7887587547302246\n",
      "Epoch=515, iteration=3, train_loss=2.9175963401794434\n",
      "Epoch=515, val_loss=2.9106953144073486\n",
      "Epoch=516, iteration=0, train_loss=2.7534263134002686\n",
      "Epoch=516, iteration=1, train_loss=2.749816656112671\n",
      "Epoch=516, iteration=2, train_loss=2.788757801055908\n",
      "Epoch=516, iteration=3, train_loss=2.917593479156494\n",
      "Epoch=516, val_loss=2.9106945991516113\n",
      "Epoch=517, iteration=0, train_loss=2.7534220218658447\n",
      "Epoch=517, iteration=1, train_loss=2.7498137950897217\n",
      "Epoch=517, iteration=2, train_loss=2.7887566089630127\n",
      "Epoch=517, iteration=3, train_loss=2.917591094970703\n",
      "Epoch=517, val_loss=2.9106945991516113\n",
      "Epoch=518, iteration=0, train_loss=2.753418445587158\n",
      "Epoch=518, iteration=1, train_loss=2.7498106956481934\n",
      "Epoch=518, iteration=2, train_loss=2.788755416870117\n",
      "Epoch=518, iteration=3, train_loss=2.917588710784912\n",
      "Epoch=518, val_loss=2.910694122314453\n",
      "Epoch=519, iteration=0, train_loss=2.7534141540527344\n",
      "Epoch=519, iteration=1, train_loss=2.7498083114624023\n",
      "Epoch=519, iteration=2, train_loss=2.7887539863586426\n",
      "Epoch=519, iteration=3, train_loss=2.917586326599121\n",
      "Epoch=519, val_loss=2.910694122314453\n",
      "Epoch=520, iteration=0, train_loss=2.7534098625183105\n",
      "Epoch=520, iteration=1, train_loss=2.7498056888580322\n",
      "Epoch=520, iteration=2, train_loss=2.788752794265747\n",
      "Epoch=520, iteration=3, train_loss=2.9175832271575928\n",
      "Epoch=520, val_loss=2.910693883895874\n",
      "Epoch=521, iteration=0, train_loss=2.753406286239624\n",
      "Epoch=521, iteration=1, train_loss=2.749802827835083\n",
      "Epoch=521, iteration=2, train_loss=2.7887511253356934\n",
      "Epoch=521, iteration=3, train_loss=2.917581081390381\n",
      "Epoch=521, val_loss=2.910693883895874\n",
      "Epoch=522, iteration=0, train_loss=2.7534022331237793\n",
      "Epoch=522, iteration=1, train_loss=2.749800443649292\n",
      "Epoch=522, iteration=2, train_loss=2.788750171661377\n",
      "Epoch=522, iteration=3, train_loss=2.917579174041748\n",
      "Epoch=522, val_loss=2.910693645477295\n",
      "Epoch=523, iteration=0, train_loss=2.7533981800079346\n",
      "Epoch=523, iteration=1, train_loss=2.7497973442077637\n",
      "Epoch=523, iteration=2, train_loss=2.7887487411499023\n",
      "Epoch=523, iteration=3, train_loss=2.917576313018799\n",
      "Epoch=523, val_loss=2.9106929302215576\n",
      "Epoch=524, iteration=0, train_loss=2.75339412689209\n",
      "Epoch=524, iteration=1, train_loss=2.7497947216033936\n",
      "Epoch=524, iteration=2, train_loss=2.788747549057007\n",
      "Epoch=524, iteration=3, train_loss=2.917573928833008\n",
      "Epoch=524, val_loss=2.9106929302215576\n",
      "Epoch=525, iteration=0, train_loss=2.7533905506134033\n",
      "Epoch=525, iteration=1, train_loss=2.7497923374176025\n",
      "Epoch=525, iteration=2, train_loss=2.7887468338012695\n",
      "Epoch=525, iteration=3, train_loss=2.917571544647217\n",
      "Epoch=525, val_loss=2.9106924533843994\n",
      "Epoch=526, iteration=0, train_loss=2.7533862590789795\n",
      "Epoch=526, iteration=1, train_loss=2.7497894763946533\n",
      "Epoch=526, iteration=2, train_loss=2.788745164871216\n",
      "Epoch=526, iteration=3, train_loss=2.9175689220428467\n",
      "Epoch=526, val_loss=2.9106922149658203\n",
      "Epoch=527, iteration=0, train_loss=2.753382444381714\n",
      "Epoch=527, iteration=1, train_loss=2.749786615371704\n",
      "Epoch=527, iteration=2, train_loss=2.7887439727783203\n",
      "Epoch=527, iteration=3, train_loss=2.9175667762756348\n",
      "Epoch=527, val_loss=2.9106922149658203\n",
      "Epoch=528, iteration=0, train_loss=2.7533788681030273\n",
      "Epoch=528, iteration=1, train_loss=2.749783992767334\n",
      "Epoch=528, iteration=2, train_loss=2.788742780685425\n",
      "Epoch=528, iteration=3, train_loss=2.9175643920898438\n",
      "Epoch=528, val_loss=2.910691738128662\n",
      "Epoch=529, iteration=0, train_loss=2.7533748149871826\n",
      "Epoch=529, iteration=1, train_loss=2.749781608581543\n",
      "Epoch=529, iteration=2, train_loss=2.78874135017395\n",
      "Epoch=529, iteration=3, train_loss=2.917562246322632\n",
      "Epoch=529, val_loss=2.910691499710083\n",
      "Epoch=530, iteration=0, train_loss=2.753370761871338\n",
      "Epoch=530, iteration=1, train_loss=2.749779224395752\n",
      "Epoch=530, iteration=2, train_loss=2.7887401580810547\n",
      "Epoch=530, iteration=3, train_loss=2.9175596237182617\n",
      "Epoch=530, val_loss=2.910691261291504\n",
      "Epoch=531, iteration=0, train_loss=2.753366470336914\n",
      "Epoch=531, iteration=1, train_loss=2.7497761249542236\n",
      "Epoch=531, iteration=2, train_loss=2.788738965988159\n",
      "Epoch=531, iteration=3, train_loss=2.91755747795105\n",
      "Epoch=531, val_loss=2.910691261291504\n",
      "Epoch=532, iteration=0, train_loss=2.7533628940582275\n",
      "Epoch=532, iteration=1, train_loss=2.7497739791870117\n",
      "Epoch=532, iteration=2, train_loss=2.788738250732422\n",
      "Epoch=532, iteration=3, train_loss=2.917555093765259\n",
      "Epoch=532, val_loss=2.9106907844543457\n",
      "Epoch=533, iteration=0, train_loss=2.753359317779541\n",
      "Epoch=533, iteration=1, train_loss=2.7497706413269043\n",
      "Epoch=533, iteration=2, train_loss=2.7887368202209473\n",
      "Epoch=533, iteration=3, train_loss=2.9175527095794678\n",
      "Epoch=533, val_loss=2.9106907844543457\n",
      "Epoch=534, iteration=0, train_loss=2.7533552646636963\n",
      "Epoch=534, iteration=1, train_loss=2.7497682571411133\n",
      "Epoch=534, iteration=2, train_loss=2.7887353897094727\n",
      "Epoch=534, iteration=3, train_loss=2.917550802230835\n",
      "Epoch=534, val_loss=2.9106900691986084\n",
      "Epoch=535, iteration=0, train_loss=2.7533514499664307\n",
      "Epoch=535, iteration=1, train_loss=2.7497658729553223\n",
      "Epoch=535, iteration=2, train_loss=2.7887344360351562\n",
      "Epoch=535, iteration=3, train_loss=2.9175477027893066\n",
      "Epoch=535, val_loss=2.9106900691986084\n",
      "Epoch=536, iteration=0, train_loss=2.753347873687744\n",
      "Epoch=536, iteration=1, train_loss=2.7497634887695312\n",
      "Epoch=536, iteration=2, train_loss=2.7887332439422607\n",
      "Epoch=536, iteration=3, train_loss=2.9175455570220947\n",
      "Epoch=536, val_loss=2.9106898307800293\n",
      "Epoch=537, iteration=0, train_loss=2.7533440589904785\n",
      "Epoch=537, iteration=1, train_loss=2.749760389328003\n",
      "Epoch=537, iteration=2, train_loss=2.7887320518493652\n",
      "Epoch=537, iteration=3, train_loss=2.9175431728363037\n",
      "Epoch=537, val_loss=2.9106898307800293\n",
      "Epoch=538, iteration=0, train_loss=2.753340005874634\n",
      "Epoch=538, iteration=1, train_loss=2.749758243560791\n",
      "Epoch=538, iteration=2, train_loss=2.7887308597564697\n",
      "Epoch=538, iteration=3, train_loss=2.917541027069092\n",
      "Epoch=538, val_loss=2.9106898307800293\n",
      "Epoch=539, iteration=0, train_loss=2.7533364295959473\n",
      "Epoch=539, iteration=1, train_loss=2.749755382537842\n",
      "Epoch=539, iteration=2, train_loss=2.788729667663574\n",
      "Epoch=539, iteration=3, train_loss=2.917539119720459\n",
      "Epoch=539, val_loss=2.910689115524292\n",
      "Epoch=540, iteration=0, train_loss=2.7533328533172607\n",
      "Epoch=540, iteration=1, train_loss=2.749752998352051\n",
      "Epoch=540, iteration=2, train_loss=2.788728713989258\n",
      "Epoch=540, iteration=3, train_loss=2.9175362586975098\n",
      "Epoch=540, val_loss=2.910689115524292\n",
      "Epoch=541, iteration=0, train_loss=2.753329277038574\n",
      "Epoch=541, iteration=1, train_loss=2.7497506141662598\n",
      "Epoch=541, iteration=2, train_loss=2.7887275218963623\n",
      "Epoch=541, iteration=3, train_loss=2.917534351348877\n",
      "Epoch=541, val_loss=2.910688638687134\n",
      "Epoch=542, iteration=0, train_loss=2.7533252239227295\n",
      "Epoch=542, iteration=1, train_loss=2.7497479915618896\n",
      "Epoch=542, iteration=2, train_loss=2.788726329803467\n",
      "Epoch=542, iteration=3, train_loss=2.917531967163086\n",
      "Epoch=542, val_loss=2.910688638687134\n",
      "Epoch=543, iteration=0, train_loss=2.753321647644043\n",
      "Epoch=543, iteration=1, train_loss=2.7497451305389404\n",
      "Epoch=543, iteration=2, train_loss=2.7887251377105713\n",
      "Epoch=543, iteration=3, train_loss=2.917529582977295\n",
      "Epoch=543, val_loss=2.910688638687134\n",
      "Epoch=544, iteration=0, train_loss=2.7533175945281982\n",
      "Epoch=544, iteration=1, train_loss=2.7497427463531494\n",
      "Epoch=544, iteration=2, train_loss=2.788723945617676\n",
      "Epoch=544, iteration=3, train_loss=2.917527437210083\n",
      "Epoch=544, val_loss=2.9106884002685547\n",
      "Epoch=545, iteration=0, train_loss=2.7533140182495117\n",
      "Epoch=545, iteration=1, train_loss=2.7497403621673584\n",
      "Epoch=545, iteration=2, train_loss=2.7887229919433594\n",
      "Epoch=545, iteration=3, train_loss=2.917525053024292\n",
      "Epoch=545, val_loss=2.9106881618499756\n",
      "Epoch=546, iteration=0, train_loss=2.753310203552246\n",
      "Epoch=546, iteration=1, train_loss=2.7497379779815674\n",
      "Epoch=546, iteration=2, train_loss=2.788721799850464\n",
      "Epoch=546, iteration=3, train_loss=2.91752290725708\n",
      "Epoch=546, val_loss=2.9106876850128174\n",
      "Epoch=547, iteration=0, train_loss=2.7533066272735596\n",
      "Epoch=547, iteration=1, train_loss=2.749735116958618\n",
      "Epoch=547, iteration=2, train_loss=2.7887206077575684\n",
      "Epoch=547, iteration=3, train_loss=2.917520523071289\n",
      "Epoch=547, val_loss=2.9106876850128174\n",
      "Epoch=548, iteration=0, train_loss=2.753303050994873\n",
      "Epoch=548, iteration=1, train_loss=2.7497329711914062\n",
      "Epoch=548, iteration=2, train_loss=2.7887191772460938\n",
      "Epoch=548, iteration=3, train_loss=2.9175188541412354\n",
      "Epoch=548, val_loss=2.9106874465942383\n",
      "Epoch=549, iteration=0, train_loss=2.7532994747161865\n",
      "Epoch=549, iteration=1, train_loss=2.7497305870056152\n",
      "Epoch=549, iteration=2, train_loss=2.7887184619903564\n",
      "Epoch=549, iteration=3, train_loss=2.917515754699707\n",
      "Epoch=549, val_loss=2.91068696975708\n",
      "Epoch=550, iteration=0, train_loss=2.753295660018921\n",
      "Epoch=550, iteration=1, train_loss=2.749728202819824\n",
      "Epoch=550, iteration=2, train_loss=2.788717269897461\n",
      "Epoch=550, iteration=3, train_loss=2.917513847351074\n",
      "Epoch=550, val_loss=2.9106874465942383\n",
      "Epoch=551, iteration=0, train_loss=2.7532920837402344\n",
      "Epoch=551, iteration=1, train_loss=2.749725341796875\n",
      "Epoch=551, iteration=2, train_loss=2.7887160778045654\n",
      "Epoch=551, iteration=3, train_loss=2.917511224746704\n",
      "Epoch=551, val_loss=2.91068696975708\n",
      "Epoch=552, iteration=0, train_loss=2.7532882690429688\n",
      "Epoch=552, iteration=1, train_loss=2.749723196029663\n",
      "Epoch=552, iteration=2, train_loss=2.788715124130249\n",
      "Epoch=552, iteration=3, train_loss=2.9175093173980713\n",
      "Epoch=552, val_loss=2.910686731338501\n",
      "Epoch=553, iteration=0, train_loss=2.7532849311828613\n",
      "Epoch=553, iteration=1, train_loss=2.749721050262451\n",
      "Epoch=553, iteration=2, train_loss=2.7887141704559326\n",
      "Epoch=553, iteration=3, train_loss=2.9175071716308594\n",
      "Epoch=553, val_loss=2.9106862545013428\n",
      "Epoch=554, iteration=0, train_loss=2.753281593322754\n",
      "Epoch=554, iteration=1, train_loss=2.749718427658081\n",
      "Epoch=554, iteration=2, train_loss=2.788712978363037\n",
      "Epoch=554, iteration=3, train_loss=2.9175052642822266\n",
      "Epoch=554, val_loss=2.910686731338501\n",
      "Epoch=555, iteration=0, train_loss=2.7532777786254883\n",
      "Epoch=555, iteration=1, train_loss=2.749715805053711\n",
      "Epoch=555, iteration=2, train_loss=2.7887117862701416\n",
      "Epoch=555, iteration=3, train_loss=2.9175031185150146\n",
      "Epoch=555, val_loss=2.9106862545013428\n",
      "Epoch=556, iteration=0, train_loss=2.7532742023468018\n",
      "Epoch=556, iteration=1, train_loss=2.74971342086792\n",
      "Epoch=556, iteration=2, train_loss=2.788710594177246\n",
      "Epoch=556, iteration=3, train_loss=2.9175000190734863\n",
      "Epoch=556, val_loss=2.9106862545013428\n",
      "Epoch=557, iteration=0, train_loss=2.7532711029052734\n",
      "Epoch=557, iteration=1, train_loss=2.749711275100708\n",
      "Epoch=557, iteration=2, train_loss=2.7887096405029297\n",
      "Epoch=557, iteration=3, train_loss=2.9174985885620117\n",
      "Epoch=557, val_loss=2.9106862545013428\n",
      "Epoch=558, iteration=0, train_loss=2.753267288208008\n",
      "Epoch=558, iteration=1, train_loss=2.749709129333496\n",
      "Epoch=558, iteration=2, train_loss=2.7887086868286133\n",
      "Epoch=558, iteration=3, train_loss=2.9174962043762207\n",
      "Epoch=558, val_loss=2.9106860160827637\n",
      "Epoch=559, iteration=0, train_loss=2.753263473510742\n",
      "Epoch=559, iteration=1, train_loss=2.749706506729126\n",
      "Epoch=559, iteration=2, train_loss=2.788707733154297\n",
      "Epoch=559, iteration=3, train_loss=2.917494297027588\n",
      "Epoch=559, val_loss=2.9106857776641846\n",
      "Epoch=560, iteration=0, train_loss=2.753260374069214\n",
      "Epoch=560, iteration=1, train_loss=2.749703884124756\n",
      "Epoch=560, iteration=2, train_loss=2.7887065410614014\n",
      "Epoch=560, iteration=3, train_loss=2.917492151260376\n",
      "Epoch=560, val_loss=2.9106853008270264\n",
      "Epoch=561, iteration=0, train_loss=2.7532567977905273\n",
      "Epoch=561, iteration=1, train_loss=2.749701738357544\n",
      "Epoch=561, iteration=2, train_loss=2.788705348968506\n",
      "Epoch=561, iteration=3, train_loss=2.917489528656006\n",
      "Epoch=561, val_loss=2.9106853008270264\n",
      "Epoch=562, iteration=0, train_loss=2.75325345993042\n",
      "Epoch=562, iteration=1, train_loss=2.749699354171753\n",
      "Epoch=562, iteration=2, train_loss=2.7887043952941895\n",
      "Epoch=562, iteration=3, train_loss=2.917487382888794\n",
      "Epoch=562, val_loss=2.9106853008270264\n",
      "Epoch=563, iteration=0, train_loss=2.7532496452331543\n",
      "Epoch=563, iteration=1, train_loss=2.749696969985962\n",
      "Epoch=563, iteration=2, train_loss=2.788703203201294\n",
      "Epoch=563, iteration=3, train_loss=2.917485237121582\n",
      "Epoch=563, val_loss=2.9106850624084473\n",
      "Epoch=564, iteration=0, train_loss=2.753246307373047\n",
      "Epoch=564, iteration=1, train_loss=2.74969482421875\n",
      "Epoch=564, iteration=2, train_loss=2.7887020111083984\n",
      "Epoch=564, iteration=3, train_loss=2.917483329772949\n",
      "Epoch=564, val_loss=2.9106850624084473\n",
      "Epoch=565, iteration=0, train_loss=2.7532429695129395\n",
      "Epoch=565, iteration=1, train_loss=2.749692440032959\n",
      "Epoch=565, iteration=2, train_loss=2.788701295852661\n",
      "Epoch=565, iteration=3, train_loss=2.9174816608428955\n",
      "Epoch=565, val_loss=2.9106850624084473\n",
      "Epoch=566, iteration=0, train_loss=2.753239393234253\n",
      "Epoch=566, iteration=1, train_loss=2.749689817428589\n",
      "Epoch=566, iteration=2, train_loss=2.7886998653411865\n",
      "Epoch=566, iteration=3, train_loss=2.917478561401367\n",
      "Epoch=566, val_loss=2.9106850624084473\n",
      "Epoch=567, iteration=0, train_loss=2.7532362937927246\n",
      "Epoch=567, iteration=1, train_loss=2.749687671661377\n",
      "Epoch=567, iteration=2, train_loss=2.78869891166687\n",
      "Epoch=567, iteration=3, train_loss=2.9174766540527344\n",
      "Epoch=567, val_loss=2.9106850624084473\n",
      "Epoch=568, iteration=0, train_loss=2.753232479095459\n",
      "Epoch=568, iteration=1, train_loss=2.749685049057007\n",
      "Epoch=568, iteration=2, train_loss=2.788698196411133\n",
      "Epoch=568, iteration=3, train_loss=2.9174749851226807\n",
      "Epoch=568, val_loss=2.910684585571289\n",
      "Epoch=569, iteration=0, train_loss=2.7532289028167725\n",
      "Epoch=569, iteration=1, train_loss=2.749682903289795\n",
      "Epoch=569, iteration=2, train_loss=2.7886970043182373\n",
      "Epoch=569, iteration=3, train_loss=2.917473077774048\n",
      "Epoch=569, val_loss=2.91068434715271\n",
      "Epoch=570, iteration=0, train_loss=2.753225803375244\n",
      "Epoch=570, iteration=1, train_loss=2.749680519104004\n",
      "Epoch=570, iteration=2, train_loss=2.788695812225342\n",
      "Epoch=570, iteration=3, train_loss=2.917470693588257\n",
      "Epoch=570, val_loss=2.910684585571289\n",
      "Epoch=571, iteration=0, train_loss=2.7532222270965576\n",
      "Epoch=571, iteration=1, train_loss=2.749678373336792\n",
      "Epoch=571, iteration=2, train_loss=2.7886948585510254\n",
      "Epoch=571, iteration=3, train_loss=2.917468547821045\n",
      "Epoch=571, val_loss=2.91068434715271\n",
      "Epoch=572, iteration=0, train_loss=2.75321888923645\n",
      "Epoch=572, iteration=1, train_loss=2.749675989151001\n",
      "Epoch=572, iteration=2, train_loss=2.788693904876709\n",
      "Epoch=572, iteration=3, train_loss=2.917466163635254\n",
      "Epoch=572, val_loss=2.91068434715271\n",
      "Epoch=573, iteration=0, train_loss=2.7532155513763428\n",
      "Epoch=573, iteration=1, train_loss=2.74967360496521\n",
      "Epoch=573, iteration=2, train_loss=2.7886931896209717\n",
      "Epoch=573, iteration=3, train_loss=2.9174644947052\n",
      "Epoch=573, val_loss=2.9106838703155518\n",
      "Epoch=574, iteration=0, train_loss=2.7532122135162354\n",
      "Epoch=574, iteration=1, train_loss=2.749671697616577\n",
      "Epoch=574, iteration=2, train_loss=2.7886922359466553\n",
      "Epoch=574, iteration=3, train_loss=2.9174623489379883\n",
      "Epoch=574, val_loss=2.9106838703155518\n",
      "Epoch=575, iteration=0, train_loss=2.753209114074707\n",
      "Epoch=575, iteration=1, train_loss=2.749668836593628\n",
      "Epoch=575, iteration=2, train_loss=2.7886910438537598\n",
      "Epoch=575, iteration=3, train_loss=2.9174602031707764\n",
      "Epoch=575, val_loss=2.9106836318969727\n",
      "Epoch=576, iteration=0, train_loss=2.7532057762145996\n",
      "Epoch=576, iteration=1, train_loss=2.749666929244995\n",
      "Epoch=576, iteration=2, train_loss=2.7886898517608643\n",
      "Epoch=576, iteration=3, train_loss=2.9174578189849854\n",
      "Epoch=576, val_loss=2.9106836318969727\n",
      "Epoch=577, iteration=0, train_loss=2.753201961517334\n",
      "Epoch=577, iteration=1, train_loss=2.749664783477783\n",
      "Epoch=577, iteration=2, train_loss=2.788689136505127\n",
      "Epoch=577, iteration=3, train_loss=2.9174559116363525\n",
      "Epoch=577, val_loss=2.9106836318969727\n",
      "Epoch=578, iteration=0, train_loss=2.7531991004943848\n",
      "Epoch=578, iteration=1, train_loss=2.7496626377105713\n",
      "Epoch=578, iteration=2, train_loss=2.7886879444122314\n",
      "Epoch=578, iteration=3, train_loss=2.9174535274505615\n",
      "Epoch=578, val_loss=2.9106836318969727\n",
      "Epoch=579, iteration=0, train_loss=2.7531955242156982\n",
      "Epoch=579, iteration=1, train_loss=2.7496604919433594\n",
      "Epoch=579, iteration=2, train_loss=2.788686990737915\n",
      "Epoch=579, iteration=3, train_loss=2.917451858520508\n",
      "Epoch=579, val_loss=2.9106831550598145\n",
      "Epoch=580, iteration=0, train_loss=2.75319242477417\n",
      "Epoch=580, iteration=1, train_loss=2.7496581077575684\n",
      "Epoch=580, iteration=2, train_loss=2.7886860370635986\n",
      "Epoch=580, iteration=3, train_loss=2.917449712753296\n",
      "Epoch=580, val_loss=2.9106836318969727\n",
      "Epoch=581, iteration=0, train_loss=2.7531886100769043\n",
      "Epoch=581, iteration=1, train_loss=2.7496559619903564\n",
      "Epoch=581, iteration=2, train_loss=2.788684606552124\n",
      "Epoch=581, iteration=3, train_loss=2.917447566986084\n",
      "Epoch=581, val_loss=2.9106831550598145\n",
      "Epoch=582, iteration=0, train_loss=2.753185749053955\n",
      "Epoch=582, iteration=1, train_loss=2.7496533393859863\n",
      "Epoch=582, iteration=2, train_loss=2.7886838912963867\n",
      "Epoch=582, iteration=3, train_loss=2.917445659637451\n",
      "Epoch=582, val_loss=2.9106831550598145\n",
      "Epoch=583, iteration=0, train_loss=2.7531824111938477\n",
      "Epoch=583, iteration=1, train_loss=2.7496516704559326\n",
      "Epoch=583, iteration=2, train_loss=2.7886829376220703\n",
      "Epoch=583, iteration=3, train_loss=2.91744327545166\n",
      "Epoch=583, val_loss=2.9106831550598145\n",
      "Epoch=584, iteration=0, train_loss=2.7531790733337402\n",
      "Epoch=584, iteration=1, train_loss=2.7496492862701416\n",
      "Epoch=584, iteration=2, train_loss=2.788682222366333\n",
      "Epoch=584, iteration=3, train_loss=2.9174413681030273\n",
      "Epoch=584, val_loss=2.9106831550598145\n",
      "Epoch=585, iteration=0, train_loss=2.75317645072937\n",
      "Epoch=585, iteration=1, train_loss=2.7496471405029297\n",
      "Epoch=585, iteration=2, train_loss=2.7886812686920166\n",
      "Epoch=585, iteration=3, train_loss=2.9174396991729736\n",
      "Epoch=585, val_loss=2.9106831550598145\n",
      "Epoch=586, iteration=0, train_loss=2.7531726360321045\n",
      "Epoch=586, iteration=1, train_loss=2.7496449947357178\n",
      "Epoch=586, iteration=2, train_loss=2.788680076599121\n",
      "Epoch=586, iteration=3, train_loss=2.9174373149871826\n",
      "Epoch=586, val_loss=2.9106829166412354\n",
      "Epoch=587, iteration=0, train_loss=2.7531697750091553\n",
      "Epoch=587, iteration=1, train_loss=2.749643087387085\n",
      "Epoch=587, iteration=2, train_loss=2.7886791229248047\n",
      "Epoch=587, iteration=3, train_loss=2.91743540763855\n",
      "Epoch=587, val_loss=2.9106829166412354\n",
      "Epoch=588, iteration=0, train_loss=2.753166437149048\n",
      "Epoch=588, iteration=1, train_loss=2.749640703201294\n",
      "Epoch=588, iteration=2, train_loss=2.7886781692504883\n",
      "Epoch=588, iteration=3, train_loss=2.917433500289917\n",
      "Epoch=588, val_loss=2.9106829166412354\n",
      "Epoch=589, iteration=0, train_loss=2.7531633377075195\n",
      "Epoch=589, iteration=1, train_loss=2.749638319015503\n",
      "Epoch=589, iteration=2, train_loss=2.7886769771575928\n",
      "Epoch=589, iteration=3, train_loss=2.917431592941284\n",
      "Epoch=589, val_loss=2.9106829166412354\n",
      "Epoch=590, iteration=0, train_loss=2.753159999847412\n",
      "Epoch=590, iteration=1, train_loss=2.74963641166687\n",
      "Epoch=590, iteration=2, train_loss=2.7886762619018555\n",
      "Epoch=590, iteration=3, train_loss=2.9174294471740723\n",
      "Epoch=590, val_loss=2.9106826782226562\n",
      "Epoch=591, iteration=0, train_loss=2.7531566619873047\n",
      "Epoch=591, iteration=1, train_loss=2.749634027481079\n",
      "Epoch=591, iteration=2, train_loss=2.788675308227539\n",
      "Epoch=591, iteration=3, train_loss=2.9174275398254395\n",
      "Epoch=591, val_loss=2.9106826782226562\n",
      "Epoch=592, iteration=0, train_loss=2.7531535625457764\n",
      "Epoch=592, iteration=1, train_loss=2.7496321201324463\n",
      "Epoch=592, iteration=2, train_loss=2.7886743545532227\n",
      "Epoch=592, iteration=3, train_loss=2.9174253940582275\n",
      "Epoch=592, val_loss=2.9106826782226562\n",
      "Epoch=593, iteration=0, train_loss=2.753150463104248\n",
      "Epoch=593, iteration=1, train_loss=2.7496297359466553\n",
      "Epoch=593, iteration=2, train_loss=2.7886734008789062\n",
      "Epoch=593, iteration=3, train_loss=2.9174232482910156\n",
      "Epoch=593, val_loss=2.9106826782226562\n",
      "Epoch=594, iteration=0, train_loss=2.7531471252441406\n",
      "Epoch=594, iteration=1, train_loss=2.7496278285980225\n",
      "Epoch=594, iteration=2, train_loss=2.788672685623169\n",
      "Epoch=594, iteration=3, train_loss=2.91742205619812\n",
      "Epoch=594, val_loss=2.9106826782226562\n",
      "Epoch=595, iteration=0, train_loss=2.753143787384033\n",
      "Epoch=595, iteration=1, train_loss=2.7496254444122314\n",
      "Epoch=595, iteration=2, train_loss=2.7886714935302734\n",
      "Epoch=595, iteration=3, train_loss=2.917419672012329\n",
      "Epoch=595, val_loss=2.9106826782226562\n",
      "Epoch=596, iteration=0, train_loss=2.753140687942505\n",
      "Epoch=596, iteration=1, train_loss=2.7496235370635986\n",
      "Epoch=596, iteration=2, train_loss=2.788670539855957\n",
      "Epoch=596, iteration=3, train_loss=2.917417526245117\n",
      "Epoch=596, val_loss=2.9106826782226562\n",
      "Epoch=597, iteration=0, train_loss=2.7531378269195557\n",
      "Epoch=597, iteration=1, train_loss=2.7496213912963867\n",
      "Epoch=597, iteration=2, train_loss=2.7886695861816406\n",
      "Epoch=597, iteration=3, train_loss=2.9174160957336426\n",
      "Epoch=597, val_loss=2.9106826782226562\n",
      "Epoch=598, iteration=0, train_loss=2.7531347274780273\n",
      "Epoch=598, iteration=1, train_loss=2.749619245529175\n",
      "Epoch=598, iteration=2, train_loss=2.7886688709259033\n",
      "Epoch=598, iteration=3, train_loss=2.9174137115478516\n",
      "Epoch=598, val_loss=2.910682201385498\n",
      "Epoch=599, iteration=0, train_loss=2.753131628036499\n",
      "Epoch=599, iteration=1, train_loss=2.749617099761963\n",
      "Epoch=599, iteration=2, train_loss=2.788667917251587\n",
      "Epoch=599, iteration=3, train_loss=2.9174118041992188\n",
      "Epoch=599, val_loss=2.9106826782226562\n",
      "Epoch=600, iteration=0, train_loss=2.7531280517578125\n",
      "Epoch=600, iteration=1, train_loss=2.74961519241333\n",
      "Epoch=600, iteration=2, train_loss=2.7886669635772705\n",
      "Epoch=600, iteration=3, train_loss=2.917409896850586\n",
      "Epoch=600, val_loss=2.9106826782226562\n",
      "Epoch=601, iteration=0, train_loss=2.7531254291534424\n",
      "Epoch=601, iteration=1, train_loss=2.749612808227539\n",
      "Epoch=601, iteration=2, train_loss=2.788666009902954\n",
      "Epoch=601, iteration=3, train_loss=2.917407989501953\n",
      "Epoch=601, val_loss=2.910682201385498\n",
      "Epoch=602, iteration=0, train_loss=2.753122091293335\n",
      "Epoch=602, iteration=1, train_loss=2.7496109008789062\n",
      "Epoch=602, iteration=2, train_loss=2.7886650562286377\n",
      "Epoch=602, iteration=3, train_loss=2.917405605316162\n",
      "Epoch=602, val_loss=2.9106826782226562\n",
      "Epoch=603, iteration=0, train_loss=2.753119468688965\n",
      "Epoch=603, iteration=1, train_loss=2.7496087551116943\n",
      "Epoch=603, iteration=2, train_loss=2.7886643409729004\n",
      "Epoch=603, iteration=3, train_loss=2.9174036979675293\n",
      "Epoch=603, val_loss=2.9106826782226562\n",
      "Epoch=604, iteration=0, train_loss=2.7531158924102783\n",
      "Epoch=604, iteration=1, train_loss=2.7496066093444824\n",
      "Epoch=604, iteration=2, train_loss=2.788663625717163\n",
      "Epoch=604, iteration=3, train_loss=2.9174020290374756\n",
      "Epoch=604, val_loss=2.910682201385498\n",
      "Epoch=605, iteration=0, train_loss=2.75311279296875\n",
      "Epoch=605, iteration=1, train_loss=2.7496049404144287\n",
      "Epoch=605, iteration=2, train_loss=2.7886624336242676\n",
      "Epoch=605, iteration=3, train_loss=2.9174001216888428\n",
      "Epoch=605, val_loss=2.910681962966919\n",
      "Epoch=606, iteration=0, train_loss=2.753109931945801\n",
      "Epoch=606, iteration=1, train_loss=2.7496023178100586\n",
      "Epoch=606, iteration=2, train_loss=2.788661241531372\n",
      "Epoch=606, iteration=3, train_loss=2.917398691177368\n",
      "Epoch=606, val_loss=2.9106826782226562\n",
      "Epoch=607, iteration=0, train_loss=2.7531068325042725\n",
      "Epoch=607, iteration=1, train_loss=2.749600648880005\n",
      "Epoch=607, iteration=2, train_loss=2.7886605262756348\n",
      "Epoch=607, iteration=3, train_loss=2.917395830154419\n",
      "Epoch=607, val_loss=2.910682201385498\n",
      "Epoch=608, iteration=0, train_loss=2.753103733062744\n",
      "Epoch=608, iteration=1, train_loss=2.749598503112793\n",
      "Epoch=608, iteration=2, train_loss=2.7886600494384766\n",
      "Epoch=608, iteration=3, train_loss=2.9173941612243652\n",
      "Epoch=608, val_loss=2.9106826782226562\n",
      "Epoch=609, iteration=0, train_loss=2.753100872039795\n",
      "Epoch=609, iteration=1, train_loss=2.74959659576416\n",
      "Epoch=609, iteration=2, train_loss=2.788658857345581\n",
      "Epoch=609, iteration=3, train_loss=2.9173922538757324\n",
      "Epoch=609, val_loss=2.910682201385498\n",
      "Epoch=610, iteration=0, train_loss=2.7530975341796875\n",
      "Epoch=610, iteration=1, train_loss=2.7495946884155273\n",
      "Epoch=610, iteration=2, train_loss=2.788658380508423\n",
      "Epoch=610, iteration=3, train_loss=2.9173903465270996\n",
      "Epoch=610, val_loss=2.9106826782226562\n",
      "Epoch=611, iteration=0, train_loss=2.7530946731567383\n",
      "Epoch=611, iteration=1, train_loss=2.7495923042297363\n",
      "Epoch=611, iteration=2, train_loss=2.7886574268341064\n",
      "Epoch=611, iteration=3, train_loss=2.917388677597046\n",
      "Epoch=611, val_loss=2.910682201385498\n",
      "Epoch=612, iteration=0, train_loss=2.753091812133789\n",
      "Epoch=612, iteration=1, train_loss=2.7495903968811035\n",
      "Epoch=612, iteration=2, train_loss=2.78865647315979\n",
      "Epoch=612, iteration=3, train_loss=2.917386770248413\n",
      "Epoch=612, val_loss=2.9106826782226562\n",
      "Epoch=613, iteration=0, train_loss=2.7530887126922607\n",
      "Epoch=613, iteration=1, train_loss=2.7495882511138916\n",
      "Epoch=613, iteration=2, train_loss=2.7886557579040527\n",
      "Epoch=613, iteration=3, train_loss=2.917384624481201\n",
      "Epoch=613, val_loss=2.910682201385498\n",
      "Epoch=614, iteration=0, train_loss=2.7530856132507324\n",
      "Epoch=614, iteration=1, train_loss=2.749586582183838\n",
      "Epoch=614, iteration=2, train_loss=2.7886545658111572\n",
      "Epoch=614, iteration=3, train_loss=2.9173829555511475\n",
      "Epoch=614, val_loss=2.9106826782226562\n",
      "Epoch=615, iteration=0, train_loss=2.753082752227783\n",
      "Epoch=615, iteration=1, train_loss=2.749584674835205\n",
      "Epoch=615, iteration=2, train_loss=2.788654088973999\n",
      "Epoch=615, iteration=3, train_loss=2.9173812866210938\n",
      "Epoch=615, val_loss=2.9106826782226562\n",
      "Epoch=616, iteration=0, train_loss=2.753079891204834\n",
      "Epoch=616, iteration=1, train_loss=2.749582290649414\n",
      "Epoch=616, iteration=2, train_loss=2.7886528968811035\n",
      "Epoch=616, iteration=3, train_loss=2.917379140853882\n",
      "Epoch=616, val_loss=2.9106826782226562\n",
      "Epoch=617, iteration=0, train_loss=2.7530770301818848\n",
      "Epoch=617, iteration=1, train_loss=2.7495806217193604\n",
      "Epoch=617, iteration=2, train_loss=2.788651704788208\n",
      "Epoch=617, iteration=3, train_loss=2.917377471923828\n",
      "Epoch=617, val_loss=2.9106826782226562\n",
      "Epoch=618, iteration=0, train_loss=2.7530741691589355\n",
      "Epoch=618, iteration=1, train_loss=2.7495782375335693\n",
      "Epoch=618, iteration=2, train_loss=2.7886509895324707\n",
      "Epoch=618, iteration=3, train_loss=2.917375326156616\n",
      "Epoch=618, val_loss=2.9106826782226562\n",
      "Epoch=619, iteration=0, train_loss=2.753070831298828\n",
      "Epoch=619, iteration=1, train_loss=2.7495763301849365\n",
      "Epoch=619, iteration=2, train_loss=2.7886505126953125\n",
      "Epoch=619, iteration=3, train_loss=2.9173736572265625\n",
      "Epoch=619, val_loss=2.9106826782226562\n",
      "Epoch=620, iteration=0, train_loss=2.753068447113037\n",
      "Epoch=620, iteration=1, train_loss=2.7495744228363037\n",
      "Epoch=620, iteration=2, train_loss=2.788649797439575\n",
      "Epoch=620, iteration=3, train_loss=2.9173715114593506\n",
      "Epoch=620, val_loss=2.9106826782226562\n",
      "Epoch=621, iteration=0, train_loss=2.753065347671509\n",
      "Epoch=621, iteration=1, train_loss=2.749572515487671\n",
      "Epoch=621, iteration=2, train_loss=2.7886486053466797\n",
      "Epoch=621, iteration=3, train_loss=2.917369842529297\n",
      "Epoch=621, val_loss=2.9106826782226562\n",
      "Epoch=622, iteration=0, train_loss=2.7530624866485596\n",
      "Epoch=622, iteration=1, train_loss=2.749570608139038\n",
      "Epoch=622, iteration=2, train_loss=2.7886478900909424\n",
      "Epoch=622, iteration=3, train_loss=2.917368173599243\n",
      "Epoch=622, val_loss=2.9106826782226562\n",
      "Epoch=623, iteration=0, train_loss=2.7530593872070312\n",
      "Epoch=623, iteration=1, train_loss=2.7495687007904053\n",
      "Epoch=623, iteration=2, train_loss=2.788647174835205\n",
      "Epoch=623, iteration=3, train_loss=2.9173665046691895\n",
      "Epoch=623, val_loss=2.9106826782226562\n",
      "Epoch=624, iteration=0, train_loss=2.753056526184082\n",
      "Epoch=624, iteration=1, train_loss=2.7495667934417725\n",
      "Epoch=624, iteration=2, train_loss=2.7886459827423096\n",
      "Epoch=624, iteration=3, train_loss=2.9173643589019775\n",
      "Epoch=624, val_loss=2.9106826782226562\n",
      "Epoch=625, iteration=0, train_loss=2.753053665161133\n",
      "Epoch=625, iteration=1, train_loss=2.7495646476745605\n",
      "Epoch=625, iteration=2, train_loss=2.7886455059051514\n",
      "Epoch=625, iteration=3, train_loss=2.917362689971924\n",
      "Epoch=625, val_loss=2.9106826782226562\n",
      "Epoch=626, iteration=0, train_loss=2.7530508041381836\n",
      "Epoch=626, iteration=1, train_loss=2.7495627403259277\n",
      "Epoch=626, iteration=2, train_loss=2.788644552230835\n",
      "Epoch=626, iteration=3, train_loss=2.917360305786133\n",
      "Epoch=626, val_loss=2.9106826782226562\n",
      "Epoch=627, iteration=0, train_loss=2.7530479431152344\n",
      "Epoch=627, iteration=1, train_loss=2.749561071395874\n",
      "Epoch=627, iteration=2, train_loss=2.7886435985565186\n",
      "Epoch=627, iteration=3, train_loss=2.917358875274658\n",
      "Epoch=627, val_loss=2.9106826782226562\n",
      "Epoch=628, iteration=0, train_loss=2.7530453205108643\n",
      "Epoch=628, iteration=1, train_loss=2.749558925628662\n",
      "Epoch=628, iteration=2, train_loss=2.788642644882202\n",
      "Epoch=628, iteration=3, train_loss=2.9173572063446045\n",
      "Epoch=628, val_loss=2.9106826782226562\n",
      "Epoch=629, iteration=0, train_loss=2.753042221069336\n",
      "Epoch=629, iteration=1, train_loss=2.7495570182800293\n",
      "Epoch=629, iteration=2, train_loss=2.788641929626465\n",
      "Epoch=629, iteration=3, train_loss=2.9173552989959717\n",
      "Epoch=629, val_loss=2.9106826782226562\n",
      "Epoch=630, iteration=0, train_loss=2.753039598464966\n",
      "Epoch=630, iteration=1, train_loss=2.7495551109313965\n",
      "Epoch=630, iteration=2, train_loss=2.7886412143707275\n",
      "Epoch=630, iteration=3, train_loss=2.917353391647339\n",
      "Epoch=630, val_loss=2.9106829166412354\n",
      "Epoch=631, iteration=0, train_loss=2.7530364990234375\n",
      "Epoch=631, iteration=1, train_loss=2.7495532035827637\n",
      "Epoch=631, iteration=2, train_loss=2.7886404991149902\n",
      "Epoch=631, iteration=3, train_loss=2.9173521995544434\n",
      "Epoch=631, val_loss=2.9106826782226562\n",
      "Epoch=632, iteration=0, train_loss=2.7530336380004883\n",
      "Epoch=632, iteration=1, train_loss=2.749551296234131\n",
      "Epoch=632, iteration=2, train_loss=2.788639545440674\n",
      "Epoch=632, iteration=3, train_loss=2.917349338531494\n",
      "Epoch=632, val_loss=2.9106829166412354\n",
      "Epoch=633, iteration=0, train_loss=2.753031015396118\n",
      "Epoch=633, iteration=1, train_loss=2.749549627304077\n",
      "Epoch=633, iteration=2, train_loss=2.7886388301849365\n",
      "Epoch=633, iteration=3, train_loss=2.9173476696014404\n",
      "Epoch=633, val_loss=2.9106829166412354\n",
      "Epoch=634, iteration=0, train_loss=2.753028154373169\n",
      "Epoch=634, iteration=1, train_loss=2.749547243118286\n",
      "Epoch=634, iteration=2, train_loss=2.788638114929199\n",
      "Epoch=634, iteration=3, train_loss=2.917346239089966\n",
      "Epoch=634, val_loss=2.9106829166412354\n",
      "Epoch=635, iteration=0, train_loss=2.753025531768799\n",
      "Epoch=635, iteration=1, train_loss=2.7495455741882324\n",
      "Epoch=635, iteration=2, train_loss=2.788637161254883\n",
      "Epoch=635, iteration=3, train_loss=2.917344570159912\n",
      "Epoch=635, val_loss=2.9106829166412354\n",
      "Epoch=636, iteration=0, train_loss=2.7530224323272705\n",
      "Epoch=636, iteration=1, train_loss=2.7495434284210205\n",
      "Epoch=636, iteration=2, train_loss=2.7886364459991455\n",
      "Epoch=636, iteration=3, train_loss=2.9173429012298584\n",
      "Epoch=636, val_loss=2.9106831550598145\n",
      "Epoch=637, iteration=0, train_loss=2.7530200481414795\n",
      "Epoch=637, iteration=1, train_loss=2.749541759490967\n",
      "Epoch=637, iteration=2, train_loss=2.78863525390625\n",
      "Epoch=637, iteration=3, train_loss=2.9173407554626465\n",
      "Epoch=637, val_loss=2.9106829166412354\n",
      "Epoch=638, iteration=0, train_loss=2.753016948699951\n",
      "Epoch=638, iteration=1, train_loss=2.749539852142334\n",
      "Epoch=638, iteration=2, train_loss=2.788635015487671\n",
      "Epoch=638, iteration=3, train_loss=2.917339324951172\n",
      "Epoch=638, val_loss=2.9106831550598145\n",
      "Epoch=639, iteration=0, train_loss=2.753014087677002\n",
      "Epoch=639, iteration=1, train_loss=2.749537944793701\n",
      "Epoch=639, iteration=2, train_loss=2.7886338233947754\n",
      "Epoch=639, iteration=3, train_loss=2.917336940765381\n",
      "Epoch=639, val_loss=2.9106831550598145\n",
      "Epoch=640, iteration=0, train_loss=2.753011465072632\n",
      "Epoch=640, iteration=1, train_loss=2.7495362758636475\n",
      "Epoch=640, iteration=2, train_loss=2.788633108139038\n",
      "Epoch=640, iteration=3, train_loss=2.9173357486724854\n",
      "Epoch=640, val_loss=2.9106836318969727\n",
      "Epoch=641, iteration=0, train_loss=2.7530088424682617\n",
      "Epoch=641, iteration=1, train_loss=2.7495343685150146\n",
      "Epoch=641, iteration=2, train_loss=2.788632392883301\n",
      "Epoch=641, iteration=3, train_loss=2.9173336029052734\n",
      "Epoch=641, val_loss=2.9106836318969727\n",
      "Epoch=642, iteration=0, train_loss=2.7530059814453125\n",
      "Epoch=642, iteration=1, train_loss=2.749532461166382\n",
      "Epoch=642, iteration=2, train_loss=2.7886316776275635\n",
      "Epoch=642, iteration=3, train_loss=2.9173319339752197\n",
      "Epoch=642, val_loss=2.9106836318969727\n",
      "Epoch=643, iteration=0, train_loss=2.753002882003784\n",
      "Epoch=643, iteration=1, train_loss=2.749530792236328\n",
      "Epoch=643, iteration=2, train_loss=2.788630723953247\n",
      "Epoch=643, iteration=3, train_loss=2.917330265045166\n",
      "Epoch=643, val_loss=2.9106836318969727\n",
      "Epoch=644, iteration=0, train_loss=2.753000497817993\n",
      "Epoch=644, iteration=1, train_loss=2.7495288848876953\n",
      "Epoch=644, iteration=2, train_loss=2.788630247116089\n",
      "Epoch=644, iteration=3, train_loss=2.9173285961151123\n",
      "Epoch=644, val_loss=2.9106836318969727\n",
      "Epoch=645, iteration=0, train_loss=2.752997875213623\n",
      "Epoch=645, iteration=1, train_loss=2.7495269775390625\n",
      "Epoch=645, iteration=2, train_loss=2.7886295318603516\n",
      "Epoch=645, iteration=3, train_loss=2.9173264503479004\n",
      "Epoch=645, val_loss=2.9106836318969727\n",
      "Epoch=646, iteration=0, train_loss=2.752995014190674\n",
      "Epoch=646, iteration=1, train_loss=2.749525308609009\n",
      "Epoch=646, iteration=2, train_loss=2.788628578186035\n",
      "Epoch=646, iteration=3, train_loss=2.917325019836426\n",
      "Epoch=646, val_loss=2.9106838703155518\n",
      "Epoch=647, iteration=0, train_loss=2.7529923915863037\n",
      "Epoch=647, iteration=1, train_loss=2.749523162841797\n",
      "Epoch=647, iteration=2, train_loss=2.788627862930298\n",
      "Epoch=647, iteration=3, train_loss=2.917323112487793\n",
      "Epoch=647, val_loss=2.9106838703155518\n",
      "Epoch=648, iteration=0, train_loss=2.7529895305633545\n",
      "Epoch=648, iteration=1, train_loss=2.7495217323303223\n",
      "Epoch=648, iteration=2, train_loss=2.7886271476745605\n",
      "Epoch=648, iteration=3, train_loss=2.91732120513916\n",
      "Epoch=648, val_loss=2.9106838703155518\n",
      "Epoch=649, iteration=0, train_loss=2.7529866695404053\n",
      "Epoch=649, iteration=1, train_loss=2.7495195865631104\n",
      "Epoch=649, iteration=2, train_loss=2.788626194000244\n",
      "Epoch=649, iteration=3, train_loss=2.9173200130462646\n",
      "Epoch=649, val_loss=2.91068434715271\n",
      "Epoch=650, iteration=0, train_loss=2.7529842853546143\n",
      "Epoch=650, iteration=1, train_loss=2.7495176792144775\n",
      "Epoch=650, iteration=2, train_loss=2.788625478744507\n",
      "Epoch=650, iteration=3, train_loss=2.9173178672790527\n",
      "Epoch=650, val_loss=2.9106838703155518\n",
      "Epoch=651, iteration=0, train_loss=2.752981662750244\n",
      "Epoch=651, iteration=1, train_loss=2.749516248703003\n",
      "Epoch=651, iteration=2, train_loss=2.7886245250701904\n",
      "Epoch=651, iteration=3, train_loss=2.917316436767578\n",
      "Epoch=651, val_loss=2.9106838703155518\n",
      "Epoch=652, iteration=0, train_loss=2.752979040145874\n",
      "Epoch=652, iteration=1, train_loss=2.74951434135437\n",
      "Epoch=652, iteration=2, train_loss=2.7886240482330322\n",
      "Epoch=652, iteration=3, train_loss=2.9173145294189453\n",
      "Epoch=652, val_loss=2.9106838703155518\n",
      "Epoch=653, iteration=0, train_loss=2.752976179122925\n",
      "Epoch=653, iteration=1, train_loss=2.749512195587158\n",
      "Epoch=653, iteration=2, train_loss=2.788623571395874\n",
      "Epoch=653, iteration=3, train_loss=2.9173126220703125\n",
      "Epoch=653, val_loss=2.91068434715271\n",
      "Epoch=654, iteration=0, train_loss=2.7529733180999756\n",
      "Epoch=654, iteration=1, train_loss=2.7495105266571045\n",
      "Epoch=654, iteration=2, train_loss=2.7886223793029785\n",
      "Epoch=654, iteration=3, train_loss=2.917310953140259\n",
      "Epoch=654, val_loss=2.91068434715271\n",
      "Epoch=655, iteration=0, train_loss=2.7529711723327637\n",
      "Epoch=655, iteration=1, train_loss=2.749508857727051\n",
      "Epoch=655, iteration=2, train_loss=2.7886219024658203\n",
      "Epoch=655, iteration=3, train_loss=2.917309284210205\n",
      "Epoch=655, val_loss=2.91068434715271\n",
      "Epoch=656, iteration=0, train_loss=2.7529683113098145\n",
      "Epoch=656, iteration=1, train_loss=2.749506950378418\n",
      "Epoch=656, iteration=2, train_loss=2.788620948791504\n",
      "Epoch=656, iteration=3, train_loss=2.9173076152801514\n",
      "Epoch=656, val_loss=2.910684585571289\n",
      "Epoch=657, iteration=0, train_loss=2.7529656887054443\n",
      "Epoch=657, iteration=1, train_loss=2.7495052814483643\n",
      "Epoch=657, iteration=2, train_loss=2.7886202335357666\n",
      "Epoch=657, iteration=3, train_loss=2.9173059463500977\n",
      "Epoch=657, val_loss=2.910684585571289\n",
      "Epoch=658, iteration=0, train_loss=2.752963066101074\n",
      "Epoch=658, iteration=1, train_loss=2.7495038509368896\n",
      "Epoch=658, iteration=2, train_loss=2.7886195182800293\n",
      "Epoch=658, iteration=3, train_loss=2.917304515838623\n",
      "Epoch=658, val_loss=2.910684585571289\n",
      "Epoch=659, iteration=0, train_loss=2.752960443496704\n",
      "Epoch=659, iteration=1, train_loss=2.7495017051696777\n",
      "Epoch=659, iteration=2, train_loss=2.788618803024292\n",
      "Epoch=659, iteration=3, train_loss=2.917302370071411\n",
      "Epoch=659, val_loss=2.910684585571289\n",
      "Epoch=660, iteration=0, train_loss=2.752957582473755\n",
      "Epoch=660, iteration=1, train_loss=2.749500036239624\n",
      "Epoch=660, iteration=2, train_loss=2.7886180877685547\n",
      "Epoch=660, iteration=3, train_loss=2.9173004627227783\n",
      "Epoch=660, val_loss=2.91068434715271\n",
      "Epoch=661, iteration=0, train_loss=2.752955436706543\n",
      "Epoch=661, iteration=1, train_loss=2.7494983673095703\n",
      "Epoch=661, iteration=2, train_loss=2.7886176109313965\n",
      "Epoch=661, iteration=3, train_loss=2.9172990322113037\n",
      "Epoch=661, val_loss=2.9106850624084473\n",
      "Epoch=662, iteration=0, train_loss=2.7529525756835938\n",
      "Epoch=662, iteration=1, train_loss=2.7494962215423584\n",
      "Epoch=662, iteration=2, train_loss=2.78861665725708\n",
      "Epoch=662, iteration=3, train_loss=2.91729736328125\n",
      "Epoch=662, val_loss=2.9106850624084473\n",
      "Epoch=663, iteration=0, train_loss=2.7529501914978027\n",
      "Epoch=663, iteration=1, train_loss=2.7494943141937256\n",
      "Epoch=663, iteration=2, train_loss=2.7886157035827637\n",
      "Epoch=663, iteration=3, train_loss=2.9172959327697754\n",
      "Epoch=663, val_loss=2.9106850624084473\n",
      "Epoch=664, iteration=0, train_loss=2.7529473304748535\n",
      "Epoch=664, iteration=1, train_loss=2.749492645263672\n",
      "Epoch=664, iteration=2, train_loss=2.7886152267456055\n",
      "Epoch=664, iteration=3, train_loss=2.9172942638397217\n",
      "Epoch=664, val_loss=2.9106850624084473\n",
      "Epoch=665, iteration=0, train_loss=2.7529447078704834\n",
      "Epoch=665, iteration=1, train_loss=2.749490976333618\n",
      "Epoch=665, iteration=2, train_loss=2.788614511489868\n",
      "Epoch=665, iteration=3, train_loss=2.917292594909668\n",
      "Epoch=665, val_loss=2.9106853008270264\n",
      "Epoch=666, iteration=0, train_loss=2.7529423236846924\n",
      "Epoch=666, iteration=1, train_loss=2.7494893074035645\n",
      "Epoch=666, iteration=2, train_loss=2.788613796234131\n",
      "Epoch=666, iteration=3, train_loss=2.917290449142456\n",
      "Epoch=666, val_loss=2.9106853008270264\n",
      "Epoch=667, iteration=0, train_loss=2.7529397010803223\n",
      "Epoch=667, iteration=1, train_loss=2.74948787689209\n",
      "Epoch=667, iteration=2, train_loss=2.7886133193969727\n",
      "Epoch=667, iteration=3, train_loss=2.9172890186309814\n",
      "Epoch=667, val_loss=2.9106857776641846\n",
      "Epoch=668, iteration=0, train_loss=2.752937078475952\n",
      "Epoch=668, iteration=1, train_loss=2.749486207962036\n",
      "Epoch=668, iteration=2, train_loss=2.7886126041412354\n",
      "Epoch=668, iteration=3, train_loss=2.9172873497009277\n",
      "Epoch=668, val_loss=2.9106857776641846\n",
      "Epoch=669, iteration=0, train_loss=2.752934694290161\n",
      "Epoch=669, iteration=1, train_loss=2.7494843006134033\n",
      "Epoch=669, iteration=2, train_loss=2.788611650466919\n",
      "Epoch=669, iteration=3, train_loss=2.917285680770874\n",
      "Epoch=669, val_loss=2.9106857776641846\n",
      "Epoch=670, iteration=0, train_loss=2.752931833267212\n",
      "Epoch=670, iteration=1, train_loss=2.7494821548461914\n",
      "Epoch=670, iteration=2, train_loss=2.7886111736297607\n",
      "Epoch=670, iteration=3, train_loss=2.9172842502593994\n",
      "Epoch=670, val_loss=2.9106857776641846\n",
      "Epoch=671, iteration=0, train_loss=2.752929449081421\n",
      "Epoch=671, iteration=1, train_loss=2.749480724334717\n",
      "Epoch=671, iteration=2, train_loss=2.7886102199554443\n",
      "Epoch=671, iteration=3, train_loss=2.9172823429107666\n",
      "Epoch=671, val_loss=2.9106860160827637\n",
      "Epoch=672, iteration=0, train_loss=2.75292706489563\n",
      "Epoch=672, iteration=1, train_loss=2.749479055404663\n",
      "Epoch=672, iteration=2, train_loss=2.788609743118286\n",
      "Epoch=672, iteration=3, train_loss=2.917280912399292\n",
      "Epoch=672, val_loss=2.9106862545013428\n",
      "Epoch=673, iteration=0, train_loss=2.752924680709839\n",
      "Epoch=673, iteration=1, train_loss=2.7494773864746094\n",
      "Epoch=673, iteration=2, train_loss=2.788609027862549\n",
      "Epoch=673, iteration=3, train_loss=2.91727876663208\n",
      "Epoch=673, val_loss=2.9106860160827637\n",
      "Epoch=674, iteration=0, train_loss=2.7529218196868896\n",
      "Epoch=674, iteration=1, train_loss=2.7494754791259766\n",
      "Epoch=674, iteration=2, train_loss=2.7886083126068115\n",
      "Epoch=674, iteration=3, train_loss=2.9172770977020264\n",
      "Epoch=674, val_loss=2.9106862545013428\n",
      "Epoch=675, iteration=0, train_loss=2.7529196739196777\n",
      "Epoch=675, iteration=1, train_loss=2.749474048614502\n",
      "Epoch=675, iteration=2, train_loss=2.788607597351074\n",
      "Epoch=675, iteration=3, train_loss=2.9172756671905518\n",
      "Epoch=675, val_loss=2.9106862545013428\n",
      "Epoch=676, iteration=0, train_loss=2.7529170513153076\n",
      "Epoch=676, iteration=1, train_loss=2.749472141265869\n",
      "Epoch=676, iteration=2, train_loss=2.788606882095337\n",
      "Epoch=676, iteration=3, train_loss=2.9172744750976562\n",
      "Epoch=676, val_loss=2.910686731338501\n",
      "Epoch=677, iteration=0, train_loss=2.7529144287109375\n",
      "Epoch=677, iteration=1, train_loss=2.7494707107543945\n",
      "Epoch=677, iteration=2, train_loss=2.7886059284210205\n",
      "Epoch=677, iteration=3, train_loss=2.9172723293304443\n",
      "Epoch=677, val_loss=2.910686731338501\n",
      "Epoch=678, iteration=0, train_loss=2.7529118061065674\n",
      "Epoch=678, iteration=1, train_loss=2.7494688034057617\n",
      "Epoch=678, iteration=2, train_loss=2.7886054515838623\n",
      "Epoch=678, iteration=3, train_loss=2.9172706604003906\n",
      "Epoch=678, val_loss=2.910686731338501\n",
      "Epoch=679, iteration=0, train_loss=2.7529096603393555\n",
      "Epoch=679, iteration=1, train_loss=2.749466896057129\n",
      "Epoch=679, iteration=2, train_loss=2.788604497909546\n",
      "Epoch=679, iteration=3, train_loss=2.917269229888916\n",
      "Epoch=679, val_loss=2.910686731338501\n",
      "Epoch=680, iteration=0, train_loss=2.7529072761535645\n",
      "Epoch=680, iteration=1, train_loss=2.749465227127075\n",
      "Epoch=680, iteration=2, train_loss=2.7886040210723877\n",
      "Epoch=680, iteration=3, train_loss=2.9172675609588623\n",
      "Epoch=680, val_loss=2.91068696975708\n",
      "Epoch=681, iteration=0, train_loss=2.7529046535491943\n",
      "Epoch=681, iteration=1, train_loss=2.7494640350341797\n",
      "Epoch=681, iteration=2, train_loss=2.7886035442352295\n",
      "Epoch=681, iteration=3, train_loss=2.9172661304473877\n",
      "Epoch=681, val_loss=2.91068696975708\n",
      "Epoch=682, iteration=0, train_loss=2.752902030944824\n",
      "Epoch=682, iteration=1, train_loss=2.749462127685547\n",
      "Epoch=682, iteration=2, train_loss=2.788602590560913\n",
      "Epoch=682, iteration=3, train_loss=2.917264461517334\n",
      "Epoch=682, val_loss=2.9106874465942383\n",
      "Epoch=683, iteration=0, train_loss=2.752899408340454\n",
      "Epoch=683, iteration=1, train_loss=2.7494606971740723\n",
      "Epoch=683, iteration=2, train_loss=2.788602113723755\n",
      "Epoch=683, iteration=3, train_loss=2.9172627925872803\n",
      "Epoch=683, val_loss=2.9106874465942383\n",
      "Epoch=684, iteration=0, train_loss=2.752897262573242\n",
      "Epoch=684, iteration=1, train_loss=2.7494587898254395\n",
      "Epoch=684, iteration=2, train_loss=2.7886013984680176\n",
      "Epoch=684, iteration=3, train_loss=2.9172611236572266\n",
      "Epoch=684, val_loss=2.9106876850128174\n",
      "Epoch=685, iteration=0, train_loss=2.752894639968872\n",
      "Epoch=685, iteration=1, train_loss=2.7494571208953857\n",
      "Epoch=685, iteration=2, train_loss=2.788600444793701\n",
      "Epoch=685, iteration=3, train_loss=2.917259693145752\n",
      "Epoch=685, val_loss=2.9106876850128174\n",
      "Epoch=686, iteration=0, train_loss=2.75289249420166\n",
      "Epoch=686, iteration=1, train_loss=2.749455451965332\n",
      "Epoch=686, iteration=2, train_loss=2.788599729537964\n",
      "Epoch=686, iteration=3, train_loss=2.9172580242156982\n",
      "Epoch=686, val_loss=2.9106881618499756\n",
      "Epoch=687, iteration=0, train_loss=2.752890110015869\n",
      "Epoch=687, iteration=1, train_loss=2.7494540214538574\n",
      "Epoch=687, iteration=2, train_loss=2.7885992527008057\n",
      "Epoch=687, iteration=3, train_loss=2.9172565937042236\n",
      "Epoch=687, val_loss=2.9106881618499756\n",
      "Epoch=688, iteration=0, train_loss=2.752887487411499\n",
      "Epoch=688, iteration=1, train_loss=2.7494523525238037\n",
      "Epoch=688, iteration=2, train_loss=2.7885987758636475\n",
      "Epoch=688, iteration=3, train_loss=2.9172544479370117\n",
      "Epoch=688, val_loss=2.9106884002685547\n",
      "Epoch=689, iteration=0, train_loss=2.752885103225708\n",
      "Epoch=689, iteration=1, train_loss=2.749450445175171\n",
      "Epoch=689, iteration=2, train_loss=2.78859806060791\n",
      "Epoch=689, iteration=3, train_loss=2.917253255844116\n",
      "Epoch=689, val_loss=2.9106884002685547\n",
      "Epoch=690, iteration=0, train_loss=2.752882719039917\n",
      "Epoch=690, iteration=1, train_loss=2.7494490146636963\n",
      "Epoch=690, iteration=2, train_loss=2.788597345352173\n",
      "Epoch=690, iteration=3, train_loss=2.9172518253326416\n",
      "Epoch=690, val_loss=2.9106884002685547\n",
      "Epoch=691, iteration=0, train_loss=2.752880334854126\n",
      "Epoch=691, iteration=1, train_loss=2.7494475841522217\n",
      "Epoch=691, iteration=2, train_loss=2.7885966300964355\n",
      "Epoch=691, iteration=3, train_loss=2.917250156402588\n",
      "Epoch=691, val_loss=2.910688638687134\n",
      "Epoch=692, iteration=0, train_loss=2.752877712249756\n",
      "Epoch=692, iteration=1, train_loss=2.7494454383850098\n",
      "Epoch=692, iteration=2, train_loss=2.7885961532592773\n",
      "Epoch=692, iteration=3, train_loss=2.9172487258911133\n",
      "Epoch=692, val_loss=2.9106884002685547\n",
      "Epoch=693, iteration=0, train_loss=2.752875328063965\n",
      "Epoch=693, iteration=1, train_loss=2.7494444847106934\n",
      "Epoch=693, iteration=2, train_loss=2.788594961166382\n",
      "Epoch=693, iteration=3, train_loss=2.9172470569610596\n",
      "Epoch=693, val_loss=2.910688638687134\n",
      "Epoch=694, iteration=0, train_loss=2.752873182296753\n",
      "Epoch=694, iteration=1, train_loss=2.7494425773620605\n",
      "Epoch=694, iteration=2, train_loss=2.7885944843292236\n",
      "Epoch=694, iteration=3, train_loss=2.917245388031006\n",
      "Epoch=694, val_loss=2.910689115524292\n",
      "Epoch=695, iteration=0, train_loss=2.7528703212738037\n",
      "Epoch=695, iteration=1, train_loss=2.7494406700134277\n",
      "Epoch=695, iteration=2, train_loss=2.7885940074920654\n",
      "Epoch=695, iteration=3, train_loss=2.917243480682373\n",
      "Epoch=695, val_loss=2.910689115524292\n",
      "Epoch=696, iteration=0, train_loss=2.752868175506592\n",
      "Epoch=696, iteration=1, train_loss=2.749439239501953\n",
      "Epoch=696, iteration=2, train_loss=2.7885935306549072\n",
      "Epoch=696, iteration=3, train_loss=2.9172418117523193\n",
      "Epoch=696, val_loss=2.910689115524292\n",
      "Epoch=697, iteration=0, train_loss=2.752866268157959\n",
      "Epoch=697, iteration=1, train_loss=2.7494375705718994\n",
      "Epoch=697, iteration=2, train_loss=2.78859281539917\n",
      "Epoch=697, iteration=3, train_loss=2.9172401428222656\n",
      "Epoch=697, val_loss=2.910689115524292\n",
      "Epoch=698, iteration=0, train_loss=2.752863883972168\n",
      "Epoch=698, iteration=1, train_loss=2.7494359016418457\n",
      "Epoch=698, iteration=2, train_loss=2.7885921001434326\n",
      "Epoch=698, iteration=3, train_loss=2.917238712310791\n",
      "Epoch=698, val_loss=2.910689353942871\n",
      "Epoch=699, iteration=0, train_loss=2.752861499786377\n",
      "Epoch=699, iteration=1, train_loss=2.749434471130371\n",
      "Epoch=699, iteration=2, train_loss=2.7885913848876953\n",
      "Epoch=699, iteration=3, train_loss=2.9172370433807373\n",
      "Epoch=699, val_loss=2.9106898307800293\n",
      "Epoch=700, iteration=0, train_loss=2.752858877182007\n",
      "Epoch=700, iteration=1, train_loss=2.7494330406188965\n",
      "Epoch=700, iteration=2, train_loss=2.788590908050537\n",
      "Epoch=700, iteration=3, train_loss=2.917236089706421\n",
      "Epoch=700, val_loss=2.9106898307800293\n",
      "Epoch=701, iteration=0, train_loss=2.752856492996216\n",
      "Epoch=701, iteration=1, train_loss=2.7494311332702637\n",
      "Epoch=701, iteration=2, train_loss=2.7885901927948\n",
      "Epoch=701, iteration=3, train_loss=2.9172346591949463\n",
      "Epoch=701, val_loss=2.9106898307800293\n",
      "Epoch=702, iteration=0, train_loss=2.752854108810425\n",
      "Epoch=702, iteration=1, train_loss=2.749429702758789\n",
      "Epoch=702, iteration=2, train_loss=2.7885894775390625\n",
      "Epoch=702, iteration=3, train_loss=2.9172329902648926\n",
      "Epoch=702, val_loss=2.9106900691986084\n",
      "Epoch=703, iteration=0, train_loss=2.752851963043213\n",
      "Epoch=703, iteration=1, train_loss=2.7494282722473145\n",
      "Epoch=703, iteration=2, train_loss=2.7885890007019043\n",
      "Epoch=703, iteration=3, train_loss=2.917231321334839\n",
      "Epoch=703, val_loss=2.9106900691986084\n",
      "Epoch=704, iteration=0, train_loss=2.752849578857422\n",
      "Epoch=704, iteration=1, train_loss=2.7494263648986816\n",
      "Epoch=704, iteration=2, train_loss=2.788588285446167\n",
      "Epoch=704, iteration=3, train_loss=2.917229413986206\n",
      "Epoch=704, val_loss=2.9106900691986084\n",
      "Epoch=705, iteration=0, train_loss=2.75284743309021\n",
      "Epoch=705, iteration=1, train_loss=2.749424934387207\n",
      "Epoch=705, iteration=2, train_loss=2.7885873317718506\n",
      "Epoch=705, iteration=3, train_loss=2.9172277450561523\n",
      "Epoch=705, val_loss=2.9106905460357666\n",
      "Epoch=706, iteration=0, train_loss=2.752845048904419\n",
      "Epoch=706, iteration=1, train_loss=2.7494232654571533\n",
      "Epoch=706, iteration=2, train_loss=2.7885870933532715\n",
      "Epoch=706, iteration=3, train_loss=2.917226552963257\n",
      "Epoch=706, val_loss=2.9106907844543457\n",
      "Epoch=707, iteration=0, train_loss=2.752842664718628\n",
      "Epoch=707, iteration=1, train_loss=2.7494218349456787\n",
      "Epoch=707, iteration=2, train_loss=2.788586378097534\n",
      "Epoch=707, iteration=3, train_loss=2.917224645614624\n",
      "Epoch=707, val_loss=2.910691261291504\n",
      "Epoch=708, iteration=0, train_loss=2.752840280532837\n",
      "Epoch=708, iteration=1, train_loss=2.749420166015625\n",
      "Epoch=708, iteration=2, train_loss=2.788585662841797\n",
      "Epoch=708, iteration=3, train_loss=2.9172234535217285\n",
      "Epoch=708, val_loss=2.9106907844543457\n",
      "Epoch=709, iteration=0, train_loss=2.752838134765625\n",
      "Epoch=709, iteration=1, train_loss=2.7494184970855713\n",
      "Epoch=709, iteration=2, train_loss=2.7885851860046387\n",
      "Epoch=709, iteration=3, train_loss=2.9172215461730957\n",
      "Epoch=709, val_loss=2.910691261291504\n",
      "Epoch=710, iteration=0, train_loss=2.752835750579834\n",
      "Epoch=710, iteration=1, train_loss=2.7494170665740967\n",
      "Epoch=710, iteration=2, train_loss=2.7885844707489014\n",
      "Epoch=710, iteration=3, train_loss=2.9172203540802\n",
      "Epoch=710, val_loss=2.910691499710083\n",
      "Epoch=711, iteration=0, train_loss=2.752833366394043\n",
      "Epoch=711, iteration=1, train_loss=2.749415397644043\n",
      "Epoch=711, iteration=2, train_loss=2.788583993911743\n",
      "Epoch=711, iteration=3, train_loss=2.9172184467315674\n",
      "Epoch=711, val_loss=2.910691499710083\n",
      "Epoch=712, iteration=0, train_loss=2.752831220626831\n",
      "Epoch=712, iteration=1, train_loss=2.7494139671325684\n",
      "Epoch=712, iteration=2, train_loss=2.788583278656006\n",
      "Epoch=712, iteration=3, train_loss=2.917217254638672\n",
      "Epoch=712, val_loss=2.910691738128662\n",
      "Epoch=713, iteration=0, train_loss=2.7528293132781982\n",
      "Epoch=713, iteration=1, train_loss=2.7494125366210938\n",
      "Epoch=713, iteration=2, train_loss=2.7885828018188477\n",
      "Epoch=713, iteration=3, train_loss=2.9172158241271973\n",
      "Epoch=713, val_loss=2.910691738128662\n",
      "Epoch=714, iteration=0, train_loss=2.752826690673828\n",
      "Epoch=714, iteration=1, train_loss=2.74941086769104\n",
      "Epoch=714, iteration=2, train_loss=2.7885818481445312\n",
      "Epoch=714, iteration=3, train_loss=2.9172141551971436\n",
      "Epoch=714, val_loss=2.910691738128662\n",
      "Epoch=715, iteration=0, train_loss=2.752824306488037\n",
      "Epoch=715, iteration=1, train_loss=2.7494094371795654\n",
      "Epoch=715, iteration=2, train_loss=2.788581609725952\n",
      "Epoch=715, iteration=3, train_loss=2.91721248626709\n",
      "Epoch=715, val_loss=2.9106924533843994\n",
      "Epoch=716, iteration=0, train_loss=2.752822160720825\n",
      "Epoch=716, iteration=1, train_loss=2.749408006668091\n",
      "Epoch=716, iteration=2, train_loss=2.788580894470215\n",
      "Epoch=716, iteration=3, train_loss=2.9172110557556152\n",
      "Epoch=716, val_loss=2.9106922149658203\n",
      "Epoch=717, iteration=0, train_loss=2.7528200149536133\n",
      "Epoch=717, iteration=1, train_loss=2.749406099319458\n",
      "Epoch=717, iteration=2, train_loss=2.7885804176330566\n",
      "Epoch=717, iteration=3, train_loss=2.9172096252441406\n",
      "Epoch=717, val_loss=2.9106924533843994\n",
      "Epoch=718, iteration=0, train_loss=2.7528176307678223\n",
      "Epoch=718, iteration=1, train_loss=2.7494044303894043\n",
      "Epoch=718, iteration=2, train_loss=2.7885797023773193\n",
      "Epoch=718, iteration=3, train_loss=2.917207956314087\n",
      "Epoch=718, val_loss=2.9106929302215576\n",
      "Epoch=719, iteration=0, train_loss=2.7528154850006104\n",
      "Epoch=719, iteration=1, train_loss=2.749403238296509\n",
      "Epoch=719, iteration=2, train_loss=2.788579225540161\n",
      "Epoch=719, iteration=3, train_loss=2.917206287384033\n",
      "Epoch=719, val_loss=2.9106929302215576\n",
      "Epoch=720, iteration=0, train_loss=2.7528133392333984\n",
      "Epoch=720, iteration=1, train_loss=2.749401569366455\n",
      "Epoch=720, iteration=2, train_loss=2.788578510284424\n",
      "Epoch=720, iteration=3, train_loss=2.9172048568725586\n",
      "Epoch=720, val_loss=2.9106929302215576\n",
      "Epoch=721, iteration=0, train_loss=2.7528111934661865\n",
      "Epoch=721, iteration=1, train_loss=2.7493999004364014\n",
      "Epoch=721, iteration=2, train_loss=2.7885775566101074\n",
      "Epoch=721, iteration=3, train_loss=2.917203426361084\n",
      "Epoch=721, val_loss=2.9106931686401367\n",
      "Epoch=722, iteration=0, train_loss=2.7528085708618164\n",
      "Epoch=722, iteration=1, train_loss=2.7493982315063477\n",
      "Epoch=722, iteration=2, train_loss=2.7885775566101074\n",
      "Epoch=722, iteration=3, train_loss=2.9172017574310303\n",
      "Epoch=722, val_loss=2.9106931686401367\n",
      "Epoch=723, iteration=0, train_loss=2.7528061866760254\n",
      "Epoch=723, iteration=1, train_loss=2.7493972778320312\n",
      "Epoch=723, iteration=2, train_loss=2.78857684135437\n",
      "Epoch=723, iteration=3, train_loss=2.917200803756714\n",
      "Epoch=723, val_loss=2.910693645477295\n",
      "Epoch=724, iteration=0, train_loss=2.7528042793273926\n",
      "Epoch=724, iteration=1, train_loss=2.7493956089019775\n",
      "Epoch=724, iteration=2, train_loss=2.7885758876800537\n",
      "Epoch=724, iteration=3, train_loss=2.9171993732452393\n",
      "Epoch=724, val_loss=2.910693645477295\n",
      "Epoch=725, iteration=0, train_loss=2.7528018951416016\n",
      "Epoch=725, iteration=1, train_loss=2.749393939971924\n",
      "Epoch=725, iteration=2, train_loss=2.7885754108428955\n",
      "Epoch=725, iteration=3, train_loss=2.9171977043151855\n",
      "Epoch=725, val_loss=2.910693883895874\n",
      "Epoch=726, iteration=0, train_loss=2.7527999877929688\n",
      "Epoch=726, iteration=1, train_loss=2.749392509460449\n",
      "Epoch=726, iteration=2, train_loss=2.7885749340057373\n",
      "Epoch=726, iteration=3, train_loss=2.917196273803711\n",
      "Epoch=726, val_loss=2.910693883895874\n",
      "Epoch=727, iteration=0, train_loss=2.7527976036071777\n",
      "Epoch=727, iteration=1, train_loss=2.7493908405303955\n",
      "Epoch=727, iteration=2, train_loss=2.788574457168579\n",
      "Epoch=727, iteration=3, train_loss=2.9171946048736572\n",
      "Epoch=727, val_loss=2.910694122314453\n",
      "Epoch=728, iteration=0, train_loss=2.752795696258545\n",
      "Epoch=728, iteration=1, train_loss=2.749389171600342\n",
      "Epoch=728, iteration=2, train_loss=2.788573741912842\n",
      "Epoch=728, iteration=3, train_loss=2.9171929359436035\n",
      "Epoch=728, val_loss=2.910694122314453\n",
      "Epoch=729, iteration=0, train_loss=2.752793550491333\n",
      "Epoch=729, iteration=1, train_loss=2.749387502670288\n",
      "Epoch=729, iteration=2, train_loss=2.7885732650756836\n",
      "Epoch=729, iteration=3, train_loss=2.917191743850708\n",
      "Epoch=729, val_loss=2.9106945991516113\n",
      "Epoch=730, iteration=0, train_loss=2.752791404724121\n",
      "Epoch=730, iteration=1, train_loss=2.7493860721588135\n",
      "Epoch=730, iteration=2, train_loss=2.7885725498199463\n",
      "Epoch=730, iteration=3, train_loss=2.9171900749206543\n",
      "Epoch=730, val_loss=2.9106945991516113\n",
      "Epoch=731, iteration=0, train_loss=2.75278902053833\n",
      "Epoch=731, iteration=1, train_loss=2.749384880065918\n",
      "Epoch=731, iteration=2, train_loss=2.788572072982788\n",
      "Epoch=731, iteration=3, train_loss=2.917188882827759\n",
      "Epoch=731, val_loss=2.9106948375701904\n",
      "Epoch=732, iteration=0, train_loss=2.7527873516082764\n",
      "Epoch=732, iteration=1, train_loss=2.7493834495544434\n",
      "Epoch=732, iteration=2, train_loss=2.788571357727051\n",
      "Epoch=732, iteration=3, train_loss=2.917186975479126\n",
      "Epoch=732, val_loss=2.9106953144073486\n",
      "Epoch=733, iteration=0, train_loss=2.752784490585327\n",
      "Epoch=733, iteration=1, train_loss=2.7493820190429688\n",
      "Epoch=733, iteration=2, train_loss=2.7885708808898926\n",
      "Epoch=733, iteration=3, train_loss=2.9171857833862305\n",
      "Epoch=733, val_loss=2.9106955528259277\n",
      "Epoch=734, iteration=0, train_loss=2.7527823448181152\n",
      "Epoch=734, iteration=1, train_loss=2.749380350112915\n",
      "Epoch=734, iteration=2, train_loss=2.7885701656341553\n",
      "Epoch=734, iteration=3, train_loss=2.917184352874756\n",
      "Epoch=734, val_loss=2.9106955528259277\n",
      "Epoch=735, iteration=0, train_loss=2.7527804374694824\n",
      "Epoch=735, iteration=1, train_loss=2.7493789196014404\n",
      "Epoch=735, iteration=2, train_loss=2.788569688796997\n",
      "Epoch=735, iteration=3, train_loss=2.917182683944702\n",
      "Epoch=735, val_loss=2.9106955528259277\n",
      "Epoch=736, iteration=0, train_loss=2.7527782917022705\n",
      "Epoch=736, iteration=1, train_loss=2.749377727508545\n",
      "Epoch=736, iteration=2, train_loss=2.788569450378418\n",
      "Epoch=736, iteration=3, train_loss=2.9171807765960693\n",
      "Epoch=736, val_loss=2.910696029663086\n",
      "Epoch=737, iteration=0, train_loss=2.7527761459350586\n",
      "Epoch=737, iteration=1, train_loss=2.749376058578491\n",
      "Epoch=737, iteration=2, train_loss=2.7885684967041016\n",
      "Epoch=737, iteration=3, train_loss=2.917179822921753\n",
      "Epoch=737, val_loss=2.910696029663086\n",
      "Epoch=738, iteration=0, train_loss=2.7527740001678467\n",
      "Epoch=738, iteration=1, train_loss=2.7493746280670166\n",
      "Epoch=738, iteration=2, train_loss=2.7885677814483643\n",
      "Epoch=738, iteration=3, train_loss=2.917178153991699\n",
      "Epoch=738, val_loss=2.910696268081665\n",
      "Epoch=739, iteration=0, train_loss=2.7527718544006348\n",
      "Epoch=739, iteration=1, train_loss=2.749373197555542\n",
      "Epoch=739, iteration=2, train_loss=2.788567304611206\n",
      "Epoch=739, iteration=3, train_loss=2.917177200317383\n",
      "Epoch=739, val_loss=2.910696268081665\n",
      "Epoch=740, iteration=0, train_loss=2.752769947052002\n",
      "Epoch=740, iteration=1, train_loss=2.7493715286254883\n",
      "Epoch=740, iteration=2, train_loss=2.7885665893554688\n",
      "Epoch=740, iteration=3, train_loss=2.91717529296875\n",
      "Epoch=740, val_loss=2.910696506500244\n",
      "Epoch=741, iteration=0, train_loss=2.752767562866211\n",
      "Epoch=741, iteration=1, train_loss=2.7493703365325928\n",
      "Epoch=741, iteration=2, train_loss=2.7885665893554688\n",
      "Epoch=741, iteration=3, train_loss=2.9171743392944336\n",
      "Epoch=741, val_loss=2.9106969833374023\n",
      "Epoch=742, iteration=0, train_loss=2.752765417098999\n",
      "Epoch=742, iteration=1, train_loss=2.749368667602539\n",
      "Epoch=742, iteration=2, train_loss=2.7885658740997314\n",
      "Epoch=742, iteration=3, train_loss=2.91717267036438\n",
      "Epoch=742, val_loss=2.9106969833374023\n",
      "Epoch=743, iteration=0, train_loss=2.752763271331787\n",
      "Epoch=743, iteration=1, train_loss=2.7493672370910645\n",
      "Epoch=743, iteration=2, train_loss=2.7885653972625732\n",
      "Epoch=743, iteration=3, train_loss=2.917171001434326\n",
      "Epoch=743, val_loss=2.9106972217559814\n",
      "Epoch=744, iteration=0, train_loss=2.7527613639831543\n",
      "Epoch=744, iteration=1, train_loss=2.74936580657959\n",
      "Epoch=744, iteration=2, train_loss=2.788564682006836\n",
      "Epoch=744, iteration=3, train_loss=2.9171693325042725\n",
      "Epoch=744, val_loss=2.9106976985931396\n",
      "Epoch=745, iteration=0, train_loss=2.7527594566345215\n",
      "Epoch=745, iteration=1, train_loss=2.7493643760681152\n",
      "Epoch=745, iteration=2, train_loss=2.7885642051696777\n",
      "Epoch=745, iteration=3, train_loss=2.917168140411377\n",
      "Epoch=745, val_loss=2.9106976985931396\n",
      "Epoch=746, iteration=0, train_loss=2.7527568340301514\n",
      "Epoch=746, iteration=1, train_loss=2.7493629455566406\n",
      "Epoch=746, iteration=2, train_loss=2.7885634899139404\n",
      "Epoch=746, iteration=3, train_loss=2.9171669483184814\n",
      "Epoch=746, val_loss=2.9106976985931396\n",
      "Epoch=747, iteration=0, train_loss=2.7527549266815186\n",
      "Epoch=747, iteration=1, train_loss=2.749361515045166\n",
      "Epoch=747, iteration=2, train_loss=2.788562774658203\n",
      "Epoch=747, iteration=3, train_loss=2.917165517807007\n",
      "Epoch=747, val_loss=2.9106979370117188\n",
      "Epoch=748, iteration=0, train_loss=2.7527530193328857\n",
      "Epoch=748, iteration=1, train_loss=2.7493600845336914\n",
      "Epoch=748, iteration=2, train_loss=2.788562536239624\n",
      "Epoch=748, iteration=3, train_loss=2.9171640872955322\n",
      "Epoch=748, val_loss=2.910698413848877\n",
      "Epoch=749, iteration=0, train_loss=2.752751111984253\n",
      "Epoch=749, iteration=1, train_loss=2.7493581771850586\n",
      "Epoch=749, iteration=2, train_loss=2.7885618209838867\n",
      "Epoch=749, iteration=3, train_loss=2.9171624183654785\n",
      "Epoch=749, val_loss=2.910698413848877\n",
      "Epoch=750, iteration=0, train_loss=2.752748489379883\n",
      "Epoch=750, iteration=1, train_loss=2.749356746673584\n",
      "Epoch=750, iteration=2, train_loss=2.7885615825653076\n",
      "Epoch=750, iteration=3, train_loss=2.917160987854004\n",
      "Epoch=750, val_loss=2.910698413848877\n",
      "Epoch=751, iteration=0, train_loss=2.75274658203125\n",
      "Epoch=751, iteration=1, train_loss=2.7493557929992676\n",
      "Epoch=751, iteration=2, train_loss=2.7885608673095703\n",
      "Epoch=751, iteration=3, train_loss=2.9171595573425293\n",
      "Epoch=751, val_loss=2.9106991291046143\n",
      "Epoch=752, iteration=0, train_loss=2.752744674682617\n",
      "Epoch=752, iteration=1, train_loss=2.749354124069214\n",
      "Epoch=752, iteration=2, train_loss=2.788560152053833\n",
      "Epoch=752, iteration=3, train_loss=2.9171578884124756\n",
      "Epoch=752, val_loss=2.9106991291046143\n",
      "Epoch=753, iteration=0, train_loss=2.7527425289154053\n",
      "Epoch=753, iteration=1, train_loss=2.7493526935577393\n",
      "Epoch=753, iteration=2, train_loss=2.788559913635254\n",
      "Epoch=753, iteration=3, train_loss=2.91715669631958\n",
      "Epoch=753, val_loss=2.9106993675231934\n",
      "Epoch=754, iteration=0, train_loss=2.7527406215667725\n",
      "Epoch=754, iteration=1, train_loss=2.7493515014648438\n",
      "Epoch=754, iteration=2, train_loss=2.7885591983795166\n",
      "Epoch=754, iteration=3, train_loss=2.9171552658081055\n",
      "Epoch=754, val_loss=2.9106993675231934\n",
      "Epoch=755, iteration=0, train_loss=2.7527384757995605\n",
      "Epoch=755, iteration=1, train_loss=2.749350070953369\n",
      "Epoch=755, iteration=2, train_loss=2.7885587215423584\n",
      "Epoch=755, iteration=3, train_loss=2.9171535968780518\n",
      "Epoch=755, val_loss=2.9106996059417725\n",
      "Epoch=756, iteration=0, train_loss=2.7527363300323486\n",
      "Epoch=756, iteration=1, train_loss=2.7493486404418945\n",
      "Epoch=756, iteration=2, train_loss=2.7885582447052\n",
      "Epoch=756, iteration=3, train_loss=2.9171524047851562\n",
      "Epoch=756, val_loss=2.9106996059417725\n",
      "Epoch=757, iteration=0, train_loss=2.752734422683716\n",
      "Epoch=757, iteration=1, train_loss=2.7493467330932617\n",
      "Epoch=757, iteration=2, train_loss=2.788558006286621\n",
      "Epoch=757, iteration=3, train_loss=2.9171512126922607\n",
      "Epoch=757, val_loss=2.9107000827789307\n",
      "Epoch=758, iteration=0, train_loss=2.752732515335083\n",
      "Epoch=758, iteration=1, train_loss=2.7493460178375244\n",
      "Epoch=758, iteration=2, train_loss=2.788557291030884\n",
      "Epoch=758, iteration=3, train_loss=2.917149782180786\n",
      "Epoch=758, val_loss=2.9107000827789307\n",
      "Epoch=759, iteration=0, train_loss=2.752730369567871\n",
      "Epoch=759, iteration=1, train_loss=2.7493443489074707\n",
      "Epoch=759, iteration=2, train_loss=2.7885565757751465\n",
      "Epoch=759, iteration=3, train_loss=2.9171483516693115\n",
      "Epoch=759, val_loss=2.910700798034668\n",
      "Epoch=760, iteration=0, train_loss=2.7527284622192383\n",
      "Epoch=760, iteration=1, train_loss=2.749342918395996\n",
      "Epoch=760, iteration=2, train_loss=2.7885560989379883\n",
      "Epoch=760, iteration=3, train_loss=2.917146921157837\n",
      "Epoch=760, val_loss=2.910700798034668\n",
      "Epoch=761, iteration=0, train_loss=2.7527263164520264\n",
      "Epoch=761, iteration=1, train_loss=2.7493417263031006\n",
      "Epoch=761, iteration=2, train_loss=2.78855562210083\n",
      "Epoch=761, iteration=3, train_loss=2.917145252227783\n",
      "Epoch=761, val_loss=2.910701036453247\n",
      "Epoch=762, iteration=0, train_loss=2.7527241706848145\n",
      "Epoch=762, iteration=1, train_loss=2.749340057373047\n",
      "Epoch=762, iteration=2, train_loss=2.7885549068450928\n",
      "Epoch=762, iteration=3, train_loss=2.9171438217163086\n",
      "Epoch=762, val_loss=2.910701036453247\n",
      "Epoch=763, iteration=0, train_loss=2.7527222633361816\n",
      "Epoch=763, iteration=1, train_loss=2.7493388652801514\n",
      "Epoch=763, iteration=2, train_loss=2.7885546684265137\n",
      "Epoch=763, iteration=3, train_loss=2.917142629623413\n",
      "Epoch=763, val_loss=2.9107015132904053\n",
      "Epoch=764, iteration=0, train_loss=2.752720355987549\n",
      "Epoch=764, iteration=1, train_loss=2.7493371963500977\n",
      "Epoch=764, iteration=2, train_loss=2.7885541915893555\n",
      "Epoch=764, iteration=3, train_loss=2.9171411991119385\n",
      "Epoch=764, val_loss=2.9107017517089844\n",
      "Epoch=765, iteration=0, train_loss=2.752718210220337\n",
      "Epoch=765, iteration=1, train_loss=2.749335765838623\n",
      "Epoch=765, iteration=2, train_loss=2.7885537147521973\n",
      "Epoch=765, iteration=3, train_loss=2.917139768600464\n",
      "Epoch=765, val_loss=2.9107017517089844\n",
      "Epoch=766, iteration=0, train_loss=2.752716541290283\n",
      "Epoch=766, iteration=1, train_loss=2.7493345737457275\n",
      "Epoch=766, iteration=2, train_loss=2.78855299949646\n",
      "Epoch=766, iteration=3, train_loss=2.9171383380889893\n",
      "Epoch=766, val_loss=2.9107017517089844\n",
      "Epoch=767, iteration=0, train_loss=2.7527143955230713\n",
      "Epoch=767, iteration=1, train_loss=2.749332904815674\n",
      "Epoch=767, iteration=2, train_loss=2.7885525226593018\n",
      "Epoch=767, iteration=3, train_loss=2.9171371459960938\n",
      "Epoch=767, val_loss=2.9107019901275635\n",
      "Epoch=768, iteration=0, train_loss=2.7527120113372803\n",
      "Epoch=768, iteration=1, train_loss=2.7493317127227783\n",
      "Epoch=768, iteration=2, train_loss=2.7885518074035645\n",
      "Epoch=768, iteration=3, train_loss=2.917135238647461\n",
      "Epoch=768, val_loss=2.910702705383301\n",
      "Epoch=769, iteration=0, train_loss=2.7527103424072266\n",
      "Epoch=769, iteration=1, train_loss=2.749330520629883\n",
      "Epoch=769, iteration=2, train_loss=2.788551092147827\n",
      "Epoch=769, iteration=3, train_loss=2.9171340465545654\n",
      "Epoch=769, val_loss=2.9107024669647217\n",
      "Epoch=770, iteration=0, train_loss=2.7527084350585938\n",
      "Epoch=770, iteration=1, train_loss=2.749329090118408\n",
      "Epoch=770, iteration=2, train_loss=2.788550853729248\n",
      "Epoch=770, iteration=3, train_loss=2.917132616043091\n",
      "Epoch=770, val_loss=2.9107024669647217\n",
      "Epoch=771, iteration=0, train_loss=2.752706527709961\n",
      "Epoch=771, iteration=1, train_loss=2.7493271827697754\n",
      "Epoch=771, iteration=2, train_loss=2.78855037689209\n",
      "Epoch=771, iteration=3, train_loss=2.9171316623687744\n",
      "Epoch=771, val_loss=2.910702705383301\n",
      "Epoch=772, iteration=0, train_loss=2.752704620361328\n",
      "Epoch=772, iteration=1, train_loss=2.749326229095459\n",
      "Epoch=772, iteration=2, train_loss=2.7885499000549316\n",
      "Epoch=772, iteration=3, train_loss=2.9171297550201416\n",
      "Epoch=772, val_loss=2.910703182220459\n",
      "Epoch=773, iteration=0, train_loss=2.752702236175537\n",
      "Epoch=773, iteration=1, train_loss=2.7493250370025635\n",
      "Epoch=773, iteration=2, train_loss=2.7885491847991943\n",
      "Epoch=773, iteration=3, train_loss=2.917128801345825\n",
      "Epoch=773, val_loss=2.9107038974761963\n",
      "Epoch=774, iteration=0, train_loss=2.7527005672454834\n",
      "Epoch=774, iteration=1, train_loss=2.7493233680725098\n",
      "Epoch=774, iteration=2, train_loss=2.7885489463806152\n",
      "Epoch=774, iteration=3, train_loss=2.917127847671509\n",
      "Epoch=774, val_loss=2.9107041358947754\n",
      "Epoch=775, iteration=0, train_loss=2.7526986598968506\n",
      "Epoch=775, iteration=1, train_loss=2.749321937561035\n",
      "Epoch=775, iteration=2, train_loss=2.788548231124878\n",
      "Epoch=775, iteration=3, train_loss=2.917125701904297\n",
      "Epoch=775, val_loss=2.9107041358947754\n",
      "Epoch=776, iteration=0, train_loss=2.7526967525482178\n",
      "Epoch=776, iteration=1, train_loss=2.7493205070495605\n",
      "Epoch=776, iteration=2, train_loss=2.788548231124878\n",
      "Epoch=776, iteration=3, train_loss=2.9171247482299805\n",
      "Epoch=776, val_loss=2.9107046127319336\n",
      "Epoch=777, iteration=0, train_loss=2.752695083618164\n",
      "Epoch=777, iteration=1, train_loss=2.749319553375244\n",
      "Epoch=777, iteration=2, train_loss=2.7885472774505615\n",
      "Epoch=777, iteration=3, train_loss=2.9171230792999268\n",
      "Epoch=777, val_loss=2.9107046127319336\n",
      "Epoch=778, iteration=0, train_loss=2.752692937850952\n",
      "Epoch=778, iteration=1, train_loss=2.7493178844451904\n",
      "Epoch=778, iteration=2, train_loss=2.7885468006134033\n",
      "Epoch=778, iteration=3, train_loss=2.917121648788452\n",
      "Epoch=778, val_loss=2.910705089569092\n",
      "Epoch=779, iteration=0, train_loss=2.7526910305023193\n",
      "Epoch=779, iteration=1, train_loss=2.749316692352295\n",
      "Epoch=779, iteration=2, train_loss=2.788546562194824\n",
      "Epoch=779, iteration=3, train_loss=2.9171206951141357\n",
      "Epoch=779, val_loss=2.910705089569092\n",
      "Epoch=780, iteration=0, train_loss=2.7526888847351074\n",
      "Epoch=780, iteration=1, train_loss=2.7493152618408203\n",
      "Epoch=780, iteration=2, train_loss=2.788545846939087\n",
      "Epoch=780, iteration=3, train_loss=2.917119264602661\n",
      "Epoch=780, val_loss=2.910705089569092\n",
      "Epoch=781, iteration=0, train_loss=2.7526869773864746\n",
      "Epoch=781, iteration=1, train_loss=2.749314069747925\n",
      "Epoch=781, iteration=2, train_loss=2.7885451316833496\n",
      "Epoch=781, iteration=3, train_loss=2.9171178340911865\n",
      "Epoch=781, val_loss=2.91070556640625\n",
      "Epoch=782, iteration=0, train_loss=2.752685070037842\n",
      "Epoch=782, iteration=1, train_loss=2.74931263923645\n",
      "Epoch=782, iteration=2, train_loss=2.7885451316833496\n",
      "Epoch=782, iteration=3, train_loss=2.917116403579712\n",
      "Epoch=782, val_loss=2.910705804824829\n",
      "Epoch=783, iteration=0, train_loss=2.752683162689209\n",
      "Epoch=783, iteration=1, train_loss=2.7493114471435547\n",
      "Epoch=783, iteration=2, train_loss=2.788544178009033\n",
      "Epoch=783, iteration=3, train_loss=2.9171154499053955\n",
      "Epoch=783, val_loss=2.9107062816619873\n",
      "Epoch=784, iteration=0, train_loss=2.752681016921997\n",
      "Epoch=784, iteration=1, train_loss=2.74931001663208\n",
      "Epoch=784, iteration=2, train_loss=2.788543939590454\n",
      "Epoch=784, iteration=3, train_loss=2.917113780975342\n",
      "Epoch=784, val_loss=2.9107065200805664\n",
      "Epoch=785, iteration=0, train_loss=2.7526791095733643\n",
      "Epoch=785, iteration=1, train_loss=2.7493083477020264\n",
      "Epoch=785, iteration=2, train_loss=2.788543462753296\n",
      "Epoch=785, iteration=3, train_loss=2.9171128273010254\n",
      "Epoch=785, val_loss=2.9107065200805664\n",
      "Epoch=786, iteration=0, train_loss=2.7526774406433105\n",
      "Epoch=786, iteration=1, train_loss=2.749307155609131\n",
      "Epoch=786, iteration=2, train_loss=2.7885429859161377\n",
      "Epoch=786, iteration=3, train_loss=2.9171106815338135\n",
      "Epoch=786, val_loss=2.9107069969177246\n",
      "Epoch=787, iteration=0, train_loss=2.752676010131836\n",
      "Epoch=787, iteration=1, train_loss=2.7493059635162354\n",
      "Epoch=787, iteration=2, train_loss=2.7885422706604004\n",
      "Epoch=787, iteration=3, train_loss=2.917109727859497\n",
      "Epoch=787, val_loss=2.9107072353363037\n",
      "Epoch=788, iteration=0, train_loss=2.752674102783203\n",
      "Epoch=788, iteration=1, train_loss=2.74930477142334\n",
      "Epoch=788, iteration=2, train_loss=2.788541555404663\n",
      "Epoch=788, iteration=3, train_loss=2.9171082973480225\n",
      "Epoch=788, val_loss=2.9107072353363037\n",
      "Epoch=789, iteration=0, train_loss=2.752671480178833\n",
      "Epoch=789, iteration=1, train_loss=2.749303102493286\n",
      "Epoch=789, iteration=2, train_loss=2.788541555404663\n",
      "Epoch=789, iteration=3, train_loss=2.917106866836548\n",
      "Epoch=789, val_loss=2.910707473754883\n",
      "Epoch=790, iteration=0, train_loss=2.7526698112487793\n",
      "Epoch=790, iteration=1, train_loss=2.7493021488189697\n",
      "Epoch=790, iteration=2, train_loss=2.788541078567505\n",
      "Epoch=790, iteration=3, train_loss=2.9171059131622314\n",
      "Epoch=790, val_loss=2.910707950592041\n",
      "Epoch=791, iteration=0, train_loss=2.7526679039001465\n",
      "Epoch=791, iteration=1, train_loss=2.749300718307495\n",
      "Epoch=791, iteration=2, train_loss=2.7885406017303467\n",
      "Epoch=791, iteration=3, train_loss=2.9171042442321777\n",
      "Epoch=791, val_loss=2.910707950592041\n",
      "Epoch=792, iteration=0, train_loss=2.7526659965515137\n",
      "Epoch=792, iteration=1, train_loss=2.7492992877960205\n",
      "Epoch=792, iteration=2, train_loss=2.7885398864746094\n",
      "Epoch=792, iteration=3, train_loss=2.9171030521392822\n",
      "Epoch=792, val_loss=2.91070818901062\n",
      "Epoch=793, iteration=0, train_loss=2.75266432762146\n",
      "Epoch=793, iteration=1, train_loss=2.749298095703125\n",
      "Epoch=793, iteration=2, train_loss=2.7885396480560303\n",
      "Epoch=793, iteration=3, train_loss=2.9171013832092285\n",
      "Epoch=793, val_loss=2.91070818901062\n",
      "Epoch=794, iteration=0, train_loss=2.752662420272827\n",
      "Epoch=794, iteration=1, train_loss=2.7492964267730713\n",
      "Epoch=794, iteration=2, train_loss=2.788538932800293\n",
      "Epoch=794, iteration=3, train_loss=2.917100429534912\n",
      "Epoch=794, val_loss=2.9107086658477783\n",
      "Epoch=795, iteration=0, train_loss=2.7526605129241943\n",
      "Epoch=795, iteration=1, train_loss=2.749295234680176\n",
      "Epoch=795, iteration=2, train_loss=2.7885384559631348\n",
      "Epoch=795, iteration=3, train_loss=2.9170987606048584\n",
      "Epoch=795, val_loss=2.9107089042663574\n",
      "Epoch=796, iteration=0, train_loss=2.7526586055755615\n",
      "Epoch=796, iteration=1, train_loss=2.7492942810058594\n",
      "Epoch=796, iteration=2, train_loss=2.7885379791259766\n",
      "Epoch=796, iteration=3, train_loss=2.917097568511963\n",
      "Epoch=796, val_loss=2.9107089042663574\n",
      "Epoch=797, iteration=0, train_loss=2.7526566982269287\n",
      "Epoch=797, iteration=1, train_loss=2.7492926120758057\n",
      "Epoch=797, iteration=2, train_loss=2.7885377407073975\n",
      "Epoch=797, iteration=3, train_loss=2.9170963764190674\n",
      "Epoch=797, val_loss=2.9107093811035156\n",
      "Epoch=798, iteration=0, train_loss=2.752655267715454\n",
      "Epoch=798, iteration=1, train_loss=2.74929141998291\n",
      "Epoch=798, iteration=2, train_loss=2.78853702545166\n",
      "Epoch=798, iteration=3, train_loss=2.917095184326172\n",
      "Epoch=798, val_loss=2.9107096195220947\n",
      "Epoch=799, iteration=0, train_loss=2.752652883529663\n",
      "Epoch=799, iteration=1, train_loss=2.7492902278900146\n",
      "Epoch=799, iteration=2, train_loss=2.788536548614502\n",
      "Epoch=799, iteration=3, train_loss=2.917093515396118\n",
      "Epoch=799, val_loss=2.910710096359253\n",
      "Epoch=800, iteration=0, train_loss=2.7526512145996094\n",
      "Epoch=800, iteration=1, train_loss=2.74928879737854\n",
      "Epoch=800, iteration=2, train_loss=2.7885360717773438\n",
      "Epoch=800, iteration=3, train_loss=2.9170920848846436\n",
      "Epoch=800, val_loss=2.910710096359253\n",
      "Epoch=801, iteration=0, train_loss=2.7526495456695557\n",
      "Epoch=801, iteration=1, train_loss=2.7492876052856445\n",
      "Epoch=801, iteration=2, train_loss=2.7885360717773438\n",
      "Epoch=801, iteration=3, train_loss=2.917091131210327\n",
      "Epoch=801, val_loss=2.910710334777832\n",
      "Epoch=802, iteration=0, train_loss=2.752647638320923\n",
      "Epoch=802, iteration=1, train_loss=2.752633571624756\n",
      "Epoch=802, iteration=2, train_loss=2.790339231491089\n",
      "Epoch=802, iteration=3, train_loss=2.909700393676758\n",
      "Epoch=802, val_loss=2.910531997680664\n",
      "Epoch=803, iteration=0, train_loss=2.7503912448883057\n",
      "Epoch=803, iteration=1, train_loss=2.750869035720825\n",
      "Epoch=803, iteration=2, train_loss=2.7894766330718994\n",
      "Epoch=803, iteration=3, train_loss=2.9102389812469482\n",
      "Epoch=803, val_loss=2.9108893871307373\n",
      "Epoch=804, iteration=0, train_loss=2.749490737915039\n",
      "Epoch=804, iteration=1, train_loss=2.750084161758423\n",
      "Epoch=804, iteration=2, train_loss=2.7891621589660645\n",
      "Epoch=804, iteration=3, train_loss=2.9107725620269775\n",
      "Epoch=804, val_loss=2.911254405975342\n",
      "Epoch=805, iteration=0, train_loss=2.7490434646606445\n",
      "Epoch=805, iteration=1, train_loss=2.7496440410614014\n",
      "Epoch=805, iteration=2, train_loss=2.7890117168426514\n",
      "Epoch=805, iteration=3, train_loss=2.91119647026062\n",
      "Epoch=805, val_loss=2.9115500450134277\n",
      "Epoch=806, iteration=0, train_loss=2.7487781047821045\n",
      "Epoch=806, iteration=1, train_loss=2.749354362487793\n",
      "Epoch=806, iteration=2, train_loss=2.788922071456909\n",
      "Epoch=806, iteration=3, train_loss=2.9115254878997803\n",
      "Epoch=806, val_loss=2.9117846488952637\n",
      "Epoch=807, iteration=0, train_loss=2.7485995292663574\n",
      "Epoch=807, iteration=1, train_loss=2.7491443157196045\n",
      "Epoch=807, iteration=2, train_loss=2.788860559463501\n",
      "Epoch=807, iteration=3, train_loss=2.911787986755371\n",
      "Epoch=807, val_loss=2.911975383758545\n",
      "Epoch=808, iteration=0, train_loss=2.748469591140747\n",
      "Epoch=808, iteration=1, train_loss=2.7489840984344482\n",
      "Epoch=808, iteration=2, train_loss=2.788815498352051\n",
      "Epoch=808, iteration=3, train_loss=2.912001848220825\n",
      "Epoch=808, val_loss=2.9121346473693848\n",
      "Epoch=809, iteration=0, train_loss=2.7483692169189453\n",
      "Epoch=809, iteration=1, train_loss=2.748857021331787\n",
      "Epoch=809, iteration=2, train_loss=2.7887816429138184\n",
      "Epoch=809, iteration=3, train_loss=2.9121806621551514\n",
      "Epoch=809, val_loss=2.9122700691223145\n",
      "Epoch=810, iteration=0, train_loss=2.748288631439209\n",
      "Epoch=810, iteration=1, train_loss=2.7487545013427734\n",
      "Epoch=810, iteration=2, train_loss=2.7887539863586426\n",
      "Epoch=810, iteration=3, train_loss=2.91233229637146\n",
      "Epoch=810, val_loss=2.9123878479003906\n",
      "Epoch=811, iteration=0, train_loss=2.7482216358184814\n",
      "Epoch=811, iteration=1, train_loss=2.7486684322357178\n",
      "Epoch=811, iteration=2, train_loss=2.7887322902679443\n",
      "Epoch=811, iteration=3, train_loss=2.912463903427124\n",
      "Epoch=811, val_loss=2.9124913215637207\n",
      "Epoch=812, iteration=0, train_loss=2.748164653778076\n",
      "Epoch=812, iteration=1, train_loss=2.748596429824829\n",
      "Epoch=812, iteration=2, train_loss=2.7887141704559326\n",
      "Epoch=812, iteration=3, train_loss=2.9125778675079346\n",
      "Epoch=812, val_loss=2.912583112716675\n",
      "Epoch=813, iteration=0, train_loss=2.748115301132202\n",
      "Epoch=813, iteration=1, train_loss=2.74853515625\n",
      "Epoch=813, iteration=2, train_loss=2.7886993885040283\n",
      "Epoch=813, iteration=3, train_loss=2.9126789569854736\n",
      "Epoch=813, val_loss=2.912665605545044\n",
      "Epoch=814, iteration=0, train_loss=2.7480721473693848\n",
      "Epoch=814, iteration=1, train_loss=2.7484822273254395\n",
      "Epoch=814, iteration=2, train_loss=2.7886862754821777\n",
      "Epoch=814, iteration=3, train_loss=2.9127678871154785\n",
      "Epoch=814, val_loss=2.9127399921417236\n",
      "Epoch=815, iteration=0, train_loss=2.748033285140991\n",
      "Epoch=815, iteration=1, train_loss=2.7484354972839355\n",
      "Epoch=815, iteration=2, train_loss=2.788675308227539\n",
      "Epoch=815, iteration=3, train_loss=2.9128479957580566\n",
      "Epoch=815, val_loss=2.912806987762451\n",
      "Epoch=816, iteration=0, train_loss=2.7479987144470215\n",
      "Epoch=816, iteration=1, train_loss=2.7483949661254883\n",
      "Epoch=816, iteration=2, train_loss=2.788665294647217\n",
      "Epoch=816, iteration=3, train_loss=2.9129199981689453\n",
      "Epoch=816, val_loss=2.9128687381744385\n",
      "Epoch=817, iteration=0, train_loss=2.747967004776001\n",
      "Epoch=817, iteration=1, train_loss=2.7483584880828857\n",
      "Epoch=817, iteration=2, train_loss=2.78865647315979\n",
      "Epoch=817, iteration=3, train_loss=2.91298508644104\n",
      "Epoch=817, val_loss=2.9129250049591064\n",
      "Epoch=818, iteration=0, train_loss=2.7479379177093506\n",
      "Epoch=818, iteration=1, train_loss=2.748326063156128\n",
      "Epoch=818, iteration=2, train_loss=2.7886481285095215\n",
      "Epoch=818, iteration=3, train_loss=2.9130446910858154\n",
      "Epoch=818, val_loss=2.9129772186279297\n",
      "Epoch=819, iteration=0, train_loss=2.747911214828491\n",
      "Epoch=819, iteration=1, train_loss=2.7482969760894775\n",
      "Epoch=819, iteration=2, train_loss=2.7886412143707275\n",
      "Epoch=819, iteration=3, train_loss=2.9130992889404297\n",
      "Epoch=819, val_loss=2.913025140762329\n",
      "Epoch=820, iteration=0, train_loss=2.7478866577148438\n",
      "Epoch=820, iteration=1, train_loss=2.748270273208618\n",
      "Epoch=820, iteration=2, train_loss=2.7886345386505127\n",
      "Epoch=820, iteration=3, train_loss=2.913149356842041\n",
      "Epoch=820, val_loss=2.9130704402923584\n",
      "Epoch=821, iteration=0, train_loss=2.74786376953125\n",
      "Epoch=821, iteration=1, train_loss=2.74824595451355\n",
      "Epoch=821, iteration=2, train_loss=2.788628578186035\n",
      "Epoch=821, iteration=3, train_loss=2.9131956100463867\n",
      "Epoch=821, val_loss=2.913112163543701\n",
      "Epoch=822, iteration=0, train_loss=2.7478420734405518\n",
      "Epoch=822, iteration=1, train_loss=2.7482235431671143\n",
      "Epoch=822, iteration=2, train_loss=2.7886228561401367\n",
      "Epoch=822, iteration=3, train_loss=2.9132394790649414\n",
      "Epoch=822, val_loss=2.913151979446411\n",
      "Epoch=823, iteration=0, train_loss=2.7478225231170654\n",
      "Epoch=823, iteration=1, train_loss=2.7482030391693115\n",
      "Epoch=823, iteration=2, train_loss=2.7886176109313965\n",
      "Epoch=823, iteration=3, train_loss=2.9132797718048096\n",
      "Epoch=823, val_loss=2.913188934326172\n",
      "Epoch=824, iteration=0, train_loss=2.7478036880493164\n",
      "Epoch=824, iteration=1, train_loss=2.7481839656829834\n",
      "Epoch=824, iteration=2, train_loss=2.7886126041412354\n",
      "Epoch=824, iteration=3, train_loss=2.913318157196045\n",
      "Epoch=824, val_loss=2.913224220275879\n",
      "Epoch=825, iteration=0, train_loss=2.747786045074463\n",
      "Epoch=825, iteration=1, train_loss=2.7481658458709717\n",
      "Epoch=825, iteration=2, train_loss=2.7886078357696533\n",
      "Epoch=825, iteration=3, train_loss=2.9133541584014893\n",
      "Epoch=825, val_loss=2.913257122039795\n",
      "Epoch=826, iteration=0, train_loss=2.7477691173553467\n",
      "Epoch=826, iteration=1, train_loss=2.7481493949890137\n",
      "Epoch=826, iteration=2, train_loss=2.7886030673980713\n",
      "Epoch=826, iteration=3, train_loss=2.9133880138397217\n",
      "Epoch=826, val_loss=2.9132888317108154\n",
      "Epoch=827, iteration=0, train_loss=2.747753381729126\n",
      "Epoch=827, iteration=1, train_loss=2.748134136199951\n",
      "Epoch=827, iteration=2, train_loss=2.7885992527008057\n",
      "Epoch=827, iteration=3, train_loss=2.913419485092163\n",
      "Epoch=827, val_loss=2.913318395614624\n",
      "Epoch=828, iteration=0, train_loss=2.7477383613586426\n",
      "Epoch=828, iteration=1, train_loss=2.7481191158294678\n",
      "Epoch=828, iteration=2, train_loss=2.78859543800354\n",
      "Epoch=828, iteration=3, train_loss=2.9134507179260254\n",
      "Epoch=828, val_loss=2.9133472442626953\n",
      "Epoch=829, iteration=0, train_loss=2.7477240562438965\n",
      "Epoch=829, iteration=1, train_loss=2.748105049133301\n",
      "Epoch=829, iteration=2, train_loss=2.7885916233062744\n",
      "Epoch=829, iteration=3, train_loss=2.9134795665740967\n",
      "Epoch=829, val_loss=2.9133739471435547\n",
      "Epoch=830, iteration=0, train_loss=2.7477104663848877\n",
      "Epoch=830, iteration=1, train_loss=2.74809193611145\n",
      "Epoch=830, iteration=2, train_loss=2.788588047027588\n",
      "Epoch=830, iteration=3, train_loss=2.913506507873535\n",
      "Epoch=830, val_loss=2.9133999347686768\n",
      "Epoch=831, iteration=0, train_loss=2.747697353363037\n",
      "Epoch=831, iteration=1, train_loss=2.748079538345337\n",
      "Epoch=831, iteration=2, train_loss=2.7885847091674805\n",
      "Epoch=831, iteration=3, train_loss=2.9135324954986572\n",
      "Epoch=831, val_loss=2.913424491882324\n",
      "Epoch=832, iteration=0, train_loss=2.747685194015503\n",
      "Epoch=832, iteration=1, train_loss=2.748067855834961\n",
      "Epoch=832, iteration=2, train_loss=2.788581609725952\n",
      "Epoch=832, iteration=3, train_loss=2.913558006286621\n",
      "Epoch=832, val_loss=2.9134480953216553\n",
      "Epoch=833, iteration=0, train_loss=2.747673511505127\n",
      "Epoch=833, iteration=1, train_loss=2.748056411743164\n",
      "Epoch=833, iteration=2, train_loss=2.788578748703003\n",
      "Epoch=833, iteration=3, train_loss=2.9135818481445312\n",
      "Epoch=833, val_loss=2.913470506668091\n",
      "Epoch=834, iteration=0, train_loss=2.747662305831909\n",
      "Epoch=834, iteration=1, train_loss=2.7480456829071045\n",
      "Epoch=834, iteration=2, train_loss=2.7885756492614746\n",
      "Epoch=834, iteration=3, train_loss=2.913604497909546\n",
      "Epoch=834, val_loss=2.91349196434021\n",
      "Epoch=835, iteration=0, train_loss=2.7476515769958496\n",
      "Epoch=835, iteration=1, train_loss=2.7480356693267822\n",
      "Epoch=835, iteration=2, train_loss=2.7885727882385254\n",
      "Epoch=835, iteration=3, train_loss=2.9136266708374023\n",
      "Epoch=835, val_loss=2.913512945175171\n",
      "Epoch=836, iteration=0, train_loss=2.7476413249969482\n",
      "Epoch=836, iteration=1, train_loss=2.748025894165039\n",
      "Epoch=836, iteration=2, train_loss=2.788569927215576\n",
      "Epoch=836, iteration=3, train_loss=2.913647413253784\n",
      "Epoch=836, val_loss=2.9135324954986572\n",
      "Epoch=837, iteration=0, train_loss=2.747631311416626\n",
      "Epoch=837, iteration=1, train_loss=2.748016834259033\n",
      "Epoch=837, iteration=2, train_loss=2.788567066192627\n",
      "Epoch=837, iteration=3, train_loss=2.9136672019958496\n",
      "Epoch=837, val_loss=2.9135515689849854\n",
      "Epoch=838, iteration=0, train_loss=2.74762225151062\n",
      "Epoch=838, iteration=1, train_loss=2.7480077743530273\n",
      "Epoch=838, iteration=2, train_loss=2.788565158843994\n",
      "Epoch=838, iteration=3, train_loss=2.9136862754821777\n",
      "Epoch=838, val_loss=2.913569688796997\n",
      "Epoch=839, iteration=0, train_loss=2.747612953186035\n",
      "Epoch=839, iteration=1, train_loss=2.747999429702759\n",
      "Epoch=839, iteration=2, train_loss=2.788562536239624\n",
      "Epoch=839, iteration=3, train_loss=2.9137048721313477\n",
      "Epoch=839, val_loss=2.9135870933532715\n",
      "Epoch=840, iteration=0, train_loss=2.7476043701171875\n",
      "Epoch=840, iteration=1, train_loss=2.7479913234710693\n",
      "Epoch=840, iteration=2, train_loss=2.788560390472412\n",
      "Epoch=840, iteration=3, train_loss=2.913722276687622\n",
      "Epoch=840, val_loss=2.9136035442352295\n",
      "Epoch=841, iteration=0, train_loss=2.747596025466919\n",
      "Epoch=841, iteration=1, train_loss=2.747983455657959\n",
      "Epoch=841, iteration=2, train_loss=2.7885582447052\n",
      "Epoch=841, iteration=3, train_loss=2.9137394428253174\n",
      "Epoch=841, val_loss=2.9136197566986084\n",
      "Epoch=842, iteration=0, train_loss=2.7475883960723877\n",
      "Epoch=842, iteration=1, train_loss=2.747976303100586\n",
      "Epoch=842, iteration=2, train_loss=2.78855562210083\n",
      "Epoch=842, iteration=3, train_loss=2.913755416870117\n",
      "Epoch=842, val_loss=2.913635492324829\n",
      "Epoch=843, iteration=0, train_loss=2.7475807666778564\n",
      "Epoch=843, iteration=1, train_loss=2.747968912124634\n",
      "Epoch=843, iteration=2, train_loss=2.7885539531707764\n",
      "Epoch=843, iteration=3, train_loss=2.913771152496338\n",
      "Epoch=843, val_loss=2.9136502742767334\n",
      "Epoch=844, iteration=0, train_loss=2.7475736141204834\n",
      "Epoch=844, iteration=1, train_loss=2.747962236404419\n",
      "Epoch=844, iteration=2, train_loss=2.7885520458221436\n",
      "Epoch=844, iteration=3, train_loss=2.9137864112854004\n",
      "Epoch=844, val_loss=2.9136643409729004\n",
      "Epoch=845, iteration=0, train_loss=2.7475664615631104\n",
      "Epoch=845, iteration=1, train_loss=2.747955560684204\n",
      "Epoch=845, iteration=2, train_loss=2.7885501384735107\n",
      "Epoch=845, iteration=3, train_loss=2.9138007164001465\n",
      "Epoch=845, val_loss=2.913677930831909\n",
      "Epoch=846, iteration=0, train_loss=2.7475597858428955\n",
      "Epoch=846, iteration=1, train_loss=2.7479493618011475\n",
      "Epoch=846, iteration=2, train_loss=2.788548231124878\n",
      "Epoch=846, iteration=3, train_loss=2.9138143062591553\n",
      "Epoch=846, val_loss=2.913691520690918\n",
      "Epoch=847, iteration=0, train_loss=2.7475533485412598\n",
      "Epoch=847, iteration=1, train_loss=2.747943162918091\n",
      "Epoch=847, iteration=2, train_loss=2.788546562194824\n",
      "Epoch=847, iteration=3, train_loss=2.913827657699585\n",
      "Epoch=847, val_loss=2.9137039184570312\n",
      "Epoch=848, iteration=0, train_loss=2.747546911239624\n",
      "Epoch=848, iteration=1, train_loss=2.7479376792907715\n",
      "Epoch=848, iteration=2, train_loss=2.7885451316833496\n",
      "Epoch=848, iteration=3, train_loss=2.9138405323028564\n",
      "Epoch=848, val_loss=2.9137158393859863\n",
      "Epoch=849, iteration=0, train_loss=2.7475411891937256\n",
      "Epoch=849, iteration=1, train_loss=2.747932195663452\n",
      "Epoch=849, iteration=2, train_loss=2.788543462753296\n",
      "Epoch=849, iteration=3, train_loss=2.913853168487549\n",
      "Epoch=849, val_loss=2.9137279987335205\n",
      "Epoch=850, iteration=0, train_loss=2.747535467147827\n",
      "Epoch=850, iteration=1, train_loss=2.7479264736175537\n",
      "Epoch=850, iteration=2, train_loss=2.788541793823242\n",
      "Epoch=850, iteration=3, train_loss=2.9138643741607666\n",
      "Epoch=850, val_loss=2.9137392044067383\n",
      "Epoch=851, iteration=0, train_loss=2.7475297451019287\n",
      "Epoch=851, iteration=1, train_loss=2.7479217052459717\n",
      "Epoch=851, iteration=2, train_loss=2.7885403633117676\n",
      "Epoch=851, iteration=3, train_loss=2.9138762950897217\n",
      "Epoch=851, val_loss=2.913750171661377\n",
      "Epoch=852, iteration=0, train_loss=2.7475247383117676\n",
      "Epoch=852, iteration=1, train_loss=2.7479166984558105\n",
      "Epoch=852, iteration=2, train_loss=2.788538932800293\n",
      "Epoch=852, iteration=3, train_loss=2.9138870239257812\n",
      "Epoch=852, val_loss=2.9137604236602783\n",
      "Epoch=853, iteration=0, train_loss=2.7475194931030273\n",
      "Epoch=853, iteration=1, train_loss=2.7479119300842285\n",
      "Epoch=853, iteration=2, train_loss=2.7885379791259766\n",
      "Epoch=853, iteration=3, train_loss=2.9138972759246826\n",
      "Epoch=853, val_loss=2.9137704372406006\n",
      "Epoch=854, iteration=0, train_loss=2.7475147247314453\n",
      "Epoch=854, iteration=1, train_loss=2.7479074001312256\n",
      "Epoch=854, iteration=2, train_loss=2.7885360717773438\n",
      "Epoch=854, iteration=3, train_loss=2.913908004760742\n",
      "Epoch=854, val_loss=2.9137802124023438\n",
      "Epoch=855, iteration=0, train_loss=2.7475101947784424\n",
      "Epoch=855, iteration=1, train_loss=2.747903347015381\n",
      "Epoch=855, iteration=2, train_loss=2.7885348796844482\n",
      "Epoch=855, iteration=3, train_loss=2.9139177799224854\n",
      "Epoch=855, val_loss=2.9137892723083496\n",
      "Epoch=856, iteration=0, train_loss=2.7475054264068604\n",
      "Epoch=856, iteration=1, train_loss=2.747898817062378\n",
      "Epoch=856, iteration=2, train_loss=2.7885336875915527\n",
      "Epoch=856, iteration=3, train_loss=2.9139270782470703\n",
      "Epoch=856, val_loss=2.9137988090515137\n",
      "Epoch=857, iteration=0, train_loss=2.7475011348724365\n",
      "Epoch=857, iteration=1, train_loss=2.747894763946533\n",
      "Epoch=857, iteration=2, train_loss=2.7885324954986572\n",
      "Epoch=857, iteration=3, train_loss=2.913935899734497\n",
      "Epoch=857, val_loss=2.9138071537017822\n",
      "Epoch=858, iteration=0, train_loss=2.747497081756592\n",
      "Epoch=858, iteration=1, train_loss=2.7478911876678467\n",
      "Epoch=858, iteration=2, train_loss=2.7885313034057617\n",
      "Epoch=858, iteration=3, train_loss=2.913945198059082\n",
      "Epoch=858, val_loss=2.9138152599334717\n",
      "Epoch=859, iteration=0, train_loss=2.747493028640747\n",
      "Epoch=859, iteration=1, train_loss=2.747886896133423\n",
      "Epoch=859, iteration=2, train_loss=2.7885303497314453\n",
      "Epoch=859, iteration=3, train_loss=2.9139535427093506\n",
      "Epoch=859, val_loss=2.913823366165161\n",
      "Epoch=860, iteration=0, train_loss=2.7474889755249023\n",
      "Epoch=860, iteration=1, train_loss=2.7478833198547363\n",
      "Epoch=860, iteration=2, train_loss=2.788529396057129\n",
      "Epoch=860, iteration=3, train_loss=2.913961410522461\n",
      "Epoch=860, val_loss=2.9138309955596924\n",
      "Epoch=861, iteration=0, train_loss=2.747485637664795\n",
      "Epoch=861, iteration=1, train_loss=2.747879981994629\n",
      "Epoch=861, iteration=2, train_loss=2.7885282039642334\n",
      "Epoch=861, iteration=3, train_loss=2.9139692783355713\n",
      "Epoch=861, val_loss=2.9138386249542236\n",
      "Epoch=862, iteration=0, train_loss=2.7474818229675293\n",
      "Epoch=862, iteration=1, train_loss=2.7478766441345215\n",
      "Epoch=862, iteration=2, train_loss=2.788527011871338\n",
      "Epoch=862, iteration=3, train_loss=2.9139771461486816\n",
      "Epoch=862, val_loss=2.913846015930176\n",
      "Epoch=863, iteration=0, train_loss=2.747478485107422\n",
      "Epoch=863, iteration=1, train_loss=2.747873306274414\n",
      "Epoch=863, iteration=2, train_loss=2.7885260581970215\n",
      "Epoch=863, iteration=3, train_loss=2.9139842987060547\n",
      "Epoch=863, val_loss=2.9138526916503906\n",
      "Epoch=864, iteration=0, train_loss=2.7474753856658936\n",
      "Epoch=864, iteration=1, train_loss=2.747870445251465\n",
      "Epoch=864, iteration=2, train_loss=2.788525342941284\n",
      "Epoch=864, iteration=3, train_loss=2.9139912128448486\n",
      "Epoch=864, val_loss=2.9138591289520264\n",
      "Epoch=865, iteration=0, train_loss=2.747471809387207\n",
      "Epoch=865, iteration=1, train_loss=2.7478673458099365\n",
      "Epoch=865, iteration=2, train_loss=2.788524627685547\n",
      "Epoch=865, iteration=3, train_loss=2.9139978885650635\n",
      "Epoch=865, val_loss=2.9138660430908203\n",
      "Epoch=866, iteration=0, train_loss=2.7474687099456787\n",
      "Epoch=866, iteration=1, train_loss=2.7478644847869873\n",
      "Epoch=866, iteration=2, train_loss=2.7885234355926514\n",
      "Epoch=866, iteration=3, train_loss=2.9140045642852783\n",
      "Epoch=866, val_loss=2.913872003555298\n",
      "Epoch=867, iteration=0, train_loss=2.7474656105041504\n",
      "Epoch=867, iteration=1, train_loss=2.747861385345459\n",
      "Epoch=867, iteration=2, train_loss=2.788522958755493\n",
      "Epoch=867, iteration=3, train_loss=2.914010524749756\n",
      "Epoch=867, val_loss=2.9138782024383545\n",
      "Epoch=868, iteration=0, train_loss=2.7474629878997803\n",
      "Epoch=868, iteration=1, train_loss=2.747858762741089\n",
      "Epoch=868, iteration=2, train_loss=2.7885215282440186\n",
      "Epoch=868, iteration=3, train_loss=2.9140167236328125\n",
      "Epoch=868, val_loss=2.913883686065674\n",
      "Epoch=869, iteration=0, train_loss=2.74746036529541\n",
      "Epoch=869, iteration=1, train_loss=2.747856378555298\n",
      "Epoch=869, iteration=2, train_loss=2.7885210514068604\n",
      "Epoch=869, iteration=3, train_loss=2.91402268409729\n",
      "Epoch=869, val_loss=2.9138896465301514\n",
      "Epoch=870, iteration=0, train_loss=2.74745774269104\n",
      "Epoch=870, iteration=1, train_loss=2.747853994369507\n",
      "Epoch=870, iteration=2, train_loss=2.788520336151123\n",
      "Epoch=870, iteration=3, train_loss=2.9140281677246094\n",
      "Epoch=870, val_loss=2.9138951301574707\n",
      "Epoch=871, iteration=0, train_loss=2.74745512008667\n",
      "Epoch=871, iteration=1, train_loss=2.7478513717651367\n",
      "Epoch=871, iteration=2, train_loss=2.7885196208953857\n",
      "Epoch=871, iteration=3, train_loss=2.9140336513519287\n",
      "Epoch=871, val_loss=2.913900136947632\n",
      "Epoch=872, iteration=0, train_loss=2.7474524974823\n",
      "Epoch=872, iteration=1, train_loss=2.747849225997925\n",
      "Epoch=872, iteration=2, train_loss=2.7885193824768066\n",
      "Epoch=872, iteration=3, train_loss=2.914039134979248\n",
      "Epoch=872, val_loss=2.913904905319214\n",
      "Epoch=873, iteration=0, train_loss=2.747450113296509\n",
      "Epoch=873, iteration=1, train_loss=2.7478466033935547\n",
      "Epoch=873, iteration=2, train_loss=2.7885184288024902\n",
      "Epoch=873, iteration=3, train_loss=2.91404390335083\n",
      "Epoch=873, val_loss=2.913909673690796\n",
      "Epoch=874, iteration=0, train_loss=2.7474477291107178\n",
      "Epoch=874, iteration=1, train_loss=2.7478444576263428\n",
      "Epoch=874, iteration=2, train_loss=2.788517475128174\n",
      "Epoch=874, iteration=3, train_loss=2.914048910140991\n",
      "Epoch=874, val_loss=2.913914918899536\n",
      "Epoch=875, iteration=0, train_loss=2.747445583343506\n",
      "Epoch=875, iteration=1, train_loss=2.74784255027771\n",
      "Epoch=875, iteration=2, train_loss=2.7885172367095947\n",
      "Epoch=875, iteration=3, train_loss=2.9140541553497314\n",
      "Epoch=875, val_loss=2.913918972015381\n",
      "Epoch=876, iteration=0, train_loss=2.747443437576294\n",
      "Epoch=876, iteration=1, train_loss=2.747840642929077\n",
      "Epoch=876, iteration=2, train_loss=2.7885162830352783\n",
      "Epoch=876, iteration=3, train_loss=2.9140584468841553\n",
      "Epoch=876, val_loss=2.913923740386963\n",
      "Epoch=877, iteration=0, train_loss=2.7474417686462402\n",
      "Epoch=877, iteration=1, train_loss=2.7478384971618652\n",
      "Epoch=877, iteration=2, train_loss=2.788515567779541\n",
      "Epoch=877, iteration=3, train_loss=2.914062738418579\n",
      "Epoch=877, val_loss=2.9139280319213867\n",
      "Epoch=878, iteration=0, train_loss=2.747439384460449\n",
      "Epoch=878, iteration=1, train_loss=2.7478368282318115\n",
      "Epoch=878, iteration=2, train_loss=2.788515329360962\n",
      "Epoch=878, iteration=3, train_loss=2.914067029953003\n",
      "Epoch=878, val_loss=2.9139320850372314\n",
      "Epoch=879, iteration=0, train_loss=2.7474377155303955\n",
      "Epoch=879, iteration=1, train_loss=2.7478346824645996\n",
      "Epoch=879, iteration=2, train_loss=2.7885146141052246\n",
      "Epoch=879, iteration=3, train_loss=2.9140713214874268\n",
      "Epoch=879, val_loss=2.913935899734497\n",
      "Epoch=880, iteration=0, train_loss=2.7474355697631836\n",
      "Epoch=880, iteration=1, train_loss=2.747833013534546\n",
      "Epoch=880, iteration=2, train_loss=2.7885143756866455\n",
      "Epoch=880, iteration=3, train_loss=2.9140753746032715\n",
      "Epoch=880, val_loss=2.9139394760131836\n",
      "Epoch=881, iteration=0, train_loss=2.747433662414551\n",
      "Epoch=881, iteration=1, train_loss=2.747831106185913\n",
      "Epoch=881, iteration=2, train_loss=2.788513422012329\n",
      "Epoch=881, iteration=3, train_loss=2.914079189300537\n",
      "Epoch=881, val_loss=2.91394305229187\n",
      "Epoch=882, iteration=0, train_loss=2.747431993484497\n",
      "Epoch=882, iteration=1, train_loss=2.7478299140930176\n",
      "Epoch=882, iteration=2, train_loss=2.788512945175171\n",
      "Epoch=882, iteration=3, train_loss=2.914083242416382\n",
      "Epoch=882, val_loss=2.9139468669891357\n",
      "Epoch=883, iteration=0, train_loss=2.7474303245544434\n",
      "Epoch=883, iteration=1, train_loss=2.747828245162964\n",
      "Epoch=883, iteration=2, train_loss=2.7885124683380127\n",
      "Epoch=883, iteration=3, train_loss=2.9140870571136475\n",
      "Epoch=883, val_loss=2.913950204849243\n",
      "Epoch=884, iteration=0, train_loss=2.7474286556243896\n",
      "Epoch=884, iteration=1, train_loss=2.7478268146514893\n",
      "Epoch=884, iteration=2, train_loss=2.7885119915008545\n",
      "Epoch=884, iteration=3, train_loss=2.914090394973755\n",
      "Epoch=884, val_loss=2.9139535427093506\n",
      "Epoch=885, iteration=0, train_loss=2.747427225112915\n",
      "Epoch=885, iteration=1, train_loss=2.7478251457214355\n",
      "Epoch=885, iteration=2, train_loss=2.7885115146636963\n",
      "Epoch=885, iteration=3, train_loss=2.914093494415283\n",
      "Epoch=885, val_loss=2.913957118988037\n",
      "Epoch=886, iteration=0, train_loss=2.7474257946014404\n",
      "Epoch=886, iteration=1, train_loss=2.747823715209961\n",
      "Epoch=886, iteration=2, train_loss=2.788511276245117\n",
      "Epoch=886, iteration=3, train_loss=2.9140968322753906\n",
      "Epoch=886, val_loss=2.9139597415924072\n",
      "Epoch=887, iteration=0, train_loss=2.7474241256713867\n",
      "Epoch=887, iteration=1, train_loss=2.7478220462799072\n",
      "Epoch=887, iteration=2, train_loss=2.78851056098938\n",
      "Epoch=887, iteration=3, train_loss=2.914099931716919\n",
      "Epoch=887, val_loss=2.9139628410339355\n",
      "Epoch=888, iteration=0, train_loss=2.747422933578491\n",
      "Epoch=888, iteration=1, train_loss=2.747821092605591\n",
      "Epoch=888, iteration=2, train_loss=2.7885100841522217\n",
      "Epoch=888, iteration=3, train_loss=2.914102792739868\n",
      "Epoch=888, val_loss=2.913965940475464\n",
      "Epoch=889, iteration=0, train_loss=2.7474217414855957\n",
      "Epoch=889, iteration=1, train_loss=2.747819423675537\n",
      "Epoch=889, iteration=2, train_loss=2.7885098457336426\n",
      "Epoch=889, iteration=3, train_loss=2.9141061305999756\n",
      "Epoch=889, val_loss=2.913968563079834\n",
      "Epoch=890, iteration=0, train_loss=2.7474205493927\n",
      "Epoch=890, iteration=1, train_loss=2.7478182315826416\n",
      "Epoch=890, iteration=2, train_loss=2.7885093688964844\n",
      "Epoch=890, iteration=3, train_loss=2.9141085147857666\n",
      "Epoch=890, val_loss=2.913971424102783\n",
      "Epoch=891, iteration=0, train_loss=2.7474191188812256\n",
      "Epoch=891, iteration=1, train_loss=2.747817039489746\n",
      "Epoch=891, iteration=2, train_loss=2.788508892059326\n",
      "Epoch=891, iteration=3, train_loss=2.914111614227295\n",
      "Epoch=891, val_loss=2.9139740467071533\n",
      "Epoch=892, iteration=0, train_loss=2.747417449951172\n",
      "Epoch=892, iteration=1, train_loss=2.7478158473968506\n",
      "Epoch=892, iteration=2, train_loss=2.788508653640747\n",
      "Epoch=892, iteration=3, train_loss=2.914114475250244\n",
      "Epoch=892, val_loss=2.9139769077301025\n",
      "Epoch=893, iteration=0, train_loss=2.7474162578582764\n",
      "Epoch=893, iteration=1, train_loss=2.747814893722534\n",
      "Epoch=893, iteration=2, train_loss=2.788508176803589\n",
      "Epoch=893, iteration=3, train_loss=2.914116859436035\n",
      "Epoch=893, val_loss=2.9139792919158936\n",
      "Epoch=894, iteration=0, train_loss=2.747415542602539\n",
      "Epoch=894, iteration=1, train_loss=2.7478134632110596\n",
      "Epoch=894, iteration=2, train_loss=2.7885079383850098\n",
      "Epoch=894, iteration=3, train_loss=2.9141194820404053\n",
      "Epoch=894, val_loss=2.9139816761016846\n",
      "Epoch=895, iteration=0, train_loss=2.7474141120910645\n",
      "Epoch=895, iteration=1, train_loss=2.7478127479553223\n",
      "Epoch=895, iteration=2, train_loss=2.7885076999664307\n",
      "Epoch=895, iteration=3, train_loss=2.9141218662261963\n",
      "Epoch=895, val_loss=2.9139840602874756\n",
      "Epoch=896, iteration=0, train_loss=2.747413158416748\n",
      "Epoch=896, iteration=1, train_loss=2.7478113174438477\n",
      "Epoch=896, iteration=2, train_loss=2.7885069847106934\n",
      "Epoch=896, iteration=3, train_loss=2.9141247272491455\n",
      "Epoch=896, val_loss=2.9139864444732666\n",
      "Epoch=897, iteration=0, train_loss=2.7474122047424316\n",
      "Epoch=897, iteration=1, train_loss=2.7478103637695312\n",
      "Epoch=897, iteration=2, train_loss=2.7885067462921143\n",
      "Epoch=897, iteration=3, train_loss=2.9141271114349365\n",
      "Epoch=897, val_loss=2.9139885902404785\n",
      "Epoch=898, iteration=0, train_loss=2.7474112510681152\n",
      "Epoch=898, iteration=1, train_loss=2.747809648513794\n",
      "Epoch=898, iteration=2, train_loss=2.788506269454956\n",
      "Epoch=898, iteration=3, train_loss=2.9141290187835693\n",
      "Epoch=898, val_loss=2.9139904975891113\n",
      "Epoch=899, iteration=0, train_loss=2.7474100589752197\n",
      "Epoch=899, iteration=1, train_loss=2.7478086948394775\n",
      "Epoch=899, iteration=2, train_loss=2.788506269454956\n",
      "Epoch=899, iteration=3, train_loss=2.9141314029693604\n",
      "Epoch=899, val_loss=2.9139928817749023\n",
      "Epoch=900, iteration=0, train_loss=2.747408866882324\n",
      "Epoch=900, iteration=1, train_loss=2.7478079795837402\n",
      "Epoch=900, iteration=2, train_loss=2.788505792617798\n",
      "Epoch=900, iteration=3, train_loss=2.9141335487365723\n",
      "Epoch=900, val_loss=2.913994789123535\n",
      "Epoch=901, iteration=0, train_loss=2.747408151626587\n",
      "Epoch=901, iteration=1, train_loss=2.747807025909424\n",
      "Epoch=901, iteration=2, train_loss=2.7885053157806396\n",
      "Epoch=901, iteration=3, train_loss=2.914135694503784\n",
      "Epoch=901, val_loss=2.913996458053589\n",
      "Epoch=902, iteration=0, train_loss=2.7474071979522705\n",
      "Epoch=902, iteration=1, train_loss=2.7478058338165283\n",
      "Epoch=902, iteration=2, train_loss=2.7885050773620605\n",
      "Epoch=902, iteration=3, train_loss=2.914137363433838\n",
      "Epoch=902, val_loss=2.91399884223938\n",
      "Epoch=903, iteration=0, train_loss=2.747406482696533\n",
      "Epoch=903, iteration=1, train_loss=2.747805118560791\n",
      "Epoch=903, iteration=2, train_loss=2.7885050773620605\n",
      "Epoch=903, iteration=3, train_loss=2.9141392707824707\n",
      "Epoch=903, val_loss=2.9140005111694336\n",
      "Epoch=904, iteration=0, train_loss=2.747405529022217\n",
      "Epoch=904, iteration=1, train_loss=2.7478044033050537\n",
      "Epoch=904, iteration=2, train_loss=2.7885046005249023\n",
      "Epoch=904, iteration=3, train_loss=2.9141411781311035\n",
      "Epoch=904, val_loss=2.9140021800994873\n",
      "Epoch=905, iteration=0, train_loss=2.7474048137664795\n",
      "Epoch=905, iteration=1, train_loss=2.7478034496307373\n",
      "Epoch=905, iteration=2, train_loss=2.7885043621063232\n",
      "Epoch=905, iteration=3, train_loss=2.9141430854797363\n",
      "Epoch=905, val_loss=2.914004325866699\n",
      "Epoch=906, iteration=0, train_loss=2.747403621673584\n",
      "Epoch=906, iteration=1, train_loss=2.747802495956421\n",
      "Epoch=906, iteration=2, train_loss=2.788503885269165\n",
      "Epoch=906, iteration=3, train_loss=2.91414475440979\n",
      "Epoch=906, val_loss=2.914005756378174\n",
      "Epoch=907, iteration=0, train_loss=2.7474029064178467\n",
      "Epoch=907, iteration=1, train_loss=2.7478020191192627\n",
      "Epoch=907, iteration=2, train_loss=2.788503646850586\n",
      "Epoch=907, iteration=3, train_loss=2.9141461849212646\n",
      "Epoch=907, val_loss=2.9140076637268066\n",
      "Epoch=908, iteration=0, train_loss=2.7474024295806885\n",
      "Epoch=908, iteration=1, train_loss=2.7478013038635254\n",
      "Epoch=908, iteration=2, train_loss=2.788503408432007\n",
      "Epoch=908, iteration=3, train_loss=2.9141483306884766\n",
      "Epoch=908, val_loss=2.9140090942382812\n",
      "Epoch=909, iteration=0, train_loss=2.747401237487793\n",
      "Epoch=909, iteration=1, train_loss=2.747800588607788\n",
      "Epoch=909, iteration=2, train_loss=2.7885031700134277\n",
      "Epoch=909, iteration=3, train_loss=2.914149761199951\n",
      "Epoch=909, val_loss=2.914010524749756\n",
      "Epoch=910, iteration=0, train_loss=2.7474007606506348\n",
      "Epoch=910, iteration=1, train_loss=2.7477996349334717\n",
      "Epoch=910, iteration=2, train_loss=2.7885031700134277\n",
      "Epoch=910, iteration=3, train_loss=2.914151430130005\n",
      "Epoch=910, val_loss=2.9140117168426514\n",
      "Epoch=911, iteration=0, train_loss=2.7474002838134766\n",
      "Epoch=911, iteration=1, train_loss=2.7477993965148926\n",
      "Epoch=911, iteration=2, train_loss=2.7885026931762695\n",
      "Epoch=911, iteration=3, train_loss=2.9141526222229004\n",
      "Epoch=911, val_loss=2.9140138626098633\n",
      "Epoch=912, iteration=0, train_loss=2.7473995685577393\n",
      "Epoch=912, iteration=1, train_loss=2.747798442840576\n",
      "Epoch=912, iteration=2, train_loss=2.7885026931762695\n",
      "Epoch=912, iteration=3, train_loss=2.914154529571533\n",
      "Epoch=912, val_loss=2.9140148162841797\n",
      "Epoch=913, iteration=0, train_loss=2.747398614883423\n",
      "Epoch=913, iteration=1, train_loss=2.747797966003418\n",
      "Epoch=913, iteration=2, train_loss=2.7885022163391113\n",
      "Epoch=913, iteration=3, train_loss=2.914156198501587\n",
      "Epoch=913, val_loss=2.9140162467956543\n",
      "Epoch=914, iteration=0, train_loss=2.7473981380462646\n",
      "Epoch=914, iteration=1, train_loss=2.747797727584839\n",
      "Epoch=914, iteration=2, train_loss=2.7885022163391113\n",
      "Epoch=914, iteration=3, train_loss=2.9141571521759033\n",
      "Epoch=914, val_loss=2.914017677307129\n",
      "Epoch=915, iteration=0, train_loss=2.7473976612091064\n",
      "Epoch=915, iteration=1, train_loss=2.7477965354919434\n",
      "Epoch=915, iteration=2, train_loss=2.788501739501953\n",
      "Epoch=915, iteration=3, train_loss=2.914158582687378\n",
      "Epoch=915, val_loss=2.9140193462371826\n",
      "Epoch=916, iteration=0, train_loss=2.747396945953369\n",
      "Epoch=916, iteration=1, train_loss=2.7477962970733643\n",
      "Epoch=916, iteration=2, train_loss=2.788501501083374\n",
      "Epoch=916, iteration=3, train_loss=2.9141600131988525\n",
      "Epoch=916, val_loss=2.914020299911499\n",
      "Epoch=917, iteration=0, train_loss=2.747396469116211\n",
      "Epoch=917, iteration=1, train_loss=2.747795581817627\n",
      "Epoch=917, iteration=2, train_loss=2.788501501083374\n",
      "Epoch=917, iteration=3, train_loss=2.914161205291748\n",
      "Epoch=917, val_loss=2.9140214920043945\n",
      "Epoch=918, iteration=0, train_loss=2.7473955154418945\n",
      "Epoch=918, iteration=1, train_loss=2.7477951049804688\n",
      "Epoch=918, iteration=2, train_loss=2.788501262664795\n",
      "Epoch=918, iteration=3, train_loss=2.9141626358032227\n",
      "Epoch=918, val_loss=2.91402268409729\n",
      "Epoch=919, iteration=0, train_loss=2.7473952770233154\n",
      "Epoch=919, iteration=1, train_loss=2.7477941513061523\n",
      "Epoch=919, iteration=2, train_loss=2.7885007858276367\n",
      "Epoch=919, iteration=3, train_loss=2.914163589477539\n",
      "Epoch=919, val_loss=2.9140241146087646\n",
      "Epoch=920, iteration=0, train_loss=2.7473948001861572\n",
      "Epoch=920, iteration=1, train_loss=2.7477939128875732\n",
      "Epoch=920, iteration=2, train_loss=2.788501024246216\n",
      "Epoch=920, iteration=3, train_loss=2.9141647815704346\n",
      "Epoch=920, val_loss=2.914025068283081\n",
      "Epoch=921, iteration=0, train_loss=2.74739408493042\n",
      "Epoch=921, iteration=1, train_loss=2.7477939128875732\n",
      "Epoch=921, iteration=2, train_loss=2.7885007858276367\n",
      "Epoch=921, iteration=3, train_loss=2.91416597366333\n",
      "Epoch=921, val_loss=2.9140262603759766\n",
      "Epoch=922, iteration=0, train_loss=2.7473936080932617\n",
      "Epoch=922, iteration=1, train_loss=2.7477927207946777\n",
      "Epoch=922, iteration=2, train_loss=2.7885005474090576\n",
      "Epoch=922, iteration=3, train_loss=2.9141674041748047\n",
      "Epoch=922, val_loss=2.914027214050293\n",
      "Epoch=923, iteration=0, train_loss=2.7473933696746826\n",
      "Epoch=923, iteration=1, train_loss=2.7477924823760986\n",
      "Epoch=923, iteration=2, train_loss=2.7885003089904785\n",
      "Epoch=923, iteration=3, train_loss=2.914168119430542\n",
      "Epoch=923, val_loss=2.9140281677246094\n",
      "Epoch=924, iteration=0, train_loss=2.7473926544189453\n",
      "Epoch=924, iteration=1, train_loss=2.7477917671203613\n",
      "Epoch=924, iteration=2, train_loss=2.7885003089904785\n",
      "Epoch=924, iteration=3, train_loss=2.9141695499420166\n",
      "Epoch=924, val_loss=2.914029598236084\n",
      "Epoch=925, iteration=0, train_loss=2.747392177581787\n",
      "Epoch=925, iteration=1, train_loss=2.7477915287017822\n",
      "Epoch=925, iteration=2, train_loss=2.7885000705718994\n",
      "Epoch=925, iteration=3, train_loss=2.914170503616333\n",
      "Epoch=925, val_loss=2.9140303134918213\n",
      "Epoch=926, iteration=0, train_loss=2.74739146232605\n",
      "Epoch=926, iteration=1, train_loss=2.747790813446045\n",
      "Epoch=926, iteration=2, train_loss=2.788499593734741\n",
      "Epoch=926, iteration=3, train_loss=2.9141714572906494\n",
      "Epoch=926, val_loss=2.9140312671661377\n",
      "Epoch=927, iteration=0, train_loss=2.7473912239074707\n",
      "Epoch=927, iteration=1, train_loss=2.747790575027466\n",
      "Epoch=927, iteration=2, train_loss=2.788499593734741\n",
      "Epoch=927, iteration=3, train_loss=2.9141721725463867\n",
      "Epoch=927, val_loss=2.914032459259033\n",
      "Epoch=928, iteration=0, train_loss=2.7473907470703125\n",
      "Epoch=928, iteration=1, train_loss=2.7477900981903076\n",
      "Epoch=928, iteration=2, train_loss=2.788499355316162\n",
      "Epoch=928, iteration=3, train_loss=2.9141736030578613\n",
      "Epoch=928, val_loss=2.9140334129333496\n",
      "Epoch=929, iteration=0, train_loss=2.7473907470703125\n",
      "Epoch=929, iteration=1, train_loss=2.7477896213531494\n",
      "Epoch=929, iteration=2, train_loss=2.788499116897583\n",
      "Epoch=929, iteration=3, train_loss=2.9141745567321777\n",
      "Epoch=929, val_loss=2.914034366607666\n",
      "Epoch=930, iteration=0, train_loss=2.747389793395996\n",
      "Epoch=930, iteration=1, train_loss=2.7477896213531494\n",
      "Epoch=930, iteration=2, train_loss=2.788499116897583\n",
      "Epoch=930, iteration=3, train_loss=2.914175271987915\n",
      "Epoch=930, val_loss=2.9140355587005615\n",
      "Epoch=931, iteration=0, train_loss=2.747389554977417\n",
      "Epoch=931, iteration=1, train_loss=2.747788906097412\n",
      "Epoch=931, iteration=2, train_loss=2.788499116897583\n",
      "Epoch=931, iteration=3, train_loss=2.9141767024993896\n",
      "Epoch=931, val_loss=2.9140360355377197\n",
      "Epoch=932, iteration=0, train_loss=2.7473888397216797\n",
      "Epoch=932, iteration=1, train_loss=2.747788667678833\n",
      "Epoch=932, iteration=2, train_loss=2.788498640060425\n",
      "Epoch=932, iteration=3, train_loss=2.914177417755127\n",
      "Epoch=932, val_loss=2.914036750793457\n",
      "Epoch=933, iteration=0, train_loss=2.7473886013031006\n",
      "Epoch=933, iteration=1, train_loss=2.747788429260254\n",
      "Epoch=933, iteration=2, train_loss=2.788498640060425\n",
      "Epoch=933, iteration=3, train_loss=2.9141783714294434\n",
      "Epoch=933, val_loss=2.9140379428863525\n",
      "Epoch=934, iteration=0, train_loss=2.7473881244659424\n",
      "Epoch=934, iteration=1, train_loss=2.7477879524230957\n",
      "Epoch=934, iteration=2, train_loss=2.7884984016418457\n",
      "Epoch=934, iteration=3, train_loss=2.9141793251037598\n",
      "Epoch=934, val_loss=2.9140384197235107\n",
      "Epoch=935, iteration=0, train_loss=2.747387647628784\n",
      "Epoch=935, iteration=1, train_loss=2.7477872371673584\n",
      "Epoch=935, iteration=2, train_loss=2.7884981632232666\n",
      "Epoch=935, iteration=3, train_loss=2.914180040359497\n",
      "Epoch=935, val_loss=2.9140398502349854\n",
      "Epoch=936, iteration=0, train_loss=2.747387647628784\n",
      "Epoch=936, iteration=1, train_loss=2.7477869987487793\n",
      "Epoch=936, iteration=2, train_loss=2.7884979248046875\n",
      "Epoch=936, iteration=3, train_loss=2.9141807556152344\n",
      "Epoch=936, val_loss=2.9140405654907227\n",
      "Epoch=937, iteration=0, train_loss=2.747387170791626\n",
      "Epoch=937, iteration=1, train_loss=2.7477867603302\n",
      "Epoch=937, iteration=2, train_loss=2.7884979248046875\n",
      "Epoch=937, iteration=3, train_loss=2.91418194770813\n",
      "Epoch=937, val_loss=2.91404128074646\n",
      "Epoch=938, iteration=0, train_loss=2.747386932373047\n",
      "Epoch=938, iteration=1, train_loss=2.747786521911621\n",
      "Epoch=938, iteration=2, train_loss=2.7884979248046875\n",
      "Epoch=938, iteration=3, train_loss=2.914182424545288\n",
      "Epoch=938, val_loss=2.9140422344207764\n",
      "Epoch=939, iteration=0, train_loss=2.7473864555358887\n",
      "Epoch=939, iteration=1, train_loss=2.747786521911621\n",
      "Epoch=939, iteration=2, train_loss=2.7884979248046875\n",
      "Epoch=939, iteration=3, train_loss=2.9141831398010254\n",
      "Epoch=939, val_loss=2.9140427112579346\n",
      "Epoch=940, iteration=0, train_loss=2.7473859786987305\n",
      "Epoch=940, iteration=1, train_loss=2.747785806655884\n",
      "Epoch=940, iteration=2, train_loss=2.78849720954895\n",
      "Epoch=940, iteration=3, train_loss=2.9141838550567627\n",
      "Epoch=940, val_loss=2.914043426513672\n",
      "Epoch=941, iteration=0, train_loss=2.7473857402801514\n",
      "Epoch=941, iteration=1, train_loss=2.7477855682373047\n",
      "Epoch=941, iteration=2, train_loss=2.78849720954895\n",
      "Epoch=941, iteration=3, train_loss=2.914184808731079\n",
      "Epoch=941, val_loss=2.91404390335083\n",
      "Epoch=942, iteration=0, train_loss=2.747385263442993\n",
      "Epoch=942, iteration=1, train_loss=2.7477855682373047\n",
      "Epoch=942, iteration=2, train_loss=2.7884974479675293\n",
      "Epoch=942, iteration=3, train_loss=2.9141852855682373\n",
      "Epoch=942, val_loss=2.9140446186065674\n",
      "Epoch=943, iteration=0, train_loss=2.747385025024414\n",
      "Epoch=943, iteration=1, train_loss=2.7477850914001465\n",
      "Epoch=943, iteration=2, train_loss=2.78849720954895\n",
      "Epoch=943, iteration=3, train_loss=2.9141860008239746\n",
      "Epoch=943, val_loss=2.914045810699463\n",
      "Epoch=944, iteration=0, train_loss=2.747384786605835\n",
      "Epoch=944, iteration=1, train_loss=2.7477846145629883\n",
      "Epoch=944, iteration=2, train_loss=2.78849720954895\n",
      "Epoch=944, iteration=3, train_loss=2.914186716079712\n",
      "Epoch=944, val_loss=2.914046287536621\n",
      "Epoch=945, iteration=0, train_loss=2.7473843097686768\n",
      "Epoch=945, iteration=1, train_loss=2.747784376144409\n",
      "Epoch=945, iteration=2, train_loss=2.78849720954895\n",
      "Epoch=945, iteration=3, train_loss=2.9141876697540283\n",
      "Epoch=945, val_loss=2.9140467643737793\n",
      "Epoch=946, iteration=0, train_loss=2.7473840713500977\n",
      "Epoch=946, iteration=1, train_loss=2.74778413772583\n",
      "Epoch=946, iteration=2, train_loss=2.788496971130371\n",
      "Epoch=946, iteration=3, train_loss=2.9141879081726074\n",
      "Epoch=946, val_loss=2.9140477180480957\n",
      "Epoch=947, iteration=0, train_loss=2.7473840713500977\n",
      "Epoch=947, iteration=1, train_loss=2.747783660888672\n",
      "Epoch=947, iteration=2, train_loss=2.788496971130371\n",
      "Epoch=947, iteration=3, train_loss=2.914189100265503\n",
      "Epoch=947, val_loss=2.914048194885254\n",
      "Epoch=948, iteration=0, train_loss=2.7473838329315186\n",
      "Epoch=948, iteration=1, train_loss=2.7477834224700928\n",
      "Epoch=948, iteration=2, train_loss=2.788496732711792\n",
      "Epoch=948, iteration=3, train_loss=2.914189338684082\n",
      "Epoch=948, val_loss=2.914048910140991\n",
      "Epoch=949, iteration=0, train_loss=2.7473833560943604\n",
      "Epoch=949, iteration=1, train_loss=2.7477831840515137\n",
      "Epoch=949, iteration=2, train_loss=2.788496732711792\n",
      "Epoch=949, iteration=3, train_loss=2.9141900539398193\n",
      "Epoch=949, val_loss=2.9140493869781494\n",
      "Epoch=950, iteration=0, train_loss=2.7473831176757812\n",
      "Epoch=950, iteration=1, train_loss=2.7477831840515137\n",
      "Epoch=950, iteration=2, train_loss=2.788496494293213\n",
      "Epoch=950, iteration=3, train_loss=2.9141907691955566\n",
      "Epoch=950, val_loss=2.9140501022338867\n",
      "Epoch=951, iteration=0, train_loss=2.7473831176757812\n",
      "Epoch=951, iteration=1, train_loss=2.7477829456329346\n",
      "Epoch=951, iteration=2, train_loss=2.788496255874634\n",
      "Epoch=951, iteration=3, train_loss=2.914191484451294\n",
      "Epoch=951, val_loss=2.914050817489624\n",
      "Epoch=952, iteration=0, train_loss=2.747382879257202\n",
      "Epoch=952, iteration=1, train_loss=2.7477824687957764\n",
      "Epoch=952, iteration=2, train_loss=2.7884960174560547\n",
      "Epoch=952, iteration=3, train_loss=2.914191722869873\n",
      "Epoch=952, val_loss=2.9140512943267822\n",
      "Epoch=953, iteration=0, train_loss=2.747382402420044\n",
      "Epoch=953, iteration=1, train_loss=2.7477824687957764\n",
      "Epoch=953, iteration=2, train_loss=2.7884960174560547\n",
      "Epoch=953, iteration=3, train_loss=2.9141924381256104\n",
      "Epoch=953, val_loss=2.9140517711639404\n",
      "Epoch=954, iteration=0, train_loss=2.747382164001465\n",
      "Epoch=954, iteration=1, train_loss=2.747781991958618\n",
      "Epoch=954, iteration=2, train_loss=2.7884960174560547\n",
      "Epoch=954, iteration=3, train_loss=2.9141931533813477\n",
      "Epoch=954, val_loss=2.9140524864196777\n",
      "Epoch=955, iteration=0, train_loss=2.7473819255828857\n",
      "Epoch=955, iteration=1, train_loss=2.747781991958618\n",
      "Epoch=955, iteration=2, train_loss=2.7884957790374756\n",
      "Epoch=955, iteration=3, train_loss=2.9141933917999268\n",
      "Epoch=955, val_loss=2.914052963256836\n",
      "Epoch=956, iteration=0, train_loss=2.7473814487457275\n",
      "Epoch=956, iteration=1, train_loss=2.747781753540039\n",
      "Epoch=956, iteration=2, train_loss=2.7884957790374756\n",
      "Epoch=956, iteration=3, train_loss=2.914194107055664\n",
      "Epoch=956, val_loss=2.9140536785125732\n",
      "Epoch=957, iteration=0, train_loss=2.7473812103271484\n",
      "Epoch=957, iteration=1, train_loss=2.747781753540039\n",
      "Epoch=957, iteration=2, train_loss=2.7884960174560547\n",
      "Epoch=957, iteration=3, train_loss=2.9141945838928223\n",
      "Epoch=957, val_loss=2.9140539169311523\n",
      "Epoch=958, iteration=0, train_loss=2.7473809719085693\n",
      "Epoch=958, iteration=1, train_loss=2.747781276702881\n",
      "Epoch=958, iteration=2, train_loss=2.7884955406188965\n",
      "Epoch=958, iteration=3, train_loss=2.9141952991485596\n",
      "Epoch=958, val_loss=2.9140548706054688\n",
      "Epoch=959, iteration=0, train_loss=2.747380495071411\n",
      "Epoch=959, iteration=1, train_loss=2.7477805614471436\n",
      "Epoch=959, iteration=2, train_loss=2.7884955406188965\n",
      "Epoch=959, iteration=3, train_loss=2.9141955375671387\n",
      "Epoch=959, val_loss=2.914055347442627\n",
      "Epoch=960, iteration=0, train_loss=2.747380495071411\n",
      "Epoch=960, iteration=1, train_loss=2.7477805614471436\n",
      "Epoch=960, iteration=2, train_loss=2.7884955406188965\n",
      "Epoch=960, iteration=3, train_loss=2.914196252822876\n",
      "Epoch=960, val_loss=2.9140560626983643\n",
      "Epoch=961, iteration=0, train_loss=2.747380256652832\n",
      "Epoch=961, iteration=1, train_loss=2.7477803230285645\n",
      "Epoch=961, iteration=2, train_loss=2.7884950637817383\n",
      "Epoch=961, iteration=3, train_loss=2.9141969680786133\n",
      "Epoch=961, val_loss=2.9140563011169434\n",
      "Epoch=962, iteration=0, train_loss=2.747380018234253\n",
      "Epoch=962, iteration=1, train_loss=2.7477805614471436\n",
      "Epoch=962, iteration=2, train_loss=2.7884950637817383\n",
      "Epoch=962, iteration=3, train_loss=2.9141972064971924\n",
      "Epoch=962, val_loss=2.9140570163726807\n",
      "Epoch=963, iteration=0, train_loss=2.747379779815674\n",
      "Epoch=963, iteration=1, train_loss=2.7477803230285645\n",
      "Epoch=963, iteration=2, train_loss=2.7884953022003174\n",
      "Epoch=963, iteration=3, train_loss=2.9141979217529297\n",
      "Epoch=963, val_loss=2.9140570163726807\n",
      "Epoch=964, iteration=0, train_loss=2.747379779815674\n",
      "Epoch=964, iteration=1, train_loss=2.7477800846099854\n",
      "Epoch=964, iteration=2, train_loss=2.788494825363159\n",
      "Epoch=964, iteration=3, train_loss=2.914198160171509\n",
      "Epoch=964, val_loss=2.914057731628418\n",
      "Epoch=965, iteration=0, train_loss=2.7473793029785156\n",
      "Epoch=965, iteration=1, train_loss=2.7477798461914062\n",
      "Epoch=965, iteration=2, train_loss=2.788494825363159\n",
      "Epoch=965, iteration=3, train_loss=2.914198637008667\n",
      "Epoch=965, val_loss=2.914057970046997\n",
      "Epoch=966, iteration=0, train_loss=2.7473790645599365\n",
      "Epoch=966, iteration=1, train_loss=2.747779369354248\n",
      "Epoch=966, iteration=2, train_loss=2.788494825363159\n",
      "Epoch=966, iteration=3, train_loss=2.914198875427246\n",
      "Epoch=966, val_loss=2.9140584468841553\n",
      "Epoch=967, iteration=0, train_loss=2.7473790645599365\n",
      "Epoch=967, iteration=1, train_loss=2.747779130935669\n",
      "Epoch=967, iteration=2, train_loss=2.788494825363159\n",
      "Epoch=967, iteration=3, train_loss=2.9141993522644043\n",
      "Epoch=967, val_loss=2.9140591621398926\n",
      "Epoch=968, iteration=0, train_loss=2.7473785877227783\n",
      "Epoch=968, iteration=1, train_loss=2.747779130935669\n",
      "Epoch=968, iteration=2, train_loss=2.788494825363159\n",
      "Epoch=968, iteration=3, train_loss=2.9142000675201416\n",
      "Epoch=968, val_loss=2.9140594005584717\n",
      "Epoch=969, iteration=0, train_loss=2.747378349304199\n",
      "Epoch=969, iteration=1, train_loss=2.74777889251709\n",
      "Epoch=969, iteration=2, train_loss=2.788494825363159\n",
      "Epoch=969, iteration=3, train_loss=2.914200782775879\n",
      "Epoch=969, val_loss=2.914060115814209\n",
      "Epoch=970, iteration=0, train_loss=2.747378349304199\n",
      "Epoch=970, iteration=1, train_loss=2.7477786540985107\n",
      "Epoch=970, iteration=2, train_loss=2.788494348526001\n",
      "Epoch=970, iteration=3, train_loss=2.914200782775879\n",
      "Epoch=970, val_loss=2.914060115814209\n",
      "Epoch=971, iteration=0, train_loss=2.747378349304199\n",
      "Epoch=971, iteration=1, train_loss=2.74777889251709\n",
      "Epoch=971, iteration=2, train_loss=2.788494348526001\n",
      "Epoch=971, iteration=3, train_loss=2.914201259613037\n",
      "Epoch=971, val_loss=2.9140608310699463\n",
      "Epoch=972, iteration=0, train_loss=2.747377872467041\n",
      "Epoch=972, iteration=1, train_loss=2.7477784156799316\n",
      "Epoch=972, iteration=2, train_loss=2.788494110107422\n",
      "Epoch=972, iteration=3, train_loss=2.9142017364501953\n",
      "Epoch=972, val_loss=2.9140615463256836\n",
      "Epoch=973, iteration=0, train_loss=2.747377634048462\n",
      "Epoch=973, iteration=1, train_loss=2.7477781772613525\n",
      "Epoch=973, iteration=2, train_loss=2.788494348526001\n",
      "Epoch=973, iteration=3, train_loss=2.9142019748687744\n",
      "Epoch=973, val_loss=2.9140615463256836\n",
      "Epoch=974, iteration=0, train_loss=2.747377634048462\n",
      "Epoch=974, iteration=1, train_loss=2.7477781772613525\n",
      "Epoch=974, iteration=2, train_loss=2.788494348526001\n",
      "Epoch=974, iteration=3, train_loss=2.9142026901245117\n",
      "Epoch=974, val_loss=2.9140617847442627\n",
      "Epoch=975, iteration=0, train_loss=2.747377395629883\n",
      "Epoch=975, iteration=1, train_loss=2.7477781772613525\n",
      "Epoch=975, iteration=2, train_loss=2.788494348526001\n",
      "Epoch=975, iteration=3, train_loss=2.91420316696167\n",
      "Epoch=975, val_loss=2.9140625\n",
      "Epoch=976, iteration=0, train_loss=2.747377395629883\n",
      "Epoch=976, iteration=1, train_loss=2.7477777004241943\n",
      "Epoch=976, iteration=2, train_loss=2.788494110107422\n",
      "Epoch=976, iteration=3, train_loss=2.914203405380249\n",
      "Epoch=976, val_loss=2.914062738418579\n",
      "Epoch=977, iteration=0, train_loss=2.7473771572113037\n",
      "Epoch=977, iteration=1, train_loss=2.7477777004241943\n",
      "Epoch=977, iteration=2, train_loss=2.7884938716888428\n",
      "Epoch=977, iteration=3, train_loss=2.914203405380249\n",
      "Epoch=977, val_loss=2.9140632152557373\n",
      "Epoch=978, iteration=0, train_loss=2.7473771572113037\n",
      "Epoch=978, iteration=1, train_loss=2.7477774620056152\n",
      "Epoch=978, iteration=2, train_loss=2.788494110107422\n",
      "Epoch=978, iteration=3, train_loss=2.9142043590545654\n",
      "Epoch=978, val_loss=2.9140634536743164\n",
      "Epoch=979, iteration=0, train_loss=2.7473766803741455\n",
      "Epoch=979, iteration=1, train_loss=2.747776985168457\n",
      "Epoch=979, iteration=2, train_loss=2.7884938716888428\n",
      "Epoch=979, iteration=3, train_loss=2.9142043590545654\n",
      "Epoch=979, val_loss=2.9140641689300537\n",
      "Epoch=980, iteration=0, train_loss=2.7473764419555664\n",
      "Epoch=980, iteration=1, train_loss=2.747777223587036\n",
      "Epoch=980, iteration=2, train_loss=2.7884938716888428\n",
      "Epoch=980, iteration=3, train_loss=2.9142048358917236\n",
      "Epoch=980, val_loss=2.914064645767212\n",
      "Epoch=981, iteration=0, train_loss=2.7473762035369873\n",
      "Epoch=981, iteration=1, train_loss=2.747776985168457\n",
      "Epoch=981, iteration=2, train_loss=2.7884936332702637\n",
      "Epoch=981, iteration=3, train_loss=2.9142050743103027\n",
      "Epoch=981, val_loss=2.914064884185791\n",
      "Epoch=982, iteration=0, train_loss=2.7473762035369873\n",
      "Epoch=982, iteration=1, train_loss=2.747776746749878\n",
      "Epoch=982, iteration=2, train_loss=2.7884936332702637\n",
      "Epoch=982, iteration=3, train_loss=2.914205551147461\n",
      "Epoch=982, val_loss=2.91406512260437\n",
      "Epoch=983, iteration=0, train_loss=2.7473762035369873\n",
      "Epoch=983, iteration=1, train_loss=2.747776746749878\n",
      "Epoch=983, iteration=2, train_loss=2.7884936332702637\n",
      "Epoch=983, iteration=3, train_loss=2.9142062664031982\n",
      "Epoch=983, val_loss=2.9140655994415283\n",
      "Epoch=984, iteration=0, train_loss=2.747375965118408\n",
      "Epoch=984, iteration=1, train_loss=2.747776508331299\n",
      "Epoch=984, iteration=2, train_loss=2.7884936332702637\n",
      "Epoch=984, iteration=3, train_loss=2.9142065048217773\n",
      "Epoch=984, val_loss=2.9140663146972656\n",
      "Epoch=985, iteration=0, train_loss=2.74737548828125\n",
      "Epoch=985, iteration=1, train_loss=2.747776508331299\n",
      "Epoch=985, iteration=2, train_loss=2.7884936332702637\n",
      "Epoch=985, iteration=3, train_loss=2.9142067432403564\n",
      "Epoch=985, val_loss=2.9140665531158447\n",
      "Epoch=986, iteration=0, train_loss=2.747375249862671\n",
      "Epoch=986, iteration=1, train_loss=2.747776508331299\n",
      "Epoch=986, iteration=2, train_loss=2.7884933948516846\n",
      "Epoch=986, iteration=3, train_loss=2.9142072200775146\n",
      "Epoch=986, val_loss=2.914067029953003\n",
      "Epoch=987, iteration=0, train_loss=2.747375249862671\n",
      "Epoch=987, iteration=1, train_loss=2.7477762699127197\n",
      "Epoch=987, iteration=2, train_loss=2.7884933948516846\n",
      "Epoch=987, iteration=3, train_loss=2.9142074584960938\n",
      "Epoch=987, val_loss=2.914067268371582\n",
      "Epoch=988, iteration=0, train_loss=2.747375249862671\n",
      "Epoch=988, iteration=1, train_loss=2.7477760314941406\n",
      "Epoch=988, iteration=2, train_loss=2.7884931564331055\n",
      "Epoch=988, iteration=3, train_loss=2.9142074584960938\n",
      "Epoch=988, val_loss=2.9140677452087402\n",
      "Epoch=989, iteration=0, train_loss=2.747375011444092\n",
      "Epoch=989, iteration=1, train_loss=2.7477757930755615\n",
      "Epoch=989, iteration=2, train_loss=2.7884929180145264\n",
      "Epoch=989, iteration=3, train_loss=2.914207935333252\n",
      "Epoch=989, val_loss=2.9140679836273193\n",
      "Epoch=990, iteration=0, train_loss=2.747375011444092\n",
      "Epoch=990, iteration=1, train_loss=2.7477757930755615\n",
      "Epoch=990, iteration=2, train_loss=2.7884931564331055\n",
      "Epoch=990, iteration=3, train_loss=2.9142086505889893\n",
      "Epoch=990, val_loss=2.9140679836273193\n",
      "Epoch=991, iteration=0, train_loss=2.7473745346069336\n",
      "Epoch=991, iteration=1, train_loss=2.7477755546569824\n",
      "Epoch=991, iteration=2, train_loss=2.7884931564331055\n",
      "Epoch=991, iteration=3, train_loss=2.9142086505889893\n",
      "Epoch=991, val_loss=2.9140682220458984\n",
      "Epoch=992, iteration=0, train_loss=2.7473742961883545\n",
      "Epoch=992, iteration=1, train_loss=2.7477753162384033\n",
      "Epoch=992, iteration=2, train_loss=2.7884931564331055\n",
      "Epoch=992, iteration=3, train_loss=2.9142088890075684\n",
      "Epoch=992, val_loss=2.9140686988830566\n",
      "Epoch=993, iteration=0, train_loss=2.7473742961883545\n",
      "Epoch=993, iteration=1, train_loss=2.7477753162384033\n",
      "Epoch=993, iteration=2, train_loss=2.7884929180145264\n",
      "Epoch=993, iteration=3, train_loss=2.9142091274261475\n",
      "Epoch=993, val_loss=2.9140689373016357\n",
      "Epoch=994, iteration=0, train_loss=2.7473742961883545\n",
      "Epoch=994, iteration=1, train_loss=2.7477753162384033\n",
      "Epoch=994, iteration=2, train_loss=2.7884929180145264\n",
      "Epoch=994, iteration=3, train_loss=2.9142096042633057\n",
      "Epoch=994, val_loss=2.914069414138794\n",
      "Epoch=995, iteration=0, train_loss=2.7473740577697754\n",
      "Epoch=995, iteration=1, train_loss=2.7477753162384033\n",
      "Epoch=995, iteration=2, train_loss=2.7884929180145264\n",
      "Epoch=995, iteration=3, train_loss=2.914210319519043\n",
      "Epoch=995, val_loss=2.9140701293945312\n",
      "Epoch=996, iteration=0, train_loss=2.7473738193511963\n",
      "Epoch=996, iteration=1, train_loss=2.747775077819824\n",
      "Epoch=996, iteration=2, train_loss=2.7884929180145264\n",
      "Epoch=996, iteration=3, train_loss=2.914210319519043\n",
      "Epoch=996, val_loss=2.9140703678131104\n",
      "Epoch=997, iteration=0, train_loss=2.7473738193511963\n",
      "Epoch=997, iteration=1, train_loss=2.747775077819824\n",
      "Epoch=997, iteration=2, train_loss=2.7884929180145264\n",
      "Epoch=997, iteration=3, train_loss=2.9142110347747803\n",
      "Epoch=997, val_loss=2.9140703678131104\n",
      "Epoch=998, iteration=0, train_loss=2.7473738193511963\n",
      "Epoch=998, iteration=1, train_loss=2.747774839401245\n",
      "Epoch=998, iteration=2, train_loss=2.7884929180145264\n",
      "Epoch=998, iteration=3, train_loss=2.9142110347747803\n",
      "Epoch=998, val_loss=2.9140706062316895\n",
      "Epoch=999, iteration=0, train_loss=2.747373342514038\n",
      "Epoch=999, iteration=1, train_loss=2.747774600982666\n",
      "Epoch=999, iteration=2, train_loss=2.7884929180145264\n",
      "Epoch=999, iteration=3, train_loss=2.9142112731933594\n",
      "Epoch=999, val_loss=2.9140713214874268\n",
      "Epoch=1000, iteration=0, train_loss=2.747373342514038\n",
      "Epoch=1000, iteration=1, train_loss=2.747774362564087\n",
      "Epoch=1000, iteration=2, train_loss=2.788492441177368\n",
      "Epoch=1000, iteration=3, train_loss=2.9142115116119385\n",
      "Epoch=1000, val_loss=2.9140713214874268\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for k,v in parameters_finetune.items():\n",
    "    v.requires_grad = True\n",
    "    \n",
    "model_finetune = train(parameters_finetune, X_train, y_train, X_validation, y_validation, 10, le, batch_size=BATCH_SIZE, epochs=EPOCHS, lr=lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'yihineilmjr', 'daj', 'rllaly', '', 'ahy', 'iyu', 'heateid', 'oaasiioid', 'ateihrejia']\n"
     ]
    }
   ],
   "source": [
    "# Generate names\n",
    "print([''.join(i) for i in inference(model_finetune, 10)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch_AndrejKarpathy_venv",
   "language": "python",
   "name": "pytorch_andrejkarpathy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
