{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def char_to_int(data):\n",
    "    '''\n",
    "    Given a dataset of words(names), char_to_int converts the unique characters to an integer and assigns an id to them.\n",
    "    This is for train step.\n",
    "\n",
    "    Args:\n",
    "        data: a list of names\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of keys being characters and values the corrosponding integer id to each token\n",
    "    '''\n",
    "\n",
    "    char_ids = {}\n",
    "    chars = sorted(set(''.join(data)))\n",
    "    char_ids['.'] = 0\n",
    "    for idx, c in enumerate(chars):\n",
    "        char_ids[c] = idx + 1\n",
    "    return char_ids\n",
    "\n",
    "\n",
    "def int_to_char(data: dict):\n",
    "    '''\n",
    "    Given a dataset of ids, int_to_char converts the ids to their original character. This is for inference step.\n",
    "\n",
    "    Args:\n",
    "        data: a dictionary of (chars, ids)\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of (ids, chars)\n",
    "    '''\n",
    "    chars = {}\n",
    "    for k, v in data.items():\n",
    "        chars[v] = k\n",
    "\n",
    "    return chars\n",
    "\n",
    "\n",
    "char_ids = char_to_int(words)\n",
    "id_chars = int_to_char(char_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def make_dataset(data: list, sequence_length: int) -> torch.tensor:\n",
    "    '''\n",
    "    Making dataset from x sequential tokens\n",
    "\n",
    "    Args: \n",
    "        data: words\n",
    "        sequence_length: length of sequential characters to be a sample in the dataset\n",
    "    \n",
    "    Returns:\n",
    "        X, y: data and labels of the dataset\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    random.shuffle(X)\n",
    "    \n",
    "    for word in data:\n",
    "        context = [0] * sequence_length\n",
    "        for ch in word + '.':\n",
    "            X.append(context)\n",
    "            y.append(char_ids[ch])\n",
    "            c_id = char_ids[ch]\n",
    "            \n",
    "            context = context[1:] + [c_id]    \n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "\n",
    "X, y = make_dataset(words, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def data_split(X: torch.tensor, y: torch.tensor, trian_size: int) -> torch.tensor:\n",
    "    '''\n",
    "    Splitting dataset in 3 sets of train, validation, and test\n",
    "\n",
    "    Args:\n",
    "        X: sequential characters dataset\n",
    "        y: next tokens or labels\n",
    "\n",
    "    Returns: \n",
    "        train_size: size of the training set\n",
    "    '''\n",
    "\n",
    "    trian_size = round(X.size()[0]*trian_size)\n",
    "    validation_size = round((X.size()[0] - trian_size)*0.5)\n",
    "    \n",
    "    X_train = X[:trian_size]\n",
    "    X_validation = X[trian_size:trian_size+validation_size]\n",
    "    X_test = X[trian_size+validation_size:]\n",
    "\n",
    "    y_train = y[:trian_size]\n",
    "    y_validation = y[trian_size:trian_size+validation_size]\n",
    "    y_test = y[trian_size+validation_size:]\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test = data_split(X, y, trian_size=0.8)\n",
    "print(f\"Numebr of train samples: {X_train.size()[0]}\")\n",
    "print(f\"Number of validation samples: {X_validation.size()[0]}\")\n",
    "print(f\"Number of test samples: {X_test.size()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape: tuple, layer2_neurons: int, embedding_dim: int, n_classes: int) -> dict:\n",
    "    '''\n",
    "    Creating the model of the paper with 3 layers.\n",
    "\n",
    "    Args:\n",
    "        input_shape: sequence length of the input data\n",
    "        layer2_neurons: number of hidden layer neurons\n",
    "        embedding_dim: embedding layer dimensions. The number of dimensions we want our data to be presented with\n",
    "        n_classes: number of classes to be predicted. Last layer neurons\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of all layers that we call parameters of the network.\n",
    "    \n",
    "    '''\n",
    "    C = torch.rand((n_classes, embedding_dim))\n",
    "    # embed = C[input_data] # 27*3*2\n",
    "    # layer1 = embed.view((-1, input_data.size()[-1]*embedding_dim))\n",
    "    W1 = torch.rand((input_shape*embedding_dim, layer2_neurons)) # merging dimensions is always from the most inner dimension\n",
    "    b1  = torch.rand(layer2_neurons)\n",
    "    W2 = torch.rand((layer2_neurons, n_classes))\n",
    "    b2 = torch.rand(n_classes)\n",
    "\n",
    "    return {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "parameters = create_model(X.size()[-1], 100, 2, 27)\n",
    "print(f\"Total number of parameters: {sum(v.nelement() for k, v in parameters.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w = 0 bashe chi mishe<br>\n",
    "batch norm chra jitter mikone o bad bud? vaghti y sample darim std o normesh chie asln? solved (kolle dade)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add batch norm<br>\n",
    "read andrew's blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6778454c-3130-800a-b1b4-a3ba9cb750f1\n",
    "layerNorm is not among all the data\n",
    "\n",
    "\n",
    "\n",
    "We cannot normalize based on all the data in the dataset during training because we need to perform backpropagation in real-time. Instead, the largest normalization we use during training is BatchNorm, which computes mean and variance over mini-batches. (In a sense, using all data in Gradient Descent would resemble BatchNorm over the entire dataset.)\n",
    "\n",
    "The formula involving 0.999 and 0.001 (seen in videos) is used to ensure that the mean and variance across all training data are tracked and used consistently after training. This is essential for inference, where we rely on the tracked statistics from the training phase rather than recomputing them on test data. Using test data for normalization would invalidate the model, as it would essentially \"peek\" into test data, which is against the principles of proper model evaluation. Or even we use the train data after the training is done to calculate these mean and inference and use it on test data. But this is not possible when layers are grow and it is not efficient or scalable. So a similar approach would be keeping track of 0.999 and 0.001.\n",
    "\n",
    "\n",
    "Tracking the mean and variance in BatchNorm layers during training ensures that we can use them during inference. This approach also enables single-data-point training (SGD) since the normalization is no longer based on just one data point but on the accumulated statistics from the entire training process.\n",
    "\n",
    "\n",
    "\n",
    "BatchNorm Statistics: Batch normalization tracks running mean and variance using an exponential moving average (e.g., weights like 0.999 and 0.001). This ensures that during inference, the model uses statistics computed during training rather than recalculating them on test data, which would break the principle of not relying on unseen data.\n",
    "\n",
    "SGD Compatibility: BatchNorm enables single-sample SGD to work effectively because it doesn't rely on the statistics of just one sample during inference but uses the accumulated mean and variance from training. This mitigates the issue of unreliable normalization when the batch size is small or consists of just one sample.\n",
    "\n",
    "Why Statistics Are Tracked: This approach ensures the model generalizes based on training data. Using test data to compute mean and variance would introduce data leakage, compromising model evaluation and fairness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
