{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w = 0 bashe chi mishe<br>\n",
    "batch norm chra jitter mikone o bad bud? vaghti y sample darim std o normesh chie asln? solved (kolle dade)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add batch norm<br>\n",
    "read andrew's blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6778454c-3130-800a-b1b4-a3ba9cb750f1\n",
    "layerNorm is not among all the data\n",
    "\n",
    "\n",
    "\n",
    "We cannot normalize based on all the data in the dataset during training because we need to perform backpropagation in real-time. Instead, the largest normalization we use during training is BatchNorm, which computes mean and variance over mini-batches. (In a sense, using all data in Gradient Descent would resemble BatchNorm over the entire dataset.)\n",
    "\n",
    "The formula involving 0.999 and 0.001 (seen in videos) is used to ensure that the mean and variance across all training data are tracked and used consistently after training. This is essential for inference, where we rely on the tracked statistics from the training phase rather than recomputing them on test data. Using test data for normalization would invalidate the model, as it would essentially \"peek\" into test data, which is against the principles of proper model evaluation. Or even we use the train data after the training is done to calculate these mean and inference and use it on test data. But this is not possible when layers are grow and it is not efficient or scalable. So a similar approach would be keeping track of 0.999 and 0.001.\n",
    "\n",
    "\n",
    "Tracking the mean and variance in BatchNorm layers during training ensures that we can use them during inference. This approach also enables single-data-point training (SGD) since the normalization is no longer based on just one data point but on the accumulated statistics from the entire training process.\n",
    "\n",
    "\n",
    "\n",
    "BatchNorm Statistics: Batch normalization tracks running mean and variance using an exponential moving average (e.g., weights like 0.999 and 0.001). This ensures that during inference, the model uses statistics computed during training rather than recalculating them on test data, which would break the principle of not relying on unseen data.\n",
    "\n",
    "SGD Compatibility: BatchNorm enables single-sample SGD to work effectively because it doesn't rely on the statistics of just one sample during inference but uses the accumulated mean and variance from training. This mitigates the issue of unreliable normalization when the batch size is small or consists of just one sample.\n",
    "\n",
    "Why Statistics Are Tracked: This approach ensures the model generalizes based on training data. Using test data to compute mean and variance would introduce data leakage, compromising model evaluation and fairness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
